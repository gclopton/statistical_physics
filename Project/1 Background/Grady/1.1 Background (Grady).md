

# 1. Two Equivalent Methods of Calculating thermal conductivity in a crystalline solid; GreenKubo and nonequilibrium MD

Start with energy conservation at the continuum level. If $e(\mathbf{r}, t)$ is the energy density and $\mathbf{J}(\mathbf{r}, t)$ the heat (energy) flux, the continuity equation $\partial_t e+\nabla \cdot \mathbf{J}=0$ holds. In a macroscopically homogeneous solid subjected to a small, slowly varying temperature field $T(\mathbf{r})$, linear irreversible thermodynamics posits that the flux responds linearly to the driving force $-\nabla T: \mathrm{J}=-k \nabla T$, where $k$ is the (generally anisotropic) thermal-conductivity tensor. This "Fourier law" is not an assumption about microscopic mechanisms; it is the first-order term in a gradient expansion constrained by symmetry and Onsager reciprocity. The value of $k$ is a property of the underlying many-body dynamics and can be computed from a fluctuation-dissipation relation: in equilibrium, spontaneous energy currents fluctuate, and their time correlation encodes the linear response to a small temperature gradient.

Kubo's linear-response theory makes this precise. In an equilibrium molecular-dynamics (MD) trajectory at temperature $T$ and volume $V$, define the total microscopic heat flux $\mathrm{J}(t)$ as the spatial integral of the local energy current consistent with the atomistic Hamiltonian (in practice: a sum over per-atom energies and virials). Stationarity of equilibrium time series then gives the Green-Kubo formula

$$
k_{\alpha \beta}=\frac{1}{k_{\mathrm{B}} T^2 V} \int_0^{\infty}\left\langle J_\alpha(0) J_\beta(t)\right\rangle d t,
$$


which is a direct consequence of the fluctuation-dissipation theorem. Two details matter physically. First, this is a property of the unperturbed Hamiltonian dynamics, so during the measurement one does not couple the system to a thermostat that injects or extracts energy on the same time scales as the correlations being measured. Second, the derivation assumes the linear regime: if a real gradient were applied, it would be small enough that higher-order terms in $\nabla T$ are negligible. When those conditions hold, the integral of the heat-current autocorrelation converges to a plateau that equals $k$.

Nonequilibrium MD (NEMD) takes the complementary route: one deliberately drives a steady heat current and reads off the proportionality constant. In the Müller-Plathe method, this is done microscopically by exchanging momenta between designated hot and cold slabs, which imposes a net energy flux $q$ without adding external fields to the equations of motion. After transients decay, the time-averaged temperature profile is piecewise linear away from the exchange regions; measuring $\nabla T$ in that bulk-like zone and using $k=-q / \nabla T$ yields the same linear-response coefficient. The equivalence of Green-Kubo and NEMD is again a statement about linearity: the coefficient connecting J to $-\nabla T$ is unique, whether inferred from spontaneous fluctuations at equilibrium or from a controlled, sufficiently weak steady driving. In practice each route has distinct finite-size and finite-time systematics-GK is sensitive to how well the simulation box admits long-wavelength phonons and to noise in long-time correlation tails; NEMD is sensitive to boundary conditions, the thickness of the measurement region, and the magnitude of the imposed flux––but in the regime where those effects are controlled, they converge.


Microscopically, heat in an insulating crystal is transported primarily by lattice vibrations. In the phonon-gas picture underlying the Boltzmann transport equation (BTE), one resolves normal modes $\lambda=(\mathrm{q}, s)$ with group velocities $v_\lambda$, specific heats $C_\lambda$, and scattering lifetimes $\tau_\lambda$, leading to

$$
k=\frac{1}{V} \sum_\lambda C_\lambda v_\lambda^2 \tau_\lambda \quad \text { (isotropic average gives } k=\frac{1}{3} \sum_\lambda C_\lambda v_\lambda^2 \tau_\lambda \text { ). }
$$


Anharmonic interactions generate phonon-phonon scattering (normal and Umklapp), while defects-like oxygen vacancies and the accompanying $\mathrm{Ce}^{3+}$ centers in ceria-add elastic and inelastic scattering channels that shorten $\tau_\lambda$, thereby reducing $k$. This modal decomposition is powerful for analysis and, when fed ab-initio force constants, can predict mode-resolved contributions and selection rules. Classical MD does not explicitly construct phonons or solve the BTE. Instead, it integrates the interacting many-body dynamics and, through either GK or NEMD, lets all scattering processes represented by the interatomic potential manifest implicitly. That is exactly why MD is a good tool for trend questions like "how does $k$ change with vacancy fraction $x$ and temperature $T$ for a given force field?"-all the complicated multiple-scattering physics is baked into the trajectory statistics without extra modeling assumptions.


Two caveats set the modeling scope. Classical nuclei obey equipartition, so high-frequency modes carry $k_{\mathrm{B}} T$ of energy regardless of temperature; at low temperatures this overestimates heat capacity and can bias absolute $k$. For trends with defects at moderate and high $T$ (where many modes are thermally populated), classical MD is widely used and gives robust relative changes with $x$ and $T$. And any MD result is conditioned on the fidelity of the force field: if the potential reproduces the equilibrium structure and elastic/defect energetics of ceria, the resulting transport coefficients inherit that fidelity; if it misses key physics (e.g., polarizability or proper defect-phonon coupling), $k$ will reflect those omissions. Within those boundaries, the linear-response framework explains exactly why your plan-equilibrate the correct thermodynamic state, run unperturbed dynamics, and extract $k$ from either fluctuations (GK) or a gentle steady gradient (NEMD)—produces a scientifically valid conductivity for insertion into fuel and SHI models.


[[Sources]]


# 2. Ceria Redox Chemistry


The nonstoichiometry of ceria is redox-driven. In fluorite $\mathrm{CeO}_2$, cerium sits nominally as $\mathrm{Ce}^{4+}$ on the cation sublattice and oxygen as $\mathrm{O}^{2-}$ on the anion sublattice. Removing an oxygen anion creates an oxygen vacancy $V_O$ that is effectively doubly positive relative to the lattice. In defect-chemistry (KrögerVink) notation one writes, under reducing conditions,

$$
\mathrm{O}_{\mathrm{O}}^{\times} \rightarrow V_{\mathrm{O}}^{\bullet \bullet}+\frac{1}{2} \mathrm{O}_2(g)+2 e^{\prime},
$$

followed by the reduction of two nearby cerium cations,

$$
\mathrm{Ce}_{\mathrm{Ce}}^{\times}+e^{\prime} \rightarrow \mathrm{Ce}_{\mathrm{Ce}}^{\prime},
$$


so that each removed $\mathrm{O}^{2-}$ is compensated by two $\mathrm{Ce}^{3+}$ centers. In classical MD this "electron bookkeeping" is represented by changing species labels and charges: for every vacancy you explicitly retag two neighboring $\mathrm{Ce}^{4+}$ as $\mathrm{Ce}^{3+}$. That preserves charge neutrality without a uniform background and encodes the experimentally observed association between $V_0$ and small-polaron $\mathrm{Ce}^{3+}$ centers. Electrostatics alone already favors this local pairing: bringing +2 effective charge on $V_{\mathrm{O}}$ closer to two +3 cations is less costly than leaving compensation far away, and the ionic radius of $\mathrm{Ce}^{3+}$ is larger than $\mathrm{Ce}^{4+}$ , which helps relax the tensile strain field around a missing oxygen. The result is a defect complex that perturbs masses, equilibrium distances, and force constants in its vicinity.


Those local perturbations are exactly what scatter lattice vibrations. In the standard picture of pointdefect phonon scattering, the rate contains a Rayleigh term $\propto \omega^4$ with a strength set by a "disorder parameter" that bundles mass mismatch, size (strain) mismatch, and bond-stiffness changes. An oxygen vacancy is extreme on all three counts: it removes the oxygen mass entirely, eliminates two strong $\mathrm{Ce}-\mathrm{O}$ bonds, and introduces an anisotropic strain field. Even when the modal specific heat is roughly classical at your temperatures, the phonon lifetimes $\tau_\lambda$ shorten in the defected regions, and by Matthiessen-type reasoning that reduces the bulk $k$. As vacancy fraction $x$ increases, more of the lattice is within a scattering radius of some $V_{\mathrm{O}} / \mathrm{Ce}^{3+}$ complex, so the cumulative lifetime reduction grows and $k(T, x)$ trends downward; temperature modulates the balance between intrinsic anharmonic scattering and defect scattering, which is why you should expect a nontrivial $T$-dependence at fixed $x$.


All of this only helps if the interatomic model actually represents the relevant physics of ceria across oxidation states. The workhorse for ionic oxides in classical MD is a fixed-charge, pairwise Buckingham + Coulomb form,

$$
U(r)=A e^{-r / \rho}-\frac{C}{r^6}+\frac{q_i q_j}{4 \pi \varepsilon_0 r},
$$

with parameter sets fitted to reproduce lattice constants, elastic constants, and sometimes defect formation energies. Two modeling choices are pivotal for reduced ceria. First, $\mathrm{Ce}^{3+}$ and $\mathrm{Ce}^{4+}$ must be distinct species with different charges and Buckingham parameters; "smearing" reduction by giving every Ce a partial charge is not equivalent, because the short-range repulsion and dispersion differ between the two oxidation states, and so do their equilibrium Ce-O distances. Second, the oxygen anion in fluorites is highly polarizable; nonpolarizable fixed-charge models can reproduce average structures but often misrepresent dielectric response and phonon dispersions that matter for heat transport. Polarizable models (e.g., shell models or dipole-induction schemes like DIPPIM) add an induced-dipole degree of freedom to oxygen (and sometimes cations), which improves transferability across stoichiometries and temperatures at extra cost. Your proposal's scope is perfectly compatible with a well-vetted fixed-charge model that includes explicit $\mathrm{Ce}^{3+} / \mathrm{Ce}^{4+}$. If you choose that route, you should validate that the trained properties include at least stoichiometric $\mathrm{CeO}_2$ lattice parameter and moduli and that the same parameterization has been exercised for reduced compositions; if not, you risk encoding the wrong relaxation around $V_0$ and therefore the wrong scattering strength.


These physics choices map directly onto the implementation you sketched. When you generate a target $x$ , you are not just deleting oxygen sites; you are creating a well-posed mixed-valence lattice. Assigning the two nearest cerium cations to each vacancy as $\mathrm{Ce}^{3+}$ is a low-energy, reproducible rule that seeds the expected local complex; running NPT and then NVT lets the larger $\mathrm{Ce}^{3+}$ ions and the surrounding oxygens find their relaxed geometry at the correct density. Long-range Coulombics must be summed (PPPM/Ewald) because defect-defect and defect-phonon interactions are long-ranged in an ionic crystal; ad-hoc screening would undercut the very scattering you intend to study. Finally, the validation targets you listed-lattice parameter and density for $x=0$; a reasonable $k$ bracket for perfect $\mathrm{CeO}_2$-are not box-checking. They are the quickest ways to detect whether your chosen potential captures the $\mathrm{Ce}^{3+} / \mathrm{Ce}^{4+}$ chemistry and the elastic background on which the vacancy strain fields live. If those pass, the same model can be trusted to supply the local stiffness and mass contrast that control phonon lifetimes and therefore $k(T, x)$ in the reduced system.


# 3. Long-Range Electrostatics and Energy Partition


Ionic crystals like ceria are dominated by Coulomb interactions that decay as $1 / r$. Under periodic boundary conditions those lattice sums are conditionally convergent: the value depends on the order in which you add terms unless you specify a summation convention. Ewald summation-and its FFTaccelerated variant, PPPM-fixes the convention and gives a rapidly convergent, parameter-independent result by splitting the interaction into a short-range real-space part and a smooth long-range part carried in reciprocal space. A Gaussian screening charge of width $\alpha^{-1}$ is added and subtracted around each ion; the added piece makes the real-space potential short-ranged (cut at $r_c$ ), while the subtracted piece produces a smooth "k-space" field that can be summed analytically (Ewald) or on a mesh (PPPM). Provided the system is charge-neutral-which our explicit $V_{\mathrm{O}}^{* *}+2 \mathrm{Ce}^{3+}$ bookkeeping ensures-the total energy and forces do not depend on $\alpha, r_{c r}$, or the mesh once each part is converged. That invariance is exactly why we keep PPPM instead of "screening away" Coulomb with ad-hoc truncations like Wolf/DSF. Those real-space screening schemes can be useful in some liquids, but in an ionic solid they change the physics: they alter the long-wavelength dielectric response, shift phonon dispersions, and bias defectdefect interactions because they replace the true lattice sum with a different, strictly short-range interaction. Since thermal conductivity is sensitive to long-wavelength phonons and to how defects scatter them through both near-field and far-field strains/electrostatics, you want the correct long-range tail.


The Green-Kubo heat flux requires a consistent microscopic bookkeeping of energy and stress. In linear response one defines the energy density $e(\mathbf{r}, t)$ and its current $\mathbf{j}_e(\mathbf{r}, t)$ so that the continuity equation $\partial_t e+\nabla \cdot \mathrm{j}_e=0$ holds. Coarse-graining à la Irving-Kirkwood/Hardy leads to a total energy flux

$$
\mathbf{J}(t)=\frac{d}{d t} \sum_i \mathbf{r}_i e_i=\sum_i e_i \mathbf{v}_i+\frac{1}{2} \sum_{i \neq j} \mathbf{r}_{i j}\left(\mathbf{f}_{i j} \cdot \mathbf{v}_i\right)+\ldots
$$


The first term is convective transport of per-atom energy $e_i$; the second is the virial contribution (work done by interparticle forces as contacts sweep through space). For simple pairwise forces the ". . ." vanish; for many-body forces there are extra, but analogous, terms. Two subtleties matter. First, the peratom energy $e_i$ and per-atom virial (stress) are not mathematically unique; you can add a divergence-free "gauge" to the microscopic flux without changing macroscopic conservation laws. The Green-Kubo integral is gauge-invariant if, and only if, you use a self-consistent partition: the same definition of $e_i$ and virial must be used in both the convective and virial parts when assembling J. Second, with Ewald/PPPM there is genuine $\boldsymbol{k}$-space energy and stress: part of $e_i$ and part of the virial arise from the smooth reciprocal-space field. If you drop those pieces, the gauge balance is broken and $\mathbf{J}$ is incomplete; the GK integral will then spuriously depend on the Ewald splitting parameter or the mesh.

That is the theoretical reason behind the very specific computes you'll use. You need a per-atom potential energy that includes the reciprocal-space contribution and a per-atom stress (virial) that likewise includes it; you then form J from those two atom-wise fields and the velocities. When you run the GK production in NVE, the equations of motion are Hamiltonian and the fluctuation-dissipation link holds; thermostats/barostats would otherwise inject sink/source terms that contaminate $\mathbf{J}$ at the same time scales as the correlations you integrate. In practice this also means tightening the PPPM accuracy so that energy drift and force noise from the mesh are below the statistical noise of the heat-flux correlation; if the k -space solve is sloppy, the long-time tail of $\langle\mathbf{J}(0) \cdot \mathbf{J}(t)\rangle$ acquires numerical artifacts that look like physics.

Two additional boundary conditions close the loop. Ewald/PPPM implicitly assumes a macroscopic dielectric "outside" the periodically replicated crystal. The usual "tin-foil" (conducting) boundary condition sets the external dielectric to infinity so the net dipole of the simulation cell does not create a spurious uniform field; this is the standard choice for bulk ionic solids. And because the Coulomb lattice sum only converges for neutral cells, our explicit $\mathrm{Ce}^{3+} / \mathrm{Ce}^{4+}$ assignment at every vacancy is not optional bookkeeping-it is what makes the Ewald sum well-posed and ensures that the k-space energy/stress we feed into the flux are defined.

Put plainly: long-range electrostatics are part of the signal in $k$ for ceria, not a nuisance to be trimmed. PPPM/Ewald gives you the correct forces and the correct partition of energy/virial into real- and reciprocal-space pieces; building the heat flux from those same per-atom fields is what guarantees that the Green-Kubo integral is independent of arbitrary Ewald parameters and reflects only the physics of your chosen force field.



# 4. NPT → NVT → NVE (Set the state →settle the microstate → measure without touching )


In an ionic crystal like ceria, the conductivity is sensitive to density because phonon velocities and dispersions shift with volume. The elastic background that phonons propagate through-sound speeds, Debye cutoff, even defect strain fields-depends on the equation of state at the target ( $T, P$ ). Sampling the isothermal-isobaric ensemble with a barostat lets the box find the volume (and, if allowed, shape) consistent with your potential at that thermodynamic point. Nosé-Hoover-style barostats achieve this by extending the Hamiltonian with a box degree of freedom whose inertia and damping set how quickly pressure fluctuations relax. If you started from an arbitrary box, or from a stoichiometric lattice and then introduced vacancies, the instantaneous stress is wrong; running NPT moves you onto the correct density manifold so the subsequent transport lives on the right phonon spectrum.

Once the box volume is right, you still need to distribute energy correctly across modes and kill transients introduced by the barostat. An NVT stage uses a thermostat to equilibrate kinetic and potential energy and to damp the slow ring-down of the barostat mode without further changing the volume. In extendedensemble language, you now freeze the box d.o.f. and only let the thermal reservoir exchange heat with the atoms. Properly chosen thermostat parameters (Nosé-Hoover chains with coupling times longer than the fastest vibrational periods, not shorter) produce the canonical distribution while minimally distorting short-time dynamics. Practically, this step lets defect complexes, local strains around $V_0 / \mathrm{Ce}^{3+}$, and any residual acoustic oscillations relax at the correct density, leaving you with a stationary microstructure and a temperature field that is flat up to equilibrium fluctuations.

The Green-Kubo measurement then requires unperturbed Hamiltonian dynamics. The fluctuationdissipation theorem that turns equilibrium current-current correlations into transport coefficients assumes that the only evolution is generated by the system Hamiltonian. Thermostats, even gentle ones, add non-Hamiltonian terms that act as hidden sources and sinks for energy and momentum and imprint their own correlation times on the dynamics. If you keep a thermostat on while measuring the heat flux, its feedback loop can bleed or inject energy on the same time scales as the heat-current autocorrelation you intend to integrate, biasing both the amplitude and the long-time tail. Switching to NVE removes this interference: the only currents are those the interatomic forces generate, and the time correlation of the microscopic heat flux reflects the true equilibrium fluctuations of your model. That is why a good NVE check includes verifying tiny energy drift (mesh and integrator tolerances set so the drift over the measurement window is far below statistical noise) and removing the center-of-mass momentum so a wandering box doesn't masquerade as a convective contribution to the flux.

There are edge cases and practicalities worth knowing. If you were doing rNEMD instead of GK, you would still prepare with NPT $\rightarrow$ NVT, but during the driven steady state you would thermostat only the exchange slabs (or weakly at the ends) and leave the bulk measurement region unthermostatted so that $\nabla T$ and the flux reflect internal dynamics. In anisotropic phases one might use a Parrinello-Rahman barostat or an anisotropic NPT to let the cell shape relax; fluorite ceria is cubic, so isotropic NPT is sufficient and more stable. And while some thermostats (e.g., stochastic Langevin at vanishing coupling) can be shown to leave GK results unchanged in the infinite-time limit, in practice finite trajectories and finite coupling make NVE the clean, defensible choice for production.


# 5. Green-Kubo (equilibrium) methodology

The Green-Kubo expression is the linear-response identity that turns spontaneous equilibrium fluctuations of the microscopic energy current into the macroscopic transport coefficient. For a 3-D solid with essentially isotropic transport, the scalar conductivity is the one-third trace of the tensor,

$$
k=\frac{1}{3 V k_{\mathrm{B}} T^2} \int_0^{\infty}\langle\mathrm{J}(0) \cdot \mathrm{J}(t)\rangle d t=\frac{1}{V k_{\mathrm{B}} T^2} \int_0^{\infty} \frac{1}{3} \sum_{\alpha=x, y, z}\left\langle J_\alpha(0) J_\alpha(t)\right\rangle d t
$$


Here $\mathbf{J}(t)$ is the total heat (energy) flux constructed from a consistent per-atom energy and virial (including reciprocal-space pieces when PPPM/Ewald is used). The equality follows from the fluctuationdissipation theorem under two assumptions: stationarity (correlations depend only on time differences) and ergodicity (long time averages equal ensemble averages). That is why the practical recipe is to run a long, unthermostatted NVE trajectory at the target ( $T, V$ ), compute the heat-current autocorrelation function (HCACF) from the time series $\mathbf{J}(t)$, and integrate it forward in time. In discrete MD, you sample $\mathbf{J}$ every $\Delta t_s$ (often a small multiple of the integration step), estimate $C_{J J}\left(n \Delta t_s\right)$ with many overlapping time origins to reduce variance, and form a trapezoidal running integral. The Green-Kubo "answer" is the plateau of that running integral: once the true correlation has decayed to zero, further integration only adds noise and should not systematically change $k$.


Two statistical facts shape how you get a trustworthy plateau. First, the HCACF is a strongly autocorrelated estimator: successive values share many of the same trajectory segments when you use multiple origins, and the heat flux itself has nontrivial memory due to phonon lifetimes. The relevant figure of merit is the integrated autocorrelation time $\tau_{\text {int }}$, which sets the effective number of independent samples $N_{\text {eff }} \approx T_{\text {run }} / \tau_{\text {int }}$. Confidence intervals must scale as $N_{\text {eff }}^{-1 / 2}$, not $T_{\text {run }}^{-1 / 2}$ naively. Second, real solids exhibit hydrodynamic long-time tails-slow algebraic decays in correlation functions-so the HCACF can cross zero and meander near it with a noise floor that is not purely white. A robust way to declare the plateau is to use a windowed integral: integrate the HCACF only up to a cutoff $t^{\star}$ that grows with $\tau_{\text {int }}$ (e.g., a Sokal-type rule $t^{\star}=c \tau_{\text {int }}$ with $c \sim 5$ ), and estimate the uncertainty by blocking (coarse-graining the time series until adjacent blocks decorrelate) or by fitting the variance of the windowed integral over independently seeded segments. This deliberately trades a small bias (not integrating the infinitesimal true tail) for a large variance reduction; for MD-length runs the bias is negligible compared to the noise you avoid.


System size and run length introduce physical, not just statistical, systematics. In a periodic crystal, the simulation box discretizes the phonon spectrum and excludes the longest wavelengths; those long-wavelength acoustic modes often carry heat efficiently. As a result, the GK conductivity in too small a box can be depressed because important low- $q$ contributions are missing. The remedy is simple: use a large supercell chosen for the physics (here, also to make $x N_{\mathrm{Ce}}$ an integer) and perform at least one size check by stretching a box dimension substantially and confirming that the plateau does not move outside the statistical error. Finite time effects are the other side of the coin: if your trajectory isn't long compared to $\tau_{\text {int }}$, the HCACF may not reach a clean noise floor, and the running integral will exhibit a slow drift rather than a flat region. In practice you counter both issues by budgeting correlation time (multiple long shards per state with independent seeds), sampling J frequently enough to resolve the fast initial decay, removing center-of-mass momentum to avoid spurious convective pieces, and demonstrating that the windowed integral $k\left(t^{\star}\right)$ and its error bar are stable when you vary $t^{\star}$ over a sensible range. When those checks pass-stable plateau, consistent estimates across shards and modest size variations-you have a conductivity that reflects the true linear-response behavior of your model, with an uncertainty that honestly accounts for correlated noise.


# 6. rNEMD (Müller-Plathe) as cross-check/backup

Reverse nonequilibrium MD realizes Fourier's law by creating a steady, divergence-free energy current inside an otherwise Hamiltonian system and then reading off the proportionality constant from the emergent temperature profile. The Müller-Plathe construction does this without external fields: one partitions the periodically replicated box into slabs along a chosen transport axis, designates one slab "cold" and another "hot," and at fixed intervals swaps the velocities of the hottest particle in the cold slab with the coldest particle in the hot slab. Each swap transfers a precisely known amount of kinetic energy from cold to hot, thereby imposing a net heat flux $q$ opposite to the direction of the swaps. After a transient, the system reaches a statistically steady state in which the time-averaged temperature becomes piecewise linear along the axis away from the exchange slabs. In the linear-response regime, the slope of that profile is constant in the bulk region and Fourier's law applies pointwise, so the conductivity follows from

$$
k=-\frac{q}{\nabla T} .
$$


Because $q$ is controlled by the known cumulative energy you have shuffled divided by the cross-sectional area and the elapsed time, this route produces a direct, dimensionally transparent estimate of $k$ that depends on the same interatomic potential as Green-Kubo but carries different finite-size and sampling systematics.

Two theoretical conditions ensure that the number you extract is the linear-response $k$, not a nonlinear transport coefficient. First, the driving must be weak: the imposed $q$ should be small enough that the temperature field is only slightly perturbed from the target $T$, local structure and phonon spectra remain those of the equilibrium state, and the measured gradient is independent of the swap rate. Operationally, you test this by halving and doubling the swap frequency and verifying that both $\nabla T$ and the inferred $k$ remain unchanged within noise. Second, the measurement must exclude the exchange slabs themselves, which act as localized sources/sinks and host sharp kinetic discontinuities; the constitutive relation is valid in the interior, where energy transport is mediated solely by interatomic forces. In practice you discard a few slabs near each exchange region and fit a line to the remaining temperature profile. A flat residual and a high $R^2$ over a broad central window are the signatures that you are reading a genuine bulk gradient.

Thermostatting requires care. The elegance of the MP scheme is that it keeps the bulk dynamics microcanonical; adding a thermostat everywhere would superimpose an artificial energy sink/source whose feedback time scales can interfere with the steady state you are trying to measure. The clean setup thermostats, at most, thin buffer zones near the heat-source and heat-sink slabs (or uses very weak end thermostats) to pin the mean temperature, while leaving the bulk measurement region unthermostatted. Momentum conservation is handled separately: you periodically remove total center-of-mass velocity so that a small net drift does not masquerade as a convective heat current.

Finite-size effects look different than in GK. Along the transport axis you need enough slabs to resolve a smooth gradient and to place the excluded source/sink regions well away from the fit window; across the axis you should ensure a cross-section large enough that the temperature field is one-dimensional on average and that boundary scattering from periodic images does not dominate. Since the flux is imposed discretely in time, you also check that the temperature profile is linear across the fit window (no curvature), that the two halves of the box yield the same slope with opposite sign when folded about the center, and that profiles accumulated over independent seeds and time blocks superpose. When those checks pass and the rate-dependence test is flat, the rNEMD value provides an independent, internally consistent conductivity. Agreement with GK within combined uncertainties is a strong indicator that both your long-time correlation handling (GK) and your steady-state gradient control (rNEMD) are in the linear regime and that any residual bias is below your quoted error bars.



# 7. Vacancy Generation and Sampling Theory

When you say "make $x$ oxygen vacancies," you are really choosing a point in a high-dimensional configurational ensemble. In a macroscopic crystal, vacancy positions and the accompanying $\mathrm{Ce}^{3+}$ reductions are not independent coin flips: elastic fields, electrostatics, and the local chemistry correlate them. A single MD supercell cannot represent that full distribution; it provides one draw from it. The goal, then, is to draw in a way that is unbiased for bulk trends, reproducible for science, and relaxed enough that short-range correlations the model would naturally produce are allowed to form.

Two constraints come first: exact composition and periodicity. For target $x$, the number of O vacancies must be an integer, so you pick supercell sizes with $x N_{\mathrm{Ge}} \in \mathbb{Z}$. Under periodic boundary conditions, each vacancy interacts with its images; at small $x$ those elastic/electrostatic image effects are weak if the box is large, but they are not zero. That's one reason to avoid pathological placements (e.g., two vacancies on adjacent sites across a periodic face) and to report the minimum image separation statistics you actually obtain.

A good baseline sampling procedure is "blue-noise" rather than purely Poisson. Purely random draws can produce arbitrarily close pairs that exaggerate clustering artifacts and produce unusually strong local softening; forbidding very small separations forces the initial configuration to explore the representative part of configuration space first, while still allowing realistic short-range association to develop during relaxation. In practice you sample $O$ sites without replacement under a minimum-distance constraint $r_{\min }$ chosen small enough not to forbid known near-neighbor pairing, but large enough to avoid doublets separated by a single lattice hop. You then let NPT $\rightarrow$ NVT equilibration do the physics: local strains and electrostatics pull defects toward slightly lower-energy motifs if the potential prefers them. This two-stage approach separates "don't start in a pathological corner" from "let the model express its correlations," and it yields placements that are both reproducible (because you fix RNG seeds) and representative (because you didn't hard-wire a pattern). If you want to reduce configuration bias further, you repeat the entire draw-relax-measure pipeline for a handful of seeds and average $k$ across them; that converts configuration dispersion into an explicit component of your uncertainty budget.

Charge compensation is not optional bookkeeping; it is what makes the periodic electrostatics well-posed and the local chemistry sensible. In reduced ceria, each $V_0$ is compensated by two $\mathrm{Ce}^{3+}$ centers. In a classical fixed-charge model, there are no itinerant electrons or explicit small polarons, so you represent the redox change by species relabeling: two nearby $\mathrm{Ce}^{4+}$ become $\mathrm{Ce}^{3+}$ with their own short-range parameters and charges. The simplest and most defensible assignment rule at insertion is geometrical: for each vacancy, pick the two nearest cerium cations and convert them. That choice has three virtues. It reflects the electrostatic and elastic logic that compensation occurs locally; it reproduces the common association of $\mathrm{Ce}^{3+}$ with the vacancy's nearest shells, which your force field should also prefer after relaxation; and it is deterministic given a seed, which keeps the workflow auditable. If two or more Ce are equidistant, you break ties with a fixed convention (e.g., lexicographic order of indices) so that repeated runs are bit-reproducible.

There are alternatives, and it helps to understand why they are less attractive here. You could "optimize" the $\mathrm{Ce}^{3+}$ choices by minimizing an electrostatic proxy or by doing a short 0 K relaxation after testing multiple assignments. That adds cost and, paradoxically, bakes in more model bias up front: you're privileging one microstate before the proper finite- $T$ relaxation has had a chance to explore nearby basins. You could also smear reduction by giving all Ce a fractional charge; that preserves neutrality but deletes the discrete local softening and size change that make $\mathrm{Ce}^{3+}-V_{\mathrm{O}}$ complexes strong phonon scatterers. For transport, those local contrasts are the signal.

Because placement is part of the science, you should measure and report what you created. After NVT, compute the vacancy-vacancy pair distribution (or just a histogram of minimum separations), the distribution of $\mathrm{Ce}^{3+}-V_0$ distances, and the fraction of vacancies with both compensating $\mathrm{Ce}^{3+}$ in the first coordination shell. If your blue-noise $r_{\text {min }}$ was set too large, these diagnostics will show a deficit of firstshell association even after relaxation; if it was reasonable, relaxation will repopulate the expected shells and the statistics will be seed-insensitive. Those same diagnostics let you justify how many seeds you needed: if $k$ and the local-defect statistics are stable across two or three independent draws at a given $x$, further averaging buys little.

Finally, remember self-averaging. At larger supercells the sample contains more independent local environments, and configuration variance in extensive observables shrinks as $1 / \sqrt{N}$. That is why a single well-equilibrated $10 \times 10 \times 10$ cell at $x=0.03$ can be nearly as informative as multiple smaller cells with many seeds; you can pick a mixed strategy-one large cell per ( $T, x$ ) with two seeds-to keep both finitesize and configuration noise under control. All told, the combination of integer-exact stoichiometry, bluenoise vacancy sampling, nearest-neighbor $\mathrm{Ce}^{3+}$ assignment, and short, finite- $T$ relaxation gives you a clean, physics-aware way to prepare defect ensembles that your transport calculation can trust.



# 8. Integrators, thermostats, and barostats

For transport you care about two things above all: (i) that the trajectories you average over are faithful to the model Hamiltonian (no hidden numerical heating or artificial damping on the time scales of the correlations), and (ii) that the equilibrium ensembles used to prepare those trajectories are sampled without distorting the very fluctuations that set $k$. The integration/thermostat/barostat choices are how you guarantee both.

Velocity-Verlet is the default because it is symplectic and time-reversible, which means it nearly conserves a "shadow" Hamiltonian even over very long runs. The practical stability bound is set by the highest vibrational frequency in the system, here the stiff $\mathrm{O}-\mathrm{Ce}$ stretches in fluorite. If $\omega_{\text {max }}$ is the largest angular frequency, you want $\omega_{\max } \Delta t \ll 1$; a conservative rule of thumb is $\Delta t \lesssim \frac{\pi}{10 \omega_{\max }}$. With Buckingham+Coulomb oxides that typically lands you at $0.5-1.0$ fs. Larger timesteps do not merely add noise-they bias both the virial (hence pressure) and the short-time current correlations that feed the HCACF, so the error couples directly to your observable. The way you police this is empirical but quantitative: measure total-energy drift in an NVE sanity run over the same horizon you'll use for GK (drift should be orders of magnitude below the stochastic spread of the running integral), and track temperature and virial stationarity in NVT/NPT (no slow trends after equilibration). Removing center-ofmass momentum at intervals avoids a tiny integration-error-driven drift masquerading as a convective heat current.


Thermostats are only for preparation (NVT) and possibly the ends of an rNEMD setup; GK production is NVE. Among thermostats, Nosé-Hoover (in its modern Martyna-Klein-Tuckerman chain formulation) is preferred because it samples the canonical ensemble exactly for harmonic systems and does not inject white noise that would blur fast correlations. The single tuning knob you actually feel is the coupling time $\tau_T$ (equivalently the thermostat "mass" $Q$ ), which should be matched to the system's intrinsic periods: too tight (small $\tau_T$ ) and the thermostat will fight the natural energy sloshing among modes, flattening fluctuations you intend to measure later; too loose and it simply takes forever to equilibrate. A safe target is $\tau_T$ longer than the fastest optical period and comparable to several acoustic periods-e.g., $\tau_T \sim 0.1-0.5 \mathrm{ps}$ for oxides-with a short NH chain (length 3-5) to improve ergodicity without overconstraining dynamics. You can verify you're in the right regime by checking that velocity distributions are Maxwellian and that time autocorrelations of kinetic temperature decay on physical, not thermostat, time scales. Stochastic thermostats (Langevin) are fine for rough pre-equilibration, but avoid them near the GK stage because their random forces leave a fingerprint in the very correlations you integrate.


Pressure control is analogous but slower. Barostats add cell degrees of freedom so the system samples the isothermal-isobaric ensemble and finds the correct density before you measure transport. ParrinelloRahman lets the full $3 \times 3$ cell matrix fluctuate and is essential for anisotropic or low-symmetry phases; it also couples strongly to acoustic modes and can ring if you set the mass parameter too light. Fluorite ceria is cubic and elastically isotropic at the scales we care about, so an isotropic Nosé-Hoover (MTK) barostat is both sufficient and more stable: only the volume breathes, the shape is fixed, and spurious shear-cell resonances are avoided. Choose a barostat coupling time $\tau_P$ comfortably longer than $\tau_T$-on the order of 1-5 ps is typical-so the box does not chase instantaneous pressure spikes from individual phonons. As with the thermostat, you validate by watching that the instantaneous volume/pressure execute stationary fluctuations around a mean with no secular drift once equilibrated, and by confirming that repeating NPT with different $\tau_P$ in a sensible range does not change the final average density within noise.


All three layers interact with the long-range solver and cutoff choices: if PPPM accuracy or real-space neighbor updates are too loose, you will see energy/virial "texture" that no integrator can fix. The final check, before any production, is to run a short NVE at your chosen $\Delta t$ and mesh/cutoff settings, confirm minimal energy drift and stable virial/temperature statistics, then snapshot that microstate as the starting point for your GK measurement. That sequence ensures the only "physics" in your heat-flux autocorrelations comes from the potential you meant to study, not from the numerics driving the trajectory.



# 9. Data analysis and uncertainty quantification

What you measure in GK is not a number but a time series $\mathrm{J}(t)$; the conductivity is a functional of its second-order statistics. The estimator lives or dies on how you turn that series into a heat-current autocorrelation function (HCACF) and when you decide to stop integrating it.

The HCACF for a stationary trajectory is $C_{J J}(t)=\frac{1}{3} \sum_\alpha\left\langle J_\alpha(0) J_\alpha(t)\right\rangle$. In discrete MD you sample J every $\Delta t_s$ and estimate $C_{J J}\left(n \Delta t_s\right)$ by averaging over many time origins. Using overlapping origins is essential: if the total production length is $T$ and you only used non-overlapping blocks of width $n \Delta t_s$, you would throw away $\mathcal{O}(n)$ out of every $\mathcal{O}(n)$ samples. With overlaps, every point in the record contributes to every lag up to the lag's window, which lowers variance at fixed $T$ without biasing the mean because stationarity makes all origins equivalent. At very short lags the number of available origins is essentially $T / \Delta t_s$; at longer lags it rolls off linearly as $T-n \Delta t_s$. That "triangular" availability is one reason long-lag estimates are intrinsically noisy; it is also why windowing the integral is statistically sound.

The conductivity estimator is a windowed integral of the HCACF. The formal identity,

$$
k=\frac{1}{V k_B T^2} \int_0^{\infty} C_{J J}(t) d t,
$$

is replaced in practice by

$$
\hat{k}\left(t^{\star}\right)=\frac{1}{V k_B T^2} \int_0^{t^*} \widehat{C}_{J J}(t) d t,
$$


with a finite cutoff $t^{\star}$. Past some time the true correlation has effectively died; beyond that point, adding more area only integrates noise. If you push $t^{\star}$ too far, the partial sum performs a random walk whose variance grows with the window, and your "plateau" drifts. If you cut too early, you incur bias by omitting a real tail. The cure is to tie $t^{\star}$ to the integrated autocorrelation time (IAT) of the HCACF (or of $J$ itself): choose $t^{\star}=c \tau_{\text {int }}$ with a conservative constant $c$ (values in the $5-10$ range work well), and report $\hat{k}\left(t^{\star}\right)$ together with an error bar that reflects the residual correlation. This is the Sokal-window logic: accept a tiny, controlled bias in exchange for a large variance reduction, and make $c$ big enough that the bias is smaller than the quoted uncertainty. As a sanity check, plot $\hat{k}(t)$ with its estimated standard error as a function of $t$; a genuine plateau is a broad time interval over which both the mean and the error bar are flat within noise. If widening $t^{\star}$ a bit changes $\hat{k}$ by less than one sigma, you are in the safe zone.

Estimating uncertainties requires acknowledging that successive samples are not independent. Let $\tau_{\text {int }}$ be the IAT of the underlying process that drives the variance of $\hat{k}$; the effective number of independent samples in a record of length $T$ is $N_{\text {eff }} \approx T / \tau_{\text {int }}$. Any confidence interval must scale like $1 / \sqrt{N_{\text {eff }}}$, not $1 / \sqrt{T / \Delta t_s}$. There are two practical, consistent routes. The first is blocking (a.k.a. batch means): recursively coarse-grain the J time series (or the HCACF), doubling the block size until adjacent blocks decorrelate, and then compute the variance of block-averaged contributions to the windowed integral. The second is segmenting and resampling: split the production into independent shards (different RNG seeds or well-separated time chunks), compute $\hat{k}\left(t^*\right)$ per shard, and use their sample variance (or a simple bootstrap over shards) to obtain the standard error. Both methods are compatible; in fact, using blocking within each shard and treating the shard means as i.i.d. is clean and robust. Either way, you should see the estimated error collapse roughly as $\frac{1}{\sqrt{M}}$ when you increase the number of statistically independent shards $M$ at fixed shard length.

Two additional choices keep the estimator honest. First, sample $\mathbf{J}$ fast enough to resolve the initial decay of $C_{J J}$ (set $\Delta t_s$ on the order of the MD step or a small multiple), but do not oversample so aggressively that you drown the integral in highly correlated points; oversampling does not reduce variance unless you increase $T$. Second, treat tensor components symmetrically: for an isotropic crystal the $x x, y y$, and $z z$ contributions are identically distributed. Average their windowed integrals (not their raw HCACFs) and use cross-component scatter as part of the uncertainty assessment; this both reduces variance and serves as an internal consistency check.

All the usual QA plots tell you whether your numbers deserve to be trusted: the HCACF should be smooth and rapidly decaying at short times, then hover near zero with symmetric noise; the running integral should rise quickly, bend over, and enter a broad, statistically flat region; the plateau value and its error should agree across seeds and modest variations of $t^{\star}$; and the shard-wise estimates should be mutually consistent within the predicted $1 / \sqrt{N_{\text {eff }}}$ scaling. If any of those pictures misbehave, the remedy is not a cleverer statistic but more decorrelated data: longer NVE time, more seeds, or a larger box to tame finitesize low- $q$ artifacts.


# 10. Validation targets and pass/fail criteria

Before you ask the system any transport question, you have to prove you're simulating the right material at the right thermodynamic point with numerics that don't leak into the physics. The quickest way to do that is to validate the equation of state and a single, stoichiometric conductivity point-then lock all knobs.

Start with the equation of state and structure. At the target ( $T, P$ ) you should recover the lattice parameter and density that the force field was trained or validated against; if those are off, every phonon velocity and elastic constant is off. A clean procedure is: (i) run short NPTs at a few nearby pressures, fit a Birch-Murnaghan (or linear) EOS, and read off the bulk modulus $B$ and equilibrium volume $V_0$; (ii) freeze the box at the NPT average and run NVT to check that the mean stress is consistent with $P$ and that the pair distribution (Ce-O peak positions) matches expectations; (ii) verify that an NVE sanity run at this volume conserves energy (drift << the stochastic spread of the GK integral over the same time horizon). As concrete gates: lattice parameter and density within $\sim 1-2 \%$ of the potential's published values at ( $T, P$ ); $B$ within ~10-15\%; PPPM accuracy tight enough that halving the k -space tolerance and realspace cutoff shift the mean pressure by less than the equilibrium fluctuations; no secular drift in temperature or virial in NVT once equilibrated. Passing these means "the box is the right size, the modes live on the right elastic background, and the numerics are quiet."

Only then touch stoichiometric $k$. Pick a single temperature (e.g., 300 K or your project's baseline), and perform a full GK run on a large cell. Establish time and size convergence explicitly: multiple long NVE shards with independent seeds that deliver a stable Green-Kubo plateau and overlapping confidence intervals; one elongated cell (e.g., double a box length along one axis) that leaves $k$ unchanged within error. Cross-check with a gentle rNEMD run at the same state to ensure the steady-gradient estimate agrees with GK within combined uncertainties. Gates here are procedural rather than a single "magic number": (i) the windowed GK integral exhibits a broad, flat plateau whose value is insensitive to modest changes of the cutoff; (ii) shard-wise $k$ estimates scatter as $1 / \sqrt{M}$ when you increase the number of independent shards $M$; (iii) doubling the total correlation time changes $k$ by less than one quoted sigma; (iv) elongating the cell, or tightening PPPM accuracy by an order of magnitude, moves the mean by less than the statistical error bar. Finally, the absolute value should sit inside the bracket implied by the model's own pedigree and the literature for dense, defect-free $\mathrm{CeO}_2$ : ab-initio/BTE "clean crystal" values and high-density experimental values define that bracket, and a well-resolved MD result with your chosen potential should land in it. If it doesn't, you don't "tune" the workflow-you stop and reconsider the potential or the ensemble preparation.

Passing both stages gives you permission to explore $x>0$ with the same settings frozen. Failing either is diagnostic: EOS/structure failures point to a force-field or barostat problem; GK/rNEMD failures point to insufficient sampling, noisy long-range electrostatics, or a flux-assembly mistake. The discipline here is what makes the later $k(T, x)$ numbers defensible: once you've shown the base crystal and the measurement pipeline behave, any systematic changes you see with vacancies are far more likely to be real physics of the model rather than artifacts of numerics.


# 11. Simulation Design and Grid Selection


Two design choices control whether your numbers mean what you think they mean: how you size the supercell so composition is exact under PBC, and how you spend finite MD time across ( $T, x$ ) so each datum clears a quality bar.

For fluorite $\mathrm{CeO}_2$ there are $Z=4$ Ce cations per cubic conventional cell. An $n \times n \times n$ supercell therefore has $N_{\mathrm{Ce}}=4 n^3$ and $N_{\mathrm{O}}=8 n^3$. At vacancy fraction $x$ the number of oxygen vacancies is $x N_{\text {Ce }}$ (one $V_{\mathrm{O}}$ per two reduced Ce), so you must choose $n$ such that $x 4 n^3 \in \mathbb{Z}$. This is not pedantry: composition errors of a few tenths of a percent are large enough to move $k$ at the level of your intended uncertainty. It is why "round" boxes like $10 \times 10 \times 10$ are attractive- $n=10$ gives $N_{\mathrm{Ce}}=4000$, so common choices $x=0.03$ and 0.06 correspond to exactly 120 and 240 vacancies, while keeping the linear size ( $\sim 5.4 \mathrm{~nm}$ for $a \approx 5.4 \AA$ ) large enough that low- $q$ acoustic modes are reasonably represented. If a particular $x$ forces an awkward $n$, you either slightly adjust $x$ to the nearest exactly realizable value or move to the next larger $n$ so you don't give up physical box size just to hit an integer count.


The second design axis is the ( $T, x$ ) grid versus your correlation-time budget. Green-Kubo error bars shrink like $1 / \sqrt{N_{\text {eff }}}$ with $N_{\text {eff }} \approx T_{\text {run }} / \tau_{\text {int }}$, so what you are truly allocating is independent correlation time per state. That favors a "small, high-quality map" over a broad, shallow sweep: pick a minimal set of ( $T, x$ ) that spans what downstream models actually need, and hit each point hard enough to deliver a stable plateau and a transparent confidence interval. A pragmatic pattern is three temperatures (e.g., a low, mid, and high point relevant to your fuel/SHI use case) and three compositions ( $x=$ 0 , $x_{\text {moderate }}, x_{\text {high }}$ ), with exactly the same supercell and PPPM tolerances across the grid. For each state, accumulate total NVE sampling as several long shards with independent RNG seeds; two seeds are often enough if each shard is genuinely long relative to $\tau_{\text {int }}$, and they also help expose run-to-run variability. Keep one designated state (typically $T$ mid, $x=0$ ) for all convergence demonstrationselongated cell for a finite-size check, doubled total sampling for the GK plateau stability test-and then freeze those settings for the rest of the grid.

Finally, remember that configuration averaging and finite-size control compete for the same wall-clock. For vacancy cases, one larger box with a modest number of seeds often self-averages better than many tiny boxes: as $n$ grows, more independent local environments fit in one sample and configuration noise in extensive observables drops roughly as $1 / \sqrt{N}$. That is the quiet reason the $10 \times 10 \times 10$ "style box" works well here-it satisfies commensurability exactly for common $x$, is big enough to keep longwavelength artifacts in check, and lets you spend time where it buys statistical power: long, reproducible NVE correlation sampling at each carefully chosen ( $T, x$ ).



# 12. Reproducibility engineering

For this project, "reproducible" means two levels of guarantee. First, statistical reproducibility: if someone repeats your workflow with the same force field and settings, they obtain $k(T, x)$ consistent within quoted confidence intervals. Second, where feasible, bitwise reproducibility: given the exact same inputs and platform, they can regenerate identical files (manifests, HCACFs, plateau estimates). The plan below ties every moving part of your pipeline to explicit, recorded state so results are auditable and rerunnable.

Start with seeds, then freeze everything that can randomize the dynamics. Vacancy placement uses a master RNG seed; record it and the resulting vacancy indices and $\mathrm{Ce}^{3+}$ assignments so the geometry is reproduceable without RNGs. For dynamics, give each phase its own seed namespace: NPT seed, NVT seed, and one seed per NVE shard. Never reuse seeds across states. Store them in a run manifest alongside the thermostat/barostat coupling times, integrator step $\Delta t$, sampling stride for $\mathbf{J}(t)$, PPPM tolerances, cutoffs, neighbor skin, and any COM-momentum removal period. Once the stoichiometric validation passes, freeze these knobs for the full ( $T, x$ ) grid. That "frozen protocol" is half of reproducibility: your later changes are science (vacancies, temperature), not numerics.

Provenance is the other half. Every dataset should carry a content-addressed manifest: Git commit hashes of your input generators and analysis code; LAMMPS version and build flags; the exact potential files' checksums; the JSON configuration used to stage runs (target $T, P$, supercell, counts of $V_{\mathrm{O}}$ and Ce ${ }^{3+}$, seeds); and platform info (CPU model, MPI count). Write this manifest at launch and copy it into the output directory; embed it again inside your results file (e.g., NetCDF/JSON) so it can't be separated from the numbers. For inputs you do not control (POTCAR-like files, parameter tables), store the file content hash and a frozen local copy under external/ with a checksum so the exact parameters are recoverable years later.

Bitwise stability in MD is tricky because parallel reductions and neighbor-list ordering can make trajectories order-nondeterministic under MPI. Your goal isn't identical atom positions but identical analysis inputs. You can get most of the way by pinning LAMMPS version, disabling "optimistic" neighbor reordering, fixing PPPM mesh sizes explicitly, and keeping shard MPI layouts constant. Export the raw per-time-step heat flux time series in double precision with a fixed sampling stride; this makes the post-processing stage deterministic. Prefer structured binary (e.g., NetCDF/HDF5) over text to avoid whitespace and rounding noise; log the SHA-256 of the flux series array after writing so you can verify integrity later.Provenance is the other half. Every dataset should carry a content-addressed manifest: Git commit hashes of your input generators and analysis code; LAMMPS version and build flags; the exact potential files' checksums; the JSON configuration used to stage runs (target $T, P$, supercell, counts of $V_{\mathrm{O}}$ and Ce ${ }^{3+}$, seeds); and platform info (CPU model, MPI count). Write this manifest at launch and copy it into the output directory; embed it again inside your results file (e.g., NetCDF/JSON) so it can't be separated from the numbers. For inputs you do not control (POTCAR-like files, parameter tables), store the file content hash and a frozen local copy under external/ with a checksum so the exact parameters are recoverable years later.

Bitwise stability in MD is tricky because parallel reductions and neighbor-list ordering can make trajectories order-nondeterministic under MPI. Your goal isn't identical atom positions but identical analysis inputs. You can get most of the way by pinning LAMMPS version, disabling "optimistic" neighbor reordering, fixing PPPM mesh sizes explicitly, and keeping shard MPI layouts constant. Export the raw per-time-step heat flux time series in double precision with a fixed sampling stride; this makes the post-processing stage deterministic. Prefer structured binary (e.g., NetCDF/HDF5) over text to avoid whitespace and rounding noise; log the SHA-256 of the flux series array after writing so you can verify integrity later.


Deterministic post-processing is non-negotiable. Structure your analysis as a pure function
(flux series, control JSON) $\longrightarrow$ ( $\hat{k}$, CI, diagnostics)
with no hidden randomness and no filesystem side effects beyond declared outputs. The control JSON must declare the windowing rule (e.g., Sokal constant c), blocking scheme, minimum block length, tensorcomponent averaging policy, trapezoid vs Simpson integration, demeaning conventions, and how you estimate the integrated autocorrelation time. Record the exact values used at runtime in the result file's metadata, along with hashes of the input flux arrays. Unit-test this analysis on synthetic signals (OrnsteinUhlenbeck with known IAT; damped exponentials with known integrals) so regressions are caught automatically; add a "golden file" test where the same flux series must regenerate the same $k$ and Cl bit-for-bit.

File layout should separate inputs, transients, and products. A clean pattern is:


```bash
project/
  potentials/   # frozen copies + checksums
  configs/      # JSON control files (one per (T,x) state)
  geometry/     # POSCAR/structure + vacancy & Ce3+ maps
  runs/
    T300_x0.00/
      manifest.json
      npt.log nvt.log
      nve_shard_000/flux.bin  flux.sha256  stdout.log  lammps.input
      nve_shard_001/...
    T300_x0.06/...
  analysis/
    T300_x0.00/
      control.json
      hcacf.npy  kcumu.npy
      k_result.json  # k, CI, window, IAT, diagnostics, hashes
      plots/...
```


Each `manifest.json` is created before a run starts and contains seeds, versions, tolerances, and geometry hashes; each analysis `control.json` is archived with the outputs it produced.

Finally, make reruns cheap and transparent. Provide a single "make-it-again" entry point that, given a state directory, (i) validates that the current code hash matches the recorded one (or records a "code drift" note), (ii) verifies all input hashes, (iii) replays the analysis from flux to $k$, and (iv) compares numerical outputs against the archived result within a strict tolerance. If you later improve the analysis (say, a better IAT estimator), bump a semantic version in the results, keep the old outputs, and show both side-by-side with provenance so downstream users can decide which to adopt.

Do these things, and anyone-including future you-can take your folder, verify that the physics inputs are exactly those you claim, regenerate the numbers, and understand any discrepancy as either statistical scatter or an explicitly recorded change in code or configuration. That's real reproducibility, not vibes.


# 13. Known limitations and interpretive boundaries

What we compute is the conductivity of a classical, fixed-charge model of $\mathrm{CeO}_{2-x}$ under periodic boundary conditions. That statement already encodes several limits. Classical nuclei assign $k_{\mathrm{B}} T$ to every vibrational mode, so high-frequency optical branches remain over-populated as $T$ falls. Absolute $k$ at low temperature is therefore biased-typically high-because the specific heat is too large and quantum freeze-out is absent. At moderate and high $T$ (where many modes are thermally populated) the trends with vacancy fraction $x$ and with $T$ are far more robust; that is the regime we emphasize. If one needs low- $T$ absolutes, quantum corrections (e.g., heat-capacity rescaling from a reference phonon DOS) can be applied, but those are model-dependent and outside scope.

Force-field physics is the other boundary. A Buckingham + Coulomb potential with explicit $\mathrm{Ce}^{3+} / \mathrm{Ce}^{4+}$ captures ionic structure, elasticity, and some defect energetics, but it does not include electron-phonon coupling, small-polaron formation and hopping, or explicit polarization unless a shell/induction model is used. Vacancies in reduced ceria are experimentally tied to $\mathrm{Ce}^{3+}$ polarons; in our model that chemistry is represented by changing species labels and short-range parameters, not by itinerant carriers. Consequently, any polaronic heat conduction or defect-assisted electronic contribution to $k$ is absent, and defect energetics are only as good as the fit. This is exactly why we validate structure and a stoichiometric $k$ point up front and why we frame claims narrowly: the project targets lattice conductivity trends from mass/force-constant/strain disorder, not a full transport decomposition.

Methodological limits remain even with a perfect potential. Periodic supercells discretize the phonon spectrum and truncate the longest mean-free-paths; GK estimates can be suppressed if the box is too small, while rNEMD can curve the temperature profile if the imposed flux is too strong or if source/sink slabs contaminate the bulk. Our size checks, gentle driving, and plateau diagnostics are designed to keep us in the linear, bulk regime, but they do not reconstruct the true long-wavelength tail of an infinite crystal. Long-range electrostatics are treated with PPPM under "tin-foil" boundary conditions; that is the correct bulk limit for an ionic solid, yet it is still an ideal bulk: no grain boundaries, no porosity, no irradiation-induced microcracks. Comparisons to experiments on pellets or ion-tracked material must acknowledge extrinsic scattering (porosity, boundaries, second phases) that our perfect crystal omits; mapping our bulk numbers to pellets typically requires independent micro-structural models (e.g., Maxwell-Eucken-type porosity corrections), which we do not attempt here.

Statistically, Green-Kubo error bars account for finite sampling and correlation, not for model systematics. Two uncertainty classes should be kept separate in any figure or table: (i) the statistical Cl from HCACF windowing/blocking and seed scatter; and (ii) a model band reflecting plausible variation across vetted potentials (fixed-charge vs polarizable; alternative $\mathrm{Ce}^{3+} / \mathrm{Ce}^{4+}$ parameterizations). Our conclusions about the direction and relative magnitude of $k$ changes with $x$ rely on statistical resolution inside a single potential family; any statement about absolute $k$ beyond the validated point is conditional on that family.

Finally, interpretation. From MD we get an intensive number $k(T, x)$ with honest error bars, plus diagnostics of convergence. We do not get, without additional analysis, a unique modal breakdown, a mean-free-path spectrum, or a "mechanism" beyond the qualitative statement that added point-defect disorder shortens phonon lifetimes. If such decompositions are needed, they require separate tools (spectral energy density, normal-mode analysis, or ab-initio BTE) and bring their own assumptions. Within these boundaries, the right way to read our results is: the model's lattice $k$ at moderate/high $T$ decreases monotonically with vacancy fraction in a way that is numerically converged and reproducible; the absolutes are anchored at one stoichiometric validation point; and any extension to microstructured materials or low- $T$ quantum behavior should be treated as a separate modeling layer, not inferred from our MD alone.