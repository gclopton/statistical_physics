

# Outline

**_Phase 0 — One-time setup (Day 1–3)_**

1.) **Project repo & layout:** Create a single Git repo with three top-level dirs: `sim/` (all input templates + SLURM scripts), `analysis/` (event detection, survival, RDFs), and `results/` (figures, CSV summaries). Inside `sim/`, add `configs/` (ReaxFF parameter sets A/B; QEq/ACKS2 settings), `prep/` (packing + equilibration), and `prod/` (replica runners).
2.) **Software pins:** Pin LAMMPS (ReaxFF + ACKS2 build), Python 3.11, and the following minimal stack: `numpy`, `pandas`, `MDAnalysis`, `networkx`, `lifelines` (Kaplan-Meier + logrank), `scipy`, `matplotlib`. Avoid anything heavier unless needed.
3.) **Randomness policy:** Reserve a fixed seed list (e.g., 1..200) and commit it to `sim/seedlist.txt`. The same replica index uses the same seed across all four variants.
    


**_Phase 1 — Prepare the common starting configuration (Week 1)_**

4.) **Pack once:** Generate a periodic EC:LiPF₆ liquid with target salt fraction using your preferred packer (Packmol or built-in tools). Keep the molecule counts and box vectors in `prep/packed.data` (or LAMMPS `data.*`).
5.) **Equilibrate once:** Run a short NPT → NVT workflow (few hundred ps total) at the target (T, ρ, composition) using a generic (non-reactive) set or directly with ReaxFF(A,QEq)—pick one and stick with it for this _equilibration only_. Save `common_start.{data, restart}` and a matching `common_start.xyz`. Archive in `prep/`. This is the only structure all variants will inherit.
6.) **Freeze the coordinates:** Strip velocities. This file is the immutable starting point for every replica in every variant.
    

**_Phase 2 — Validate integrator/neighbor settings once (Week 1)_**

7.) **Timestep sanity:** On the frozen box, do 50–100 ps NVE with your candidate sub-fs timestep and neighbor settings; target negligible drift (e.g., |ΔE| < 10⁻⁴ eV/atom/ps). Adjust once. Record the chosen timestep and neighbor skin in `sim/configs/run_constants.in`.
8.) **Thermostat constants:** Choose one thermostat (e.g., Nosé–Hoover or Langevin) and fix its parameters globally. Commit them to the same constants file. You will not touch these again.
    

**_Phase 3 — Define the four model variants (Week 2)_**

9.) **Variant files.** Create four tiny include files: `A_QEq.in`, `A_ACKS2.in`, `B_QEq.in`, `B_ACKS2.in`. Each only sets (i) the parameter file path and (ii) the charge-equilibration style/tolerances. Everything else comes from `run_constants.in`.
    
10.) **Charge solver parity.** Match QEq vs. ACKS2 solver tolerances (relative/absolute) so the linear solves have comparable accuracy. Note them in comments.
    

**_Phase 4 — Replica generators (Week 2)_**

11.) **Velocity assignment.** A simple script writes per-replica LAMMPS inputs that (a) read the common coordinates, (b) set the replica’s seed from `seedlist.txt`, (c) do a brief “settle” (a few ps) under the variant’s Ewald/charges/thermostat, then (d) start production. No other differences allowed.
    
12.) **SLURM array.** One job array per variant, parameterized by `REPLICA_ID`. Keep per-replica outputs in `prod/<variant>/<replica_id>/`.
    

**_Phase 5 — Production runs (Weeks 3–6)_**

13.) **Right-sized sampling.** Aim for 40–60 replicas per variant as the default. If time is tight, 30 is a safe floor that still supports KM curves with censoring.
14.) **Window length.** Choose a single production window per temperature (e.g., 100–200 ps) that makes first events reasonably likely but not rare. Keep it identical across variants.
15.) **Writeout cadence.** Write coordinates every 100–200 fs and bond-order/charges every 10–20 fs. That’s sufficient temporal resolution for event detection without drowning in I/O.
16.) **Lightweight logs.** For each replica, write: bond-order time series (selected pairs only), per-step partial charges (Li, PF₆ atoms, carbonyl carbons/oxygens), and minimal thermo.
    

**_Phase 6 — Event detection (Weeks 4–7; begins as soon as first replicas finish)_**

17.) **Bond-order binarization.** Implement a two-threshold hysteresis for each candidate bond type (e.g., form if BO>0.30, break if BO<0.20, require ≥2–3 consecutive frames to confirm). Keep thresholds in a single YAML so you can tweak once if needed.
18.) **Define “first event.”** Scan time forward and apply a small library of rules, e.g.:  
    • EC ring opening = C–O(scission in carbonate ring) confirmed by bond break + new connectivity;  
    • Solvent deprotonation = O–H break + corresponding anion acceptor;  
    • PF₆⁻ dissociation = P–F break into PF₅ + F⁻.  
    The first rule to trigger sets the event type and time for that replica.
19.) **Post-event freeze.** After a trigger, freeze a 2–5 ps window and export a connectivity graph (from binarized BO) for product typing.
    

**_Phase 7 — Structural context (Weeks 5–8)_**

20.) **RDFs & coordinations.** Using pre-event segments only, compute `g(r)` and running coordinations for Li–O, Li–F, C–O. Pool replicas within a variant; also split by “event type eventually observed” when counts allow.
    
21.) **Li–F contact metric.** Extract Li–F distance time series; set a contact cutoff at the first minimum of Li–F `g(r)` measured from these same pre-event segments. Report contact frequency and lifetime distributions per variant.
    

**_Phase 8 — Survival & branching statistics (Weeks 6–9)_**

22.) **Censoring-aware timing.** For each variant, build Kaplan–Meier survival curves for “time-to-first-event,” compute medians (or 60th percentiles if median is censored) and bootstrap CIs. Compare variants by logrank tests.
23.) **Branching fractions.** Count first-event types per variant; compute multinomial CIs and test cross-variant differences (Fisher/χ²).
24.) **Effect sizes.** Report pairwise ratios of median times with bootstrap CIs; this gives a clean “how much faster/slower” reading.
    

**_Phase 9 — Minimal robustness checks (Weeks 8–10; short, targeted)_**

25.) **Hysteresis sensitivity.** Repeat event detection on ~10% of replicas with slightly shifted BO thresholds (±0.05) to confirm conclusions are not an artifact.
26.) **Timestep spot-check.** Re-run 5–10 replicas at Δt/1.25 to ensure identical first events and similar times within noise.  
27.) **Seed-swap sanity.** Swap seeds between two variants for a tiny subset to confirm shared stochasticity behaves as expected.  
    _(If schedule crunches, prioritize #25 only.)_
    

**_Phase 10 — Figures & narrative (Weeks 10–12)_**

28.) **One-screen dashboards.** Auto-generate, per variant: (i) KM curve with CIs, (ii) bar chart of first-event branching with CIs, (iii) Li–O/Li–F/C–O `g(r)` overlays, (iv) Li–F contact lifetime histograms.
29.) **Answer the question.** Collate a 2–3 page write-up: Does A vs. B and QEq vs. ACKS2 change (a) the identity of the first event, (b) timing distributions, or (c) Li–F contact statistics/structure? Where differences exist, state them with effect sizes and p-values; where not, assert robustness.
30.) **Repro pack.** Push final inputs, seeds, analysis scripts, and a `make reproduce` recipe that runs a 4×8-replica “miniature” of the full study for graders.
    

---

### Guardrails that keep this simple

- **No parameter tuning marathons.** You are _not_ refitting ReaxFF. Parameter sets A/B are taken as-is.
- **One thermostat, one timestep, one neighbor policy**—chosen once in Phase 2 and frozen.
- **Event rules stay small.** Three event categories only (ring opening, deprotonation, PF₆⁻ dissociation). If a rare path appears, log it as “other,” don’t branch the code.
- **Data economy.** Write only what the analysis uses: bond orders for relevant pairs, partial charges for Li/PF₆/carbonyl sites, and coordinates at moderate cadence.
    

### If you fall behind

- Cut replicas to 30 per variant.
- Drop the seed-swap and timestep spot-check; keep the hysteresis sensitivity.
- If still tight, run A(QEq) vs A(ACKS2) first; add B’s two variants only if time allows.
    

This plan stays faithful to your design (one common box, four controlled variants, many short replicas, simple rule-based event detection, censoring-aware timing) while avoiding rabbit holes. It also front-loads the one-time choices so production/analysis can proceed in parallel mid-semester.


# 1. Setting Up the Project


### 1) Project repo & layout — provenance, comparability, and “configuration as data”

By separating `sim/`, `analysis/`, and `results/`, you force a one-way flow of information: inputs → computations → outputs. That directional flow is what makes a computational experiment auditable. Within `sim/`, the `configs/` directory acts like a parameter registry: a tiny, declarative layer that defines the _model levers_ (parameter set A vs. B; QEq vs. ACKS2) independently of run mechanics (thermostat, timestep, neighbor list). This decoupling is crucial for a fair A/B comparison: only the lever should change between variants, not hidden defaults in a monolithic input. Putting packing/equilibration in `prep/` and replicas in `prod/` makes the “single common starting configuration” a first-class asset rather than an accident of history. Conceptually, you are turning the MD workflow into a DAG with an immutable root node—exactly what you want when you later argue that differences arose from force-field choices, not from drifting preparation steps. Finally, `results/` being write-only from analysis code completes a provenance chain: anyone can regenerate figures from `sim/` plus code without touching knobs by hand.

### 2) Software pins — numerical identity and the “same binary, same math” principle

Reactive MD exposes you to two forms of non-repeatability: algorithmic nondeterminism and floating-point divergence. Pinning versions is your defense against both. LAMMPS builds matter for three reasons. First, QEq and ACKS2 live behind different solver implementations and linear-algebra backends; small version shifts can change solver tolerances, preconditioners, or parallel reduction order. Second, parallel MD is sensitive to non-associative floating-point sums; different MPI/OpenMP builds (and even different compiler flags) can reorder reductions and subtly alter trajectories. Third, ReaxFF kernels sometimes change default neighbor/skin or charge-solve stopping criteria across releases. By pinning a specific LAMMPS commit (and recording compiler, MPI, and key CMake options), you guarantee the _mechanics_ of time integration and charge equilibration are identical across variants and re-runs.

On the Python side, pinning a lean stack isn’t about asceticism; it’s about statistical and graph operations you can justify. `MDAnalysis` gives you trajectory I/O and RDFs without inventing your own topology parser. `networkx` provides graph connectivity for product typing so your “what is a molecule?” decision is explicit and testable (binarized bond order → edges → components). `lifelines` gives you survival analysis primitives that handle right-censoring correctly, which is essential when some replicas don’t react within your window. Keeping the stack small reduces dependency churn (which reduces “works on my machine” variance) and makes it feasible to record _exact_ versions in a single `requirements.txt` or lockfile. In short: pinning creates numerical identity; numerical identity turns “close enough” MD into a controlled experiment.

### 3) Randomness policy — common random numbers (CRN) as a variance-reduction device

Your project relies on comparing distributions between model variants, not on any single trajectory. The biggest free lunch in such A/B designs is _common random numbers_: drive each pair of matched replicas (same index across variants) with the same stochastic inputs so that exogenous noise cancels when you take differences. In MD, “randomness” enters at least twice: initial velocity assignment and any stochastic thermostat/barostat. If replica 17 always uses seed 17 for velocity draws and for the thermostat’s random stream, then—_holding the integrator/neighbor policy fixed_—differences in first-event times between A(QEq) and A(ACKS2) for replica 17 reflect the force-field lever more than the Monte Carlo phase of randomness. Statistically, you are inducing positive correlation between paired outcomes so that Var(Δ) = Var(X−Y) shrinks relative to Var(X)+Var(Y), tightening confidence intervals and increasing power without extra replicas.

There are two practical caveats worth understanding. First, CRN is only helpful if everything else is truly like-for-like; if one variant quietly changes neighbor rebuild frequency or thermostat parameters, the shared seed no longer aligns the underlying noise processes, and you lose the variance reduction you thought you had. That is why Phase 2 freezes integrator/neighbor/thermostat constants in a single include file used by all variants. Second, parallel nondeterminism can desynchronize random streams if the code advances them conditionally on per-rank events; this is rare in LAMMPS for the common thermostats when configured consistently, but it’s another reason to fix MPI task counts and relevant settings across runs and to document them alongside seeds. Committing a literal `sim/seedlist.txt` implements the policy as data: the pairing is reproducible, reviewable, and audit-ready.

---

Put together, Phase 0 institutes three pillars. The repo layout encodes experimental invariants and separates levers from mechanics; the software pins enforce numerical identity so “same inputs → same outputs” actually holds on HPC; and the structured randomness policy turns noisy MD into a more efficient A/B test via common random numbers. That’s the theoretical scaffolding that lets every later phase make strong causal statements about QEq vs. ACKS2 and parameter set A vs. B—without getting dragged into confounds you didn’t intend.



# 2 Preparing the Common Starting Configuration


This step allows us to manufacture a single, defensible piece of “starting material” for _all_ experiments: one equilibrated microstate of bulk EC–LiPF₆ that encodes only the thermodynamic state you care about (T, ρ, composition), and none of the modeling choices you’ll later compare. The three steps—pack, equilibrate, freeze—each serve a specific theoretical purpose.

Packing is where you translate macroscopic targets into a microscopic ensemble member. Choosing molecule counts fixes composition; choosing box vectors fixes number density. For a liquid, periodic boundary conditions and a minimum-image cutoff imply a practical lower bound on system size: the box length should exceed roughly twice the largest real-space interaction range (neighbor skin + cutoff, and any charge short-range splitting) so a molecule does not “see” its own periodic image in the first coordination shell. That is not just a numerical nicety; if the box is too small, first-shell structure (and therefore early reactive encounters) is biased by self-correlation. Packing tools simply generate a high-overlap, non-equilibrium configuration consistent with these counts and edges. You retain _just_ the integers (N_EC, N_LiPF6) and the cell matrix because those are the invariants that define the state point; everything else—the disordered arrangement—will be re-shaped by thermodynamics in the next step.

Equilibration with NPT→NVT is how you let that jammed arrangement relax into a typical liquid at your state point. The NPT leg allows the volume to find the correct mean at the target pressure, removing residual packing stress; the barostat couples to the slowest degrees of freedom (cell fluctuations), so you give it time constants long enough to avoid ringing but short enough to converge within hundreds of picoseconds. For an isotropic liquid, an isotropic barostat is preferable so you do not imprint spurious anisotropy. The subsequent NVT leg removes the barostat (and its slow volume mode) and lets fast local structure—radial distributions, ion pairing, carbonate orientations—settle at fixed T and ρ. Conceptually, you are sampling from the canonical ensemble conditioned on the volume that NPT found; practically, you are producing a snapshot whose pair structure (g(r)), pressure fluctuations, and potential energy per molecule have plateaued to stationary statistics.

Which force field to use during this single equilibration matters less than it might seem because you will _not_ carry over charges, velocities, or thermostat/barostat states—only coordinates. Using a non-reactive surrogate is attractive because it eliminates the risk of chemistry happening “by accident” in the prep stage. Using ReaxFF(A,QEq) is acceptable too if you keep the window short and cool enough that reactions are vanishingly unlikely; the benefit is that short-range packing may reflect the polarizability that ReaxFF will later apply. Either way, the theoretical requirement is that prep imposes _no_ model-specific information on the production experiments beyond the one thing it must set—the liquid’s density at your chosen state point. That is why you “pick one and stick with it”: if you equilibrated once with A+QEq and once with B+ACKS2 and then mixed starting structures, you would erase the causal link between a variant and the outcomes it produces.

Freezing the coordinates is the step that converts an equilibrated _trajectory_ into a reusable _initial condition_. Stripping velocities and thermostat history breaks dynamical continuity so that each replica can draw fresh Maxwell–Boltzmann velocities from a known seed. In statistical terms, you are re-sampling initial momenta i.i.d. from the canonical momentum distribution while holding positions fixed, which is the cleanest way to enforce common-random-numbers pairing across variants later. Making the file “immutable” (and committing it to version control) enforces provenance: every downstream difference must flow from the levers you touch (parameter set; charge scheme), not from a creeping change in the starting microstate. You also save both a human-readable `.data/.xyz` and an engine-native `restart` because they serve different purposes: the former is an auditable record of particle identities and cell geometry; the latter guarantees exact binary fidelity if you ever need to resume prep diagnostics.

Put together: packing defines the state variables and avoids finite-size artifacts; NPT→NVT lets thermodynamics—not your packer—create a representative liquid while quarantining reactive noise; and freezing positions while discarding momenta creates a single, causal “root” for the entire experiment tree. That is the minimal theory you need to justify why one file in `prep/` can legitimately underwrite every result you report later.



# 3 Defining the four model variants

Phase 3 is where you turn "the levers we care about" into clean, auditable switches-and nothing else. The theory to keep front-of-mind is that partial charges in ReaxFF are not parameters; they are latent variables solved at every MD step by minimizing an electrochemical potential. Changing either the parameter set (A vs. B) or the charge-equilibration scheme (QEq vs. ACKS2) perturbs that per-step optimization and therefore the forces that drive the dynamics. Your job here is to isolate those perturbations so any downstream difference is causally attributable to the lever, not to collateral changes.

Start with the include files. In a reactive force field, the parameter set encodes bond-order exponents, dissociation energies, van der Waals terms, and the atomwise "electronegativity"/"hardness" coefficients that seed charge equilibration. The charge scheme governs how those coefficients interact through longrange electrostatics. If A_QEq. in and friends only supply a path to the parameter file and the charge style/tolerances, you've factored the experiment correctly: all integrator constants, neighbor settings, thermostats, and output cadence live in a separate, invariant run_constants. in . Conceptually, you've built a small controlled laboratory in which you flip exactly one switch per variant: "same mechanics, different constitutive assumption."

Under the hood, both QEq and ACKS2 solve a linear system each timestep, but their physics differ. Classic QEq (Rappé-Goddard-Stuart) equalizes an effective electrochemical potential by minimizing a quadratic form in the charges: the objective combines atomwise terms (electronegativity $\chi$ and hardness $\eta$ ) with pairwise Coulomb couplings. The result is a symmetric positive-definite system $A \mathrm{q}=\mathrm{b}$ with one global constraint $\sum_i q_i=Q_{\text {tot }}$ (usually zero), enforced via a Lagrange multiplier. Because the Coulomb block is dense in principle, implementations trade exactness for speed with distance-based screening or Ewaldstyle splittings, but the mathematics is "every atom talks to every atom," which encourages long-range charge transfer in polar media.

ACKS2 modifies that picture by introducing atom-condensed kernels and local constraints that effectively penalize unphysical charge sloshing and enforce better "nearsightedness" of polarization. Mathematically, you still solve a linear system each step, but now with an augmented KKT-type structure: the charge variables are coupled to constraint multipliers that encode locality and conservation beyond the single $\sum_i q_i$ constraint. The immediate consequence is twofold. First, conditioning can differ markedly between QEq and ACKS2 for the same geometry, which has algorithmic implications for iterative solvers. Second, the solution can differ even at identical $\chi$, $\eta$, and geometry, typically yielding shorter-ranged polarization with ACKS2. In a chemistry like EC + LiPF ${ }_{\mathrm{G}}$, where early events are sensitive to nucleophilicity and Li-F attraction, that difference can tilt which bond-order thresholds are crossed first.

This is why "charge solver parity" matters. The charge solve is an inner iteration inside every MD step; the forces you integrate depend on the converged charges, and the energy conservation in short NVE checks silently assumes the solve is tight enough to be on the true manifold. If you ask QEq to converge to a residual of $10^{-8}$ in $\sim 50$ iterations but allow ACKS2 to stop at $10^{-5}$ in $\sim 20$, you have made two changes: the physics and the numerical accuracy. The fair comparison is to match both the stopping criteria (relative/absolute residuals) and the iteration budget (so neither scheme is handicapped) and to document them in the include files. Practically, that means picking a relative tolerance that gives stable per-step energy and pressure in short NVE tests (your Phase 2 check) and using it verbatim in both schemes. It also means pinning the same maximum iterations and the same preconditioning choice (e.g., Jacobi) where the engine allows it. The goal is simple: when you see a $20-30 \%$ shift in a median time-toevent, you want to argue it flowed from the constitutive model (global vs. localized polarization), not from one solver leaving a few meV of electrostatic work on the table.

A few subtle points are worth making explicit in comments within each variant file. First, enforce the same total-charge constraint across variants and record it; even a stray $\pm 1 \mathrm{e}$ on a periodic box changes everything. Second, fix the charge bounds (if the code supports them) identically; asymmetric clipping will bias reactive encounters. Third, keep the charge-solve cadence identical-solve every timestep, not every n steps in one variant and every step in another. Fourth, fix any long-range electrostatics splittings that the charge kernel relies on (cutoffs, damping parameters): these settings are part of the definition of "QEq/ACKS2 as used here," not mere numerics. Finally, log the per-step or per-run residual norms for a small subset of replicas during dry runs; it's cheap insurance that the parity you think you have is actually being enforced during production.

Put together, Phase 3 is a statics-and-numerics hygiene exercise in service of clean causal inference. The include files isolate the levers, and the matched solver tolerances keep you from accidentally adding a "numerical accuracy" lever you never meant to study. Once those are in place, any divergence you later observe in first-event identity, branching, or timing can be argued on physical grounds-how global versus localized polarization reshapes early EC chemistry-rather than on solver bookkeeping.



# 4 Replica Generators

Phase 4 is where we turn a single equilibrated microstate into many statistically interpretable experiments. The two moving parts-how you mint each replica's initial conditions and how you fire them off on the cluster-both rest on simple but important theory.

Start with velocity assignment. In classical MD, once positions are fixed, the "right" way to start dynamics at temperature $T$ is to draw momenta from the Maxwell-Boltzmann distribution. That's not negotiable: it's the unique momentum marginal of the canonical ensemble, and it ensures the ensemble of trajectories you generate samples the diversity of early encounters (which bond happens to be stretched; which ion pair is approaching) without embedding hand-crafted bias. The seed you use to draw those momenta is the control knob that implements common random numbers: replica $i$ across $\mathrm{A} / \mathrm{QEq}, \mathrm{A} / \mathrm{ACKS2}, \mathrm{~B} / \mathrm{QEq}$, B/ACKS2 should see the same initial velocity field. That induces positive correlation between matched outcomes, which reduces variance in cross-variant differences and makes your logrank tests and effect sizes tighter for the same number of runs.

The "brief settle" under each variant's electrostatics is not cosmetic; it projects you onto the variant's charge manifold before you start timing chemistry. In ReaxFF, partial charges are latent variables obtained by solving a linear system each step. If you were to launch production at $t=0$ with velocities drawn but with charges inherited from some other scheme or from an uninitialized state, your first few femtoseconds would include both real dynamics and numerical relaxation of the charge solver. A short settle-just a few picoseconds with the chosen charge scheme, long-range electrostatics, and thermostat in place-lets charges, short-range polarization, and any tiny virial mismatches equilibrate while you're not counting time-to-event. You want the "clock" to start once the system is on the physical attractor defined by that variant, not while it's still sliding into it.

A few practical consequences fall straight out of this theory. First, the settle window should be long enough to bring charges and fast local structure to stationarity (tens of ps is overkill; a few ps is typically enough for charge solvers), but short enough that reactive events remain negligibly probable at your $T$. Second, if you use a stochastic thermostat (e.g., Langevin), give it its own RNG stream tied to the replica seed so the noise realization is paired across variants; if you use a deterministic thermostat (e.g., NoséHoover), the only randomness is in the initial velocities, which is exactly what you want. Third, never let the settle stage alter anything but charges and short-range packing: don't change neighbor settings, timestep, or thermostat constants between settle and production, or you'll smuggle in extra differences.

Now to the SLURM array. Statistically, your study hinges on many short, independent trials; computationally, that's an embarrassingly parallel workload. A job array is the native way to express "run the same program with index $i$ from 1 to $N$." Mapping `SLURM_ARRAY_TASK_ID` $\rightarrow$ `REPLICA_ID` $\rightarrow$ seed gives you three virtues at once. First, isolation: each task writes into `prod/<variant>/<replica_id>`/, so failures (node preemption, disk quota hiccups) can be retried without contaminating others. Second, provenance: because every directory contains the index, seed, and variant, your analysis code can reconstruct the experimental design from filenames alone. Third, idempotence: re-submitting an array range only repeats the missing replicas, not the whole set, which is essential when you're chasing right-censored runs to get cleaner Kaplan-Meier curves.

There's also a subtle reproducibility point worth surfacing. In parallel MD, floating-point reductions are not strictly associative, so changing task geometry (MPI ranks/threads) can nudge trajectories. That's harmless for distributional conclusions (you're not claiming bitwise determinism), but it can wreck the "common random numbers" trick if one variant runs with 8 ranks and another with 16 . The fix is simple: pin the same MPI/OMP layout, fix the neighbor build frequency, and log them per replica. If a job fails and must be restarted, never reuse the same seed while resuming midway through a trajectory-that would splice two unrelated random streams. Either (a) restart from a checkpoint that also preserves RNG state (ideal), or (b) mark the run as failed and rerun from $t=0$ with the same seed for clean pairing.

Finally, the outputs you write per replica should reflect the questions you'll ask. You don't need full charges and bond orders for every pair; you need (i) coordinates at moderate cadence for RDFs, (ii) bondorder traces for the handful of bonds that define your event rules, and (iii) partial charges for $\mathrm{Li}, \mathrm{PF}_6$, and carbonate sites to make QEq vs. ACKS2 effects explicit. Tying that writeout to the replica directory-and dropping a tiny manifest.json with the git commit, LAMMPS version, variant, seed, task geometry, and start time-closes the provenance loop. At that point, your "replica generator" isn't just a convenience script; it's a reproducible factory for i.i.d. draws from the ensemble you've defined, ready for survival analysis and branching statistics without hidden degrees of freedom.



# 5 Production Runs


Phase 5 is where you turn careful design into statistically credible evidence. The four knobs here-replica count, window length, writeout cadence, and what you log-are all about controlling variance, avoiding bias, and preserving enough temporal detail to classify first events without drowning the filesystem.
"Right-sized sampling" is about power, not machismo. For time-to-event data, the precision of a Kaplan-Meier (KM) estimate is driven by the number of events, not just the number of runs. Greenwood's formula makes the point: the standard error on the survival curve and on the median time shrinks roughly like $1 / \sqrt{d}$, where $d$ is the number of observed first events. If the window you choose yields events in $\sim 50-$ 70\% of replicas, then 40-60 runs per variant typically deliver 20-40 events-enough to give tight medians (or 60th percentiles when medians are censored) and meaningful logrank comparisons. Thirty per variant is a pragmatic floor that still stabilizes KM curves when some fraction right-censors. The variance-reduction you engineered with common random numbers (same seed across variants) further tightens differences between variants, so you get more statistical bite per replica than naive independent sampling would suggest.

The production "window length" must create administrative censoring that is identical across variants; that's what makes cross-variant survival comparisons causal rather than confounded by immortal-time bias. Too short, and almost everything censors, inflating uncertainty and reducing power. Too long, and you start sampling secondary chemistry, violating the endpoint definition ("first event in an early window") and consuming compute that doesn't change the answer. The sweet spot is where first events are neither rare nor saturated—often 100-200 ps at moderate $T$ for this chemistry, but you should fix a single window per temperature a priori and stick to it so the censoring mechanism is non-informative and identical across variants. If you explore more than one $T$, stratify: one window per $T$, same across all four variants.


Temporal "writeout cadence" is set by the physics of what you're detecting. Bond breaking and forming in ReaxFF happens on tens of femtoseconds; partial-charge relaxation is even faster but smooth between steps once the charge solver is tight. To avoid aliasing an event (e.g., a transient scission that dips above and below a bond-order threshold within a few frames), you want bond-order and charge records roughly an order of magnitude finer than typical transition durations. Sampling bond order and charges every 1020 fs gives you multiple points across the onset and clear hysteresis to suppress flicker. Coordinates evolve more slowly on the scale that matters for RDFs and coordination numbers, so 100-200 fs snapshots are plenty to recover smooth $g(r)$ and running coordinations without writing terabytes. Think Nyquist: sample fast signals fast and slow signals slow; don't oversample everything.

"Lightweight logs" are about sufficiency and provenance. You only need the bond-order time series for the handful of bonds that define your event rules (ring $\mathrm{C}-\mathrm{O}, \mathrm{O}-\mathrm{H}$ for deprotonation, $\mathrm{P}-\mathrm{F}$ for $\mathrm{PF}_6^{-}$breakup). Logging all pairwise bond orders creates storage problems and makes threshold tuning brittle. Likewise, you don't need full charge vectors every step; track the actors whose polarization matters for early pathways-Li, the six $\mathrm{F}^{\prime}$ s on $\mathrm{PF}_6$, and the carbonate carbonyls-so you can explicitly demonstrate how QEq vs. ACKS2 shifts their distributions. Minimal thermo (T, P, E) verifies the integrator is healthy (no drift in NVE checks; stable thermostatting in production) and gives you a quick anomaly detector without parsing trajectories. One tiny but valuable addition is to record the per-step charge-solve residual (or a per-run summary): it proves your "charge solver parity" is actually enforced during production and explains away any micro-differences in energy conservation.

Two failure modes are worth heading off. First, informative censoring: do not terminate runs early when you "suspect nothing will happen"-that couples censoring to the outcome and biases KM. Always run every replica to the fixed window end unless a first event occurs (and even then, keep writing the post-event freeze you defined for typing, but don't count time beyond the trigger). Second, desynchronizing common random numbers: keep the same MPI/OMP layout, neighbor rebuild frequency, and thermostat style across all variants. If one variant subtly changes parallel reduction order or thermostat noise cadence, the shared seed stops inducing the positive correlation you're counting on for variance reduction.

If you keep those principles tight-events numerous enough to constrain KM, a single fixed window to make censoring non-informative, cadences matched to the signal bandwidth, and logs limited to sufficient statistics-then Phase 5 turns compute into clean estimators. What you gain is not just pretty curves but effect sizes and p-values you can defend: "with identical administrative censoring and matched numerics, variant X shifts the median time-to-ring-opening by $\mathrm{Y} \%$ and increases the $\mathrm{PF}_6^{-}$dissociation branch by Z percentage points," where the uncertainty is honest and the comparisons are fair.


# 6. Event detection (Weeks 4–7; begins as soon as first replicas finish)

Phase 6 is the bridge between raw MD signals and chemically interpretable outcomes. The core ideas are signal detection with hysteresis, graph-based interpretation of molecular structure, and survival-analysis hygiene so "first event" times are unbiased and comparable across variants. Each substep has a specific theoretical job.

Bond-order binarization turns a noisy, continuous proxy for bonding into a robust on/off state suitable for logic. ReaxFF's bond order evolves continuously with interatomic separation and local coordination; around the "gray zone" where a bond is weakening or forming, thermal motion produces rapid excursions that are not chemically meaningful. A two-threshold scheme (one threshold to declare "formed," a lower one to declare "broken," with a memory of the current state) implements hysteresis: once a bond is "on," transient dips do not immediately flip it off, and vice versa. This is the same logic used in Schmitt triggers and debounced digital inputs-the goal is to suppress flicker without low-pass filtering away real transitions. Requiring two or three consecutive frames above/below the relevant threshold is a minimum-duration constraint that further reduces false positives by enforcing temporal coherence on the decision. Storing all thresholds in a single YAML (and never hard-coding them in analysis functions) makes the decision surface explicit and auditable: if you later perform a sensitivity analysis (shift C-O "break" by $\pm 0.05$ in bond order, for example), you can rerun the detector without changing code paths, which is essential for demonstrating that conclusions are not an artifact of a single threshold choice.


Defining the "first event" is fundamentally a question of endpoint specification for time-to-event analysis. You are asking: at what time $t$ does an MD trajectory first cross from "no chemistry" into "chemistry has occurred" according to a small, pre-declared library of mechanistic patterns? The rules encode chemically interpretable motifs as Boolean combinations of binarized bonds and connectivity changes. For EC ring opening, the necessary condition is a carbonate C-O scission inside the five-membered ring; the sufficient condition adds topological evidence that the ring is no longer a cycle in the molecular graph. For solvent deprotonation, an $\mathrm{O}-\mathrm{H}$ break is necessary; pairing it with the appearance of a new strong $\mathrm{H}-$ acceptor association (e.g., $\mathrm{F}^{-}$or $\mathrm{O}^{-}$within a hydrogen-bond-like geometry) guards against mislabeling high-frequency O-H stretch excursions as chemistry. For $\mathrm{PF}_{\mathrm{e}}{ }^{-}$dissociation, a $\mathrm{P}-\mathrm{F}$ break is necessary; the sufficient condition recognizes the resulting $\mathrm{PF}_{\mathrm{s}}+\mathrm{F}^{-}$connectivity. The forward scan in time with "first rule to trigger wins" does two important things statistically: it prevents multiple counting (only one first event per replica) and it makes the censoring mechanism easy to reason about (if no rule triggers by the fixed window end, the observation is right-censored at that time). The rules must be declared before you look at outcomes to avoid outcome-dependent endpoint definitions, which would bias survival comparisons. Ordering only matters when two motifs become true within the same frame; in such "ties," you can break deterministically (e.g., prioritize ring opening over deprotonation), but you should also record that a tie occurred so you can check whether tie handling affects conclusions.


The post-event freeze is about separating detection from typing. Once a trigger fires, you stop the event clock but continue collecting a short window ( $2-5 \mathrm{ps}$ ) of high-frequency data to classify products and local environment under conditions where geometry has relaxed enough to be interpretable but before secondary chemistry accumulates. The classification step uses the binarized bond-order matrix to build a simple, unweighted molecular graph: atoms are nodes; "on" bonds are edges. Connected components in this graph correspond to molecules or transient ion pairs. Counting components and their labels gives you CO vs. $\mathrm{CO}_2$, small oligomers, or the presence of a free $\mathrm{F}^{-}$coordinated to $\mathrm{Li}^{+}$. This graph view is valuable because it is invariant to rigid motions and small coordinate jitters; it also makes explicit that "molecule identity" is an analysis choice (binarization thresholds plus graph connectivity), not something the MD engine hands you. By drawing Li-F contact lifetimes from distance traces with a cutoff anchored at the first minimum of the Li-F radial distribution function measured in pre-event segments, you tie your contact definition to the system's own structure rather than an arbitrary number. That keeps the LiF-precursor statistic physically meaningful and variant-comparable.

Three methodological cautions keep this stage honest. First, aliasing and temporal resolution: because bond breaking can occur on tens of femtoseconds, your 10-20 fs sampling for bond orders and charges ensures you won't miss brief but real transitions, while the hysteresis/multi-frame rule keeps those same brief transitions from being misread as chemistry if they're merely thermal fluctuations. Second, multiple-testing leakage: expanding the rule library mid-analysis increases the chance of finding "an event" earlier simply because you're testing more patterns; pre-registration of rule forms (and reporting any additions as exploratory) protects your time-to-event distributions from such drift. Third, robustness to thresholds: because bond order is model-dependent, you should plan a small, pre-specified sensitivity sweep ( $\pm 0.05$ around each threshold) on a subset of replicas. If classifications and trigger times are stable to these perturbations within expected jitter, you can argue that your detector is measuring chemistry, not tuning.


Finally, connect the detection layer back to survival analysis. The output of this phase is, for every replica, either a labeled event time $t_i$ and event type, or a right-censoring time $C_i$ with "no event." Because administrative censoring (the fixed window end) is identical across variants and independent of unobserved event times by design, Kaplan-Meier curves and logrank tests remain valid. Because you employed common random numbers across variants, paired differences in trigger times for matched replicas carry extra precision; even though KM/logrank do not directly use pairing, you can compute bootstrap CIs for median-time ratios and report them alongside the nonparametric tests to convey effect sizes. In short, Phase 6 converts continuous, noisy MD observables into discrete, causally interpretable endpoints with controlled error rates-so that the statistics you perform in the next phase reflect chemistry and model physics, not analysis artifacts.



# 7. Structural context

Phase 7 adds the "why did that pathway fire?" layer. We're not hunting new reactions here; we're quantifying the liquid structure that preceded the first event so we can argue mechanistic plausibility for differences between parameter sets and QEq vs. ACKS2.

Start with partial radial distribution functions. For a multicomponent liquid you want $g_{\alpha \beta}(r)$, the ratio of the observed pair density at separation $r$ to the density of an ideal gas at the same bulk number density. In practice, for species $\alpha$ and $\beta$ in a cubic cell with PBC and number densities $\rho_\alpha, \rho_\beta$,

$$
g_{\alpha \beta}(r)=\frac{1}{N_\alpha} \frac{1}{4 \pi r^2 \Delta r \rho_\beta} \sum_{i \in \alpha} \sum_{j \in \beta} 1\left(r \leq r_{i j}<r+\Delta r\right),
$$

with minimum-image distances $r_{i j}$. You'll compute $g_{\mathrm{LiO}}(r), g_{\mathrm{LiF}}(r), g_{\mathrm{CO}}(r)$. The physically relevant signal is in the first peak (first-shell association) and the first minimum (shell boundary). From $g_{\alpha \beta}(r)$ you get running coordinations,

$$
n_{\alpha \beta}(r)=4 \pi \rho_\beta \int_0^r g_{\alpha \beta}(s) s^2 d s,
$$

and you'll quote $n_{\alpha \beta}$ at the first minimum as a tidy "average neighbors" metric.


Only pre-event segments feed these statistics. That keeps us on equal footing across variants (no postreaction fragments contaminating structure) and ties structure to propensity rather than to consequences. There's a subtle sampling point: replicas with longer pre-event durations would dominate a simple frame-stacked average. To prevent length bias, compute each replica's $g(r)$ on its pre-event slice and then average the per-replica curves with equal weight (or weight by 1 /autocorrelation time if you've estimated it). Pool within a variant for your headline curves; when counts allow, stratify by "event that eventually occurred" to see, e.g., whether ring-openers exhibit higher pre-event Li-O coordination.

Binning and uncertainty should be honest and light-touch. Use $\boldsymbol{\Delta} \boldsymbol{r} \boldsymbol{\sim} \mathbf{0 . 0 2 - 0 . 0 5 ~} \mathrm{nm}$ up to half the box length (the PBC Nyquist limit). Smooth only with a narrow kernel if you must; never fit away the first minimum. For error bars, bootstrap across replicas (not frames) so Cls reflect between-replicate variability. If you computed autocorrelation times earlier, block-bootstrap within replicas before aggregating.

The Li-F contact metric converts structure into a kinetic proxy for LiF precursors. First, extract Li-F distance traces $r_{\text {LiF }}(t)$ from the same pre-event windows. Define a contact cutoff $r_c$ as the first minimum of $g_{\text {LiF }}(r)$ measured for that variant (or fix $r_c$ across variants to avoid fishing; if you vary it, record both choices). That choice is principled: the first minimum separates first-shell associates from the second shell. Now build a binary contact signal $C(t)=\mathbf{1}\left(r_{\text {LiF }}(t) \leq r_c\right)$. To avoid spurious blinking from fast vibrations, allow an "intermittency tolerance": treat gaps shorter than $\tau_{\text {gap }}$ (e.g., 20-40 fs, on the order of your bond-order sampling) as still "in contact." This is standard intermittent-contact bookkeeping and aligns with your hysteresis philosophy in Phase 6.

From $C(t)$ you want two families of descriptors. Frequencies are just time-fractions in contact over pre-event windows; report per-replica means and variant-level Cls (again, bootstrap replicas). Lifetimes are durations of maximal contiguous contact episodes; because your observation windows end at the first event (administrative censoring), many episodes will be right-censored. Treat those lifetimes exactly as you treat first-reaction times: use Kaplan-Meier on the episode durations to get a survival curve $S_{\text {contact }}(\tau)$ and a censored-aware median lifetime (or a percentile if the median censors). Comparing QEq vs. ACKS2 on these curves tells you whether localized polarization (ACKS2) shortens or lengthens Li-F residency times before chemistry happens, which is the mechanistic link to different LiF-precursor propensities.

Two safeguards make the structural story causal rather than decorative. First, pre-register the contact definition: ( $\boldsymbol{r}_c, \boldsymbol{\tau}_{\text {gap }}$ ) should be chosen once (e.g., $\boldsymbol{r}_c$ at the pooled variant-agnostic $\boldsymbol{g}_{\text {LiF }}$ minimum; $\boldsymbol{\tau}_{\text {gap }}$ =one or two frames) and then used everywhere; follow up with a small sensitivity sweep ( $\pm 0.02 \mathrm{~nm} ; \pm 1$ frame) on a subset to demonstrate robustness. Second, keep the estimator invariant across variants: same bin widths, same weighting scheme, same bootstrap. That way, if you report "ACKS2 reduces preevent Li-F coordination by 0.3 neighbors and halves the median Li-F contact lifetime," readers can attribute the shift to the charge model's shorter-ranged polarization rather than to analysis drift.

Finally, connect back to Phase 6 without double-counting. Structural conditioning ("among replicas that eventually ring-open...") is descriptive; it does not alter the event detector or the survival analysis. It's there to propose mechanisms: higher Li-O coordination might speed ring opening by stabilizing the departing alkoxide; longer Li-F contacts might correlate with earlier $\mathrm{PF}_6{ }^{-}$dissociation. Those are testable correlations, not post hoc redefinitions, and they help you explain why robustness (or sensitivity) appears in your A/B comparisons.



# 8. Survival & branching statistics (Weeks 6–9)


Phase 8 is where raw "first-event timestamps" become statistically defensible answers. The three tasksKM curves, branching tests, and effect sizes-map to three ideas: handling right-censoring correctly, comparing categorical outcomes across arms, and expressing "how much" in a way that is robust to censoring and model assumptions.

Start with censoring-aware timing. A Kaplan-Meier (KM) curve estimates the survival function $S(t)= \operatorname{Pr}(T>t)$ from incomplete data by taking the product of conditional survivals at each distinct event time. It is nonparametric, needs no hazard model, and is valid when censoring is administrative (your fixed window) and independent of the underlying event time-a condition you engineered by using the same window across variants. Standard errors come from Greenwood's formula; point summaries like the median time are read off the KM curve with confidence bands propagated accordingly. If the median is censored (e.g., <50\% events by the window end), switch to a higher percentile (60th or 70th) or, even better, report the restricted mean survival time (RMST), the area under $S(t)$ up to the common cutoff $\tau$ : $\operatorname{RMST}(\tau)=\int_0^\tau S(t) d t$. RMST is interpretable ("average event-free time within $\tau^{\prime \prime}$ ) and always estimable under censoring.

Comparisons across variants use the logrank test, which is a score test for equality of hazards that weights each event by the pooled risk set at that time. It is sensitive to sustained separations between KM curves and makes no distributional assumption beyond independent censoring. Two practical refinements fit your design: (i) stratified logrank by temperature (if you run multiple T's) or by "seed blocks" to honor the common-random-numbers pairing; stratification removes baseline differences while testing a common treatment effect across strata. (ii) If curves cross markedly, supplement with a weighted logrank (e.g., Tarone-Ware or Peto-Peto) or fallback to RMST differences, whose interpretation does not rely on proportional hazards. For uncertainty on medians, percentiles, or RMST, use a nonparametric bootstrap that resamples replicas as whole units (not frames) and preserves pairing/strata in each resample.

Branching fractions answer a different question: conditional on which event fired first, do the proportions differ by variant? Here, each replica contributes one categorical outcome ("ring opening," "deprotonation," " $\mathrm{PF}_6^{-}$dissociation," "none within window"). Pool counts per variant and first apply a $\chi^2$ test of homogeneity (adequate when all expected counts $\geq 5$ ). If any cell is sparse, use an exact test (Fisher-Freeman-Halton) or a permutation test that shuffles variant labels across replicas. For intervals on the multinomial proportions, don't rely on symmetric Wald intervals-they misbehave near 0 or 1 . Use a principled multinomial interval, e.g., a Dirichlet-posterior (Jeffreys prior) credible interval per category or Goodman's simultaneous confidence sets; report each category's estimate with a $95 \%$ interval and, where useful, the difference in proportions between two variants with a bootstrap Cl . When you eventually condition by event type ("among ring-openers, ..."), remember that those are descriptive slices; keep the primary test at the unconditional first-event level to avoid selection bias.

Effect sizes convert "different" into "how much." Ratios of medians are attractive-scale-free and easy to read (" A is 1.6 x slower than B to first ring-opening"). Compute them by taking the median from each variant's KM, forming the ratio, and bootstrapping replicas (stratified by pairing/temperature) to get a percentile Cl on the ratio. If medians are censored, switch to RMST differences or ratios: $\triangle \mathrm{RMST}= \operatorname{RMST}_A(\tau)-\operatorname{RMST}_B(\tau)$ with a bootstrap Cl . Because you engineered common random numbers, you can also compute paired effect sizes on the subset where both members of a pair experienced an event (e.g., differences in event times $T_i^{(A)}-T_i^{(B)}$ ), summarize with the Hodges-Lehmann estimator, and bootstrap pairs; present these as sensitivity checks alongside the KM/logrank results, not as replacements (since pairs with one censored member drop out).


A few guardrails keep the inference honest. Pre-register the time window $\tau$, the primary endpoint ("first of \{ring-opening, deprotonation, $\mathrm{PF}_6^{-}$dissociation\}"), and the primary comparison (logrank + RMST difference). Treat "no event" by $\tau$ strictly as right-censoring for timing and as its own category for branching; don't discard them. Use identical binning and sampling rules across variants so censoring remains non-informative by design. Finally, present estimates with intervals first, p-values second; the audience will care more about whether, say, ACKS2 shortens Li-F-driven first events by ~30-40\% (Cl width and direction) than about a solitary significance threshold.

If you do those things, the statistics will say exactly what you want them to say, cleanly: with matched numerics and identical administrative censoring, variant X changes the distribution of early reaction times and the mix of first-event pathways by quantifiable, uncertainty-qualified amounts-and those amounts are large (or small) enough to matter.



# 9 Minimal robustness checks (Weeks 8–10; short, targeted)

Phase 9 is your "trust but verify" pass. You are not trying to re-do the study; you are probing three places where small analysis or numerical choices could masquerade as physics. The aim is to spend a tiny amount of extra compute to show that your headline conclusions are insensitive to reasonable perturbations.

The hysteresis sensitivity check answers a simple identifiability question: are the first-event labels and times stable to small changes in how you turn a noisy, continuous bond-order signal into discrete chemistry? ReaxFF bond orders fluctuate on the same tens-of-femtoseconds timescale as genuine bond rearrangements. Your two-threshold scheme with a multi-frame requirement already suppresses flicker, but the thresholds themselves are analysis hyperparameters. By re-running the detector on a random $10 \%$ subsample with both thresholds shifted up and down by about 0.05 , you are performing a local sensitivity analysis on the decision boundary. What you want to see is that event identities almost never flip, and that trigger times jitter by at most a few frames relative to your sampling cadence. If the KM curves, median ratios, and branching fractions for that subsample remain within the original confidence bands, you can plausibly argue that your results reflect chemistry and model physics rather than a particular digit in a YAML file. If you do observe instability, it is still salvageable: report it, tighten the multiframe confirmation, or narrow the rule library, and make clear which conclusions are robust and which are threshold-dependent.


The timestep spot-check is a discretization test. Reactive trajectories convert a stiff, continuous-time Hamiltonian system (plus charge solves) into a discrete map via velocity-Verlet. If $\Delta t$ is too large, you distort barrier topography, add integration noise that looks like extra heat, and can even change the order in which closely competing events cross their thresholds. Running a handful of replicas at $\Delta \mathrm{t} / 1.25$ while holding everything else fixed diagnoses whether you are near that cliff. The criterion is qualitative but sharp: the first event for each replica should be the same motif as in the baseline, and event times should change by amounts commensurate with your sampling resolution and charge-solve tolerance, not by factors of two. You can also repeat your short NVE drift check at the smaller $\Delta \mathrm{t}$; if the original drift was already negligible and the first-event statistics do not move, you have strong evidence that your chosen $\Delta t$ resolves the fast modes that matter for early chemistry.

The seed-swap sanity test is about your variance-reduction design rather than physics. Common random numbers work by pairing the same stochastic inputs across variants so that uncontrollable noise cancels in differences. If the pairings are implemented correctly, swapping the seed lists between two variants for a tiny subset should leave the distribution of cross-variant differences unchanged. Practically, you take a few replica indices, reassign their seeds across the two variants, and rerun just those. If the difference in event times for these pairs is statistically indistinguishable from the original pairing distribution, the only thing you changed was which replica carried which random draw, confirming that your conclusions are not an artifact of an unlucky alignment of seeds. If the differences shift systematically after a swap, it suggests that some aspect of the workflow (for example, MPI layout, neighbor rebuild cadence, or thermostat RNG stream) is desynchronizing the intended pairing; that's a fixable engineering issue you can surface and correct.


All three checks are deliberately light: a tenth of the dataset for threshold sensitivity, a handful of replicas at a slightly smaller $\Delta \mathrm{t}$, and a tiny seed-swap subset. Together they triangulate the two dominant error classes—analysis hyperparameters and numerical integration—without expanding scope. If they pass, you can close the loop in your write-up with a single paragraph: thresholds within $\pm 0.05$, timestep within 20 25\%, and seed pairing choices do not materially alter first-event identities, survival separations, or branching proportions. That sentence is the difference between an interesting result and a trustworthy one.


# 10 Figures & narrative


Phase 10 is about turning raw estimators into a compact, defensible story-and making it trivially reproducible by anyone who didn't sit next to your cluster queue. The theory here is communication, inference hygiene, and provenance.

The "one-screen dashboards" exist to let a reader visually answer three statistical questions at a glance: how long systems remain event-free (the Kaplan-Meier panel), how the first mechanistic choices are distributed (the branching panel), and what structural environment plausibly pushed them there (the RDF and Li-F panels). A KM plot with pointwise confidence bands is the nonparametric summary of your time-to-event process under right-censoring; it should display the number at risk beneath the x -axis so the audience can see where uncertainty balloons, and it should mark administratively censored trajectories explicitly. That plot ought to be paired with a short text box reporting a percentile or RMST with its bootstrap interval and a logrank p-value, because visual separations can deceive. The branching panel is a probability simplex collapsed into bars: it should carry proper uncertainty (multinomial intervals or bootstrap CIs) and, crucially, a visible "none within window" category to keep readers from forgetting censoring when they interpret counts. RDF overlays carry physical meaning only if you preserve scale; avoid arbitrary smoothing, annotate the first peak and first minimum, and show the running coordination evaluated at that minimum as a small inset number so the structural implication is legible without calculus. The Li-F panel is your kinetic proxy; histograms of contact lifetimes communicate skew but are sensitive to binning, so include either a log-x alternative or the Kaplan-Meier survival of contact episodes with a censored-aware median. All four panels should share a common x-axis where appropriate, use the same color mapping for variants everywhere (so the reader's brain doesn't re-learn the legend per figure), and avoid dual axes. The design principle is "estimate + uncertainty first, decoration last."

Answering the question in two or three pages is an exercise in mapping those estimates onto clear causal claims under your design. The backbone is simple: state the endpoint (first of ring-opening, deprotonation, $\mathrm{PF}_{\mathrm{G}}{ }^{-}$- breakup within a fixed window), the censoring mechanism (identical administrative cutoff), and the two levers (parameter set; charge scheme). Then report, in sentences that bind numbers to meaning, whether those levers change event identity, timing distributions, or Li-F structure, with the appropriate effect size and uncertainty each time. Where medians are censored, switch to RMST differences and say so; where KM curves cross strongly, acknowledge proportional-hazards violations and lean on RMST or weighted logrank. When you claim "no difference," make it quantitative (e.g., "mediantime ratio $1.07,95 \% \mathrm{Cl} 0.90-1.26$; logrank $\mathrm{p}=0.41^{\prime \prime}$ ) rather than rhetorical. Tie structural shifts to mechanism without overreaching: "ACKS2 reduces pre-event Li-F coordination by ~0.3 neighbors and halves the censored-aware median Li-F contact lifetime; in the same variants, $\mathrm{PF}_6{ }^{-}$dissociation is the first event in $28 \%$ vs. $15 \%$ of replicas." That's an argument, not a vibe. Close with your minimal robustness checks: thresholds $\pm 0.05, \Delta t / 1.25$, and seed-swap did not move conclusions beyond the original Cls. You don't need a separate "limitations" section; one paragraph noting finite-size effects, threshold choices, and that ReaxFF is a model (hence your refrain about "higher-level validation") demonstrates epistemic humility without undercutting results.

The "repro pack" is not window dressing; it is your scientific guarantee. Conceptually, you're packaging three things: the specification of the experiment, the binaries and environment that realize it, and a small but complete workload that exercises every path. The specification lives in version-controlled text: the four tiny variant includes, the constants file that freezes integrator/neighbor/thermostat settings, the single "common_start" coordinates, and the YAML that defines event thresholds and contact cutoffs. The environment can be captured with a pinned LAMMPS build note (commit, compiler, MPI, CMake flags) and a Python lockfile; if you can provide a container recipe, even better, but a manifest with checksums is already meaningful. The workload is your make reproduce : a $4 \times 8$ array that reads seedlist.txt , regenerates the minimal outputs (coordinates, bond-orders, charges) to disk, runs the detector, recomputes KM/branching/RDF/contact metrics, and writes the same figures with a "miniature" watermark; if the grader's figures don't match yours within stochastic error, the pack is broken. Embed a tiny JSON manifest in every replica directory (git commit, LAMMPS version, variant, seed, MPI/OMP geometry, walltime start) so a third party can audit provenance without your help. With that in place, your narrative is not just persuasive; it is executable, and that is the modern standard for computational chemistry.