

# Phase 0 Project Repository Setup


``` title:"Phase 0 Directory Structure"
ec-reaxff-robustness/
├─ README.md
├─ LICENSE
├─ .gitignore
├─ Makefile
├─ CITATION.cff
├─ docs/
│  ├─ design_overview.md
│  └─ figures/               # (empty; diagrams about workflow/builds)
├─ env/
│  ├─ lammps_build_notes.md  # commit hash, cmake options, MPI/OMP layout
│  ├─ conda-environment.yml  # Python 3.11 + pinned deps
│  ├─ requirements.lock.txt  # exact versions (pip-compile or uv lock)
│  └─ .python-version        # 3.11.x (for pyenv)
├─ sim/
│  ├─ seedlist.txt           # 1..200 (one per line)
│  ├─ configs/
│  │  ├─ run_constants.in    # timestep, neighbor, thermostat, I/O cadences
│  │  ├─ A_QEq.in            # include: paramfile A + charge style/tolerances
│  │  ├─ A_ACKS2.in
│  │  ├─ B_QEq.in
│  │  ├─ B_ACKS2.in
│  │  ├─ params/
│  │  │  ├─ reaxff_A.ff      # parameter set A (placeholder)
│  │  │  └─ reaxff_B.ff      # parameter set B (placeholder)
│  │  └─ README.md           # what’s invariant vs. variant; parity notes
│  ├─ prep/
│  │  ├─ pack_ec_lipf6.py    # generate initial packed box at target composition
│  │  ├─ equilibrate_npt_nvt.in   # LAMMPS script used once to equilibrate
│  │  ├─ common_start/       # produced in Phase 1 (empty in Phase 0)
│  │  │  ├─ (placeholder)
│  │  └─ README.md           # state variables (T, ρ, x); box-size rationale
│  └─ prod/
│     ├─ templates/
│     │  ├─ settle_then_prod.in   # generic, pulls in run_constants + variant
│     │  └─ input_vars.template   # jinja/format template for per-replica inputs
│     ├─ slurm/
│     │  ├─ launch_array.sh       # one array per variant; maps TASK_ID→seed
│     │  └─ cluster_profile.md    # partition, nodes, recommended resources
│     └─ README.md
├─ analysis/
│  ├─ detectors/
│  │  ├─ event_rules.yaml     # BO thresholds, hysteresis, tie-break rules
│  │  └─ detect_events.py     # parses BO/charges to first-event labels/times
│  ├─ structure/
│  │  ├─ rdf_coord.py         # g(r) + running coordination (pre-event only)
│  │  └─ lif_contact.py       # Li–F contact frequency/lifetime metrics
│  ├─ survival/
│  │  ├─ km_curves.py         # KM, RMST, logrank
│  │  └─ bootstrap_utils.py   # replica-level bootstrap, paired resampling
│  ├─ viz/
│  │  ├─ dashboard.py         # one-screen panels generator
│  │  └─ styles.mplstyle
│  ├─ io/
│  │  ├─ load_trajectories.py # MDAnalysis readers; minimal columns
│  │  └─ manifests.py         # run manifest schema + validation
│  └─ README.md
└─ results/
   ├─ figures/                # auto-generated (empty in Phase 0)
   ├─ tables/                 # CSV summaries (empty in Phase 0)
   └─ logs/                   # run + analysis manifests (empty in Phase 0)

```


```.gitignore title:.gitignore
# .gitignore
__pycache__/
*.pyc
.env
.venv/
.mamba/
.envrc
.DS_Store
results/**/*
!results/.keep
sim/prep/packed/*
sim/prep/equil/*
sim/prep/nve_check/*
sim/prep/common_start/*
sim/prod/**/r*/   # runtime outputs
.envrc
.ipynb_checkpoints/
```


```make title:"Makefile"
# Makefile
.PHONY: env seeds dirs

PY?=python

dirs:
	mkdir -p sim/configs/params sim/prep/{packed,equil,nve_check,common_start} sim/prod/{templates,generators,slurm} \
	         analysis/{io,viz} results/{figures,tables,logs,prep_logs}

env:
	conda env create -f env/conda-environment.yml || mamba env create -f env/conda-environment.yml
	@echo "Activate with: conda activate ec-reaxff"

seeds:
	@seq 1 200 > sim/seedlist.txt
	@echo "Wrote 200 seeds to sim/seedlist.txt"

```



```yaml title:"env/conda-environment.yml"
# env/conda-environment.yml
name: ec-reaxff
channels: [conda-forge, defaults]
dependencies:
  - python=3.11
  - pip
  - numpy
  - pandas
  - scipy
  - matplotlib
  - mdanalysis
  - networkx
  - pip:
      - lifelines==0.28.0
```



```text title:"env/requirements.lock.txt"
# env/requirements.lock.txt
# Optional: if you prefer pip locking (pip-tools/uv), pin exact wheels here.
# numpy==...
# pandas==...
# lifelines==0.28.0
```


```text title:"env/.python-version"
# env/.python-version
3.11.9
```


```markdown title:"<!-- env/lammps_build_notes.md -->"
<!-- env/lammps_build_notes.md -->
# LAMMPS Build Notes (ReaxFF + ACKS2)

- Commit: TODO
- Source URL: TODO
- CMake options:
  - -D BUILD_LIB=on
  - -D LAMMPS_EXCEPTIONS=on
  - -D PKG_REAXFF=yes
  - -D PKG_KSPACE=yes
  - -D PKG_MOLECULE=yes
  - -D PKG_EXTRA-FIX=yes
- Compiler / MPI:
  - CXX: TODO (e.g., icpx 2025.0 / mpicxx)
  - OpenMP: on/off
- Parallel layout used in this project:
  - MPI ranks per run: TODO
  - OMP threads per rank: TODO
- Notes:
  - QEq/ACKS2 tolerances matched; see sim/configs/*_QEq.in and *_ACKS2.in later.
```



```text title:sim/seedlist.txt
# sim/seedlist.txt (generated by `make seeds`; example head)
1
2
3
4
5
...
200

```


```ini title:"sim/configs/run_constants.in"
# sim/configs/run_constants.in
# Invariant mechanics for all variants. Filled in during Phase 2, but stub here.
# Include with: include sim/configs/run_constants.in

variable T equal 330.0                 # [K] target temperature (placeholder)
variable dt equal 0.10e-15             # [s] 0.10 fs (placeholder; validate in Phase 2)
variable neigh_skin equal 2.0          # [Å] neighbor skin (placeholder)
variable neigh_every equal 1           # rebuild frequency
variable thermo_every equal 100        # print stride
variable dump_coord_stride equal 1000  # ~100–200 fs once dt validated
variable dump_bo_stride equal 100      # 10–20 fs once dt validated

# Neighbor & integration
neighbor ${neigh_skin} bin
neigh_modify every ${neigh_every} delay 0 check yes
timestep ${dt}
fix int all nve                          # replaced by thermostat in production scripts

# Thermostat choice lives here to keep parity; pick one and stick with it
# Example: Langevin (placeholder parameters; finalize in Phase 2)
# fix tstat all langevin ${T} ${T} 50.0 12345 zero yes
# fix_modify tstat temp thermo_temp

# Output minimal thermo; bond-order & charges are defined in log_custom.compute later
thermo ${thermo_every}
thermo_style custom step temp pe ke etotal press

```



```markdown title:"<!-- sim/configs/README.md -->"
<!-- sim/configs/README.md -->
# Configs

- `run_constants.in`: single source of truth for timestep, neighbor/thermostat, and I/O cadences.
- `params/`: drop `reaxff_A.ff` and `reaxff_B.ff` here (Phase 3).
- Variant includes (`A_QEq.in`, `A_ACKS2.in`, `B_QEq.in`, `B_ACKS2.in`) live here in Phase 3 and should only:
  - point to a param file, and
  - set the charge-equilibration style/tolerances.
Everything else must come from `run_constants.in`.

```



```Python title:sim/prep/pack_ec_lipf6.py
# sim/prep/pack_ec_lipf6.py
"""
Build a packed EC:LiPF6 box at target composition and box size.

Usage:
  python sim/prep/pack_ec_lipf6.py --n-ec 512 --n-lipf6 64 --box 40.0 \
      --out sim/prep/packed/packed.data

Notes:
  - For Packmol, emit a temporary .inp if desired; otherwise write a crude lattice placement
    (OK since NPT→NVT will relax it). This is a scaffold; replace placement with Packmol calls if you prefer.
"""
from __future__ import annotations
import argparse, json, os, sys
import numpy as np

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--n-ec", type=int, required=True)
    ap.add_argument("--n-lipf6", type=int, required=True)
    ap.add_argument("--box", type=float, required=True, help="cubic box length in Å")
    ap.add_argument("--out", type=str, required=True)
    ap.add_argument("--summary", type=str, default="results/prep_logs/pack_summary.json")
    args = ap.parse_args()

    L = args.box
    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    os.makedirs(os.path.dirname(args.summary), exist_ok=True)

    # Placeholder: random non-overlapping points (very rough); swap with Packmol for real projects.
    rng = np.random.default_rng(42)
    n_atoms = args.n_ec * 8 + args.n_lipf6 * 8  # rough atom count guess for EC and LiPF6
    coords = rng.random((n_atoms, 3)) * L

    # Minimal LAMMPS data header (skeleton; replace with real molecule templates)
    with open(args.out, "w") as f:
        f.write("LAMMPS data file (scaffold) -- replace with Packmol output\n\n")
        f.write(f"{n_atoms} atoms\n\n")
        f.write(f"0.0 {L:.6f} xlo xhi\n0.0 {L:.6f} ylo yhi\n0.0 {L:.6f} zlo zhi\n\n")
        f.write("Atoms\n\n")
        for i, (x,y,z) in enumerate(coords, start=1):
            f.write(f"{i} 1 1 {x:.6f} {y:.6f} {z:.6f}\n")  # id mol type x y z (stub)

    with open(args.summary, "w") as s:
        json.dump({
            "n_ec": args.n_ec,
            "n_lipf6": args.n_lipf6,
            "box_A": L,
            "atoms_stub": n_atoms,
            "note": "Replace scaffold coordinates with Packmol-based packing."
        }, s, indent=2)

if __name__ == "__main__":
    sys.exit(main())

```



```lammps title:sim/prep/equilibrate_npt_nvt.in
# sim/prep/equilibrate_npt_nvt.in
# One-time NPT -> NVT relaxation to produce common_start.* (Phase 1 will fill in details)
units           real
atom_style      full
boundary        p p p

# INPUTS
read_data       sim/prep/packed/packed.data

# Includes
include         sim/configs/run_constants.in
# TODO: pick one FF for prep only (non-reactive surrogate or A_QEq)
# include      sim/configs/A_QEq.in

# Pair style / charges (placeholder; set correctly in Phase 1)
# pair_style    reax/c NULL
# pair_coeff    * * sim/configs/params/reaxff_A.ff C H O Li F P

# NPT leg (placeholder time constants)
reset_timestep  0
fix             bar all press/berendsen iso 1.0 1.0 1000.0
fix             tstat all langevin ${T} ${T} 50.0 98765 zero yes
fix             int all nve
run             20000    # ~2 ps at 0.1 fs (adjust later)

unfix           bar
unfix           tstat
unfix           int

# NVT leg
fix             tstat all langevin ${T} ${T} 50.0 12345 zero yes
fix             int all nve
run             20000

# Write outputs
write_data      sim/prep/equil/equil.data
write_restart   sim/prep/equil/equil.restart

```


```Python title:analysis/io/load_trajectories.py
# analysis/io/load_trajectories.py
"""
Minimal placeholder to centralize trajectory reading later.
"""
from __future__ import annotations

def sanity():
    return True

```



```markdown title:"<!-- README.md -->"
<!-- README.md -->
# EC ReaxFF Robustness Study

This repo evaluates whether earliest EC decomposition mechanisms predicted by ReaxFF
are robust to (i) parameter-set choice and (ii) charge-equilibration scheme (QEq vs. ACKS2)
under matched mechanics.

## Quick start (Phase 0)

```bash
make dirs
make env
make seeds
```


- Fill `env/lammps_build_notes.md` with your actual build details.
- Replace the scaffold in `sim/prep/pack_ec_lipf6.py` with Packmol-based packing before Phase 1.
- Edit `sim/configs/run_constants.in` only in Phase 2 once timestep/neighbor/thermostat are validated.

Outputs live under `results/`, inputs/templates under `sim/`, analysis code under `analysis/`.


```yaml title:"CITATION.cff"
# CITATION.cff
cff-version: 1.2.0
message: "If you use this project, please cite it as below."
title: "Robustness of early EC decomposition in ReaxFF: parameter sets and QEq vs. ACKS2"
authors:
  - family-names: Your-Name
    given-names: First
version: "0.1.0"
date-released: "2025-09-21"
```




# Phase 1 Prepare the Common Starting Configuration


```
ec-reaxff-robustness/
├─ sim/
│  ├─ prep/
│  │  ├─ pack_ec_lipf6.py                 # script to build EC:LiPF6 mixture (counts, box)
│  │  ├─ packmol/
│  │  │  ├─ packmol.inp                   # optional: Packmol template for initial placement
│  │  │  └─ ec_lipf6.xyz                  # EC + LiPF6 monomer geometries
│  │  ├─ equilibrate_npt_nvt.in           # LAMMPS: NPT→NVT at target (T, ρ, x)
│  │  ├─ validate_nve.in                  # short NVE drift check (timestep/neighbor sanity)
│  │  ├─ freeze_positions.py              # strips velocities; writes common_start.{data,xyz}
│  │  ├─ statepoint.yaml                  # T, density, composition, box; provenance/notes
│  │  ├─ packed/
│  │  │  ├─ packed.data                   # raw packed configuration (pre-equilibration)
│  │  │  ├─ pack.log                      # packer stdout/log
│  │  │  └─ visuals/
│  │  │     └─ packed_snapshot.xyz        # quick visual sanity snapshot
│  │  ├─ equil/
│  │  │  ├─ equil.restart                 # engine-native restart at end of NVT
│  │  │  ├─ equil.data                    # same state in text (cell + coords; no vel)
│  │  │  ├─ equil.xyz                     # trajectory snippet for inspection
│  │  │  ├─ thermo.log                    # NPT→NVT thermodynamics (P, T, ρ vs time)
│  │  │  └─ rdf_probe.csv                 # quick Li–O / Li–F / C–O g(r) check (optional)
│  │  ├─ nve_check/
│  │  │  ├─ nve.log                       # energy drift report over ~50–100 ps
│  │  │  └─ drift_summary.txt             # |ΔE| per ps; pass/fail note
│  │  └─ common_start/
│  │     ├─ common_start.data             # immutable coordinates (text)
│  │     ├─ common_start.restart          # immutable engine-native restart
│  │     └─ common_start.xyz              # human-readable snapshot (no velocities)
│  └─ configs/
│     ├─ run_constants.in                 # already present from Phase 0 (timestep, neighbor, etc.)
│     └─ README.md                        # note: prep used <which FF>, but only coords are carried forward
├─ analysis/
│  └─ io/
│     └─ quick_check_prep.py              # tiny script to verify counts, box, neutrality, RDF minima
└─ results/
   └─ prep_logs/
      ├─ pack_summary.json                # molecule counts, box vectors, density
      ├─ equil_summary.json               # stabilized ρ, ⟨P⟩, ⟨E⟩, g(r) minima
      └─ nve_summary.json                 # drift metrics used to sign off timestep/neighbor

```



```yaml title:sim/prep/statepoint.yaml
# State point used for the one-time preparation
temperature_K: 330.0
target_density_g_cm3: 1.30              # for the EC:LiPF6 mixture at chosen x (example)
composition:
  EC: 512                                # molecules
  LiPF6: 64                              # formula units
box:
  type: cubic
  length_A: 40.0                         # initial guess; NPT will relax
notes: |
  This file defines ONLY the thermodynamic state and composition used to build the
  single shared structure. No model-specific settings belong here.
```


```.j2 title:sim/prep/packmol/packmol.inp.j2
# Packmol template (Jinja2). Units: Å. Box is [0,L] in each axis.
tolerance 2.0
filetype xyz
output {{ out_xyz }}

structure {{ ec_xyz }}
  number {{ n_ec }}
  inside box 0.0 0.0 0.0 {{ L }} {{ L }} {{ L }}
end structure

structure {{ lipf6_xyz }}
  number {{ n_lipf6 }}
  inside box 0.0 0.0 0.0 {{ L }} {{ L }} {{ L }}
end structure

```


```Python title:"sim/prep/pack_ec_lipf6.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json, os, shutil, subprocess, sys, tempfile
import yaml
from pathlib import Path

ROOT = Path(__file__).resolve().parents[2]

def run(cmd, cwd=None):
    p = subprocess.run(cmd, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    if p.returncode != 0:
        print(p.stdout)
        raise SystemExit(f"Command failed: {' '.join(cmd)}")
    return p.stdout

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--state", default=str(ROOT/"sim/prep/statepoint.yaml"))
    ap.add_argument("--out-data", default=str(ROOT/"sim/prep/packed/packed.data"))
    ap.add_argument("--summary", default=str(ROOT/"results/prep_logs/pack_summary.json"))
    ap.add_argument("--packmol", default="packmol")
    args = ap.parse_args()

    with open(args.state) as f:
        sp = yaml.safe_load(f)

    n_ec = int(sp["composition"]["EC"])
    n_lipf6 = int(sp["composition"]["LiPF6"])
    L = float(sp["box"]["length_A"])

    out_xyz = ROOT/"sim/prep/packed/packed.xyz"
    out_data = Path(args.out_data)
    out_data.parent.mkdir(parents=True, exist_ok=True)
    Path(args.summary).parent.mkdir(parents=True, exist_ok=True)

    # Prefer Packmol if available; else write a crude random scaffold.
    if shutil.which(args.packmol):
        tpl = (ROOT/"sim/prep/packmol/packmol.inp.j2").read_text()
        inp = tpl.replace("{{ out_xyz }}", str(out_xyz)) \
                 .replace("{{ ec_xyz }}", str(ROOT/"sim/prep/packmol/ec.xyz")) \
                 .replace("{{ lipf6_xyz }}", str(ROOT/"sim/prep/packmol/lipf6.xyz")) \
                 .replace("{{ n_ec }}", str(n_ec)) \
                 .replace("{{ n_lipf6 }}", str(n_lipf6)) \
                 .replace("{{ L }}", f"{L}")
        with tempfile.TemporaryDirectory() as td:
            Path(td, "packmol.inp").write_text(inp)
            log = run([args.packmol, "<", "packmol.inp"], cwd=td)
            (ROOT/"sim/prep/packed/pack.log").write_text(log)
            shutil.move(str(Path(td)/out_xyz.name), out_xyz)
    else:
        # Fallback: minimal placeholder xyz (not chemistry-accurate)
        import numpy as np
        n_atoms_guess = (n_ec + n_lipf6)*8
        coords = np.random.default_rng(42).random((n_atoms_guess,3))*L
        with open(out_xyz, "w") as f:
            f.write(f"{n_atoms_guess}\nscaffold\n")
            for x,y,z in coords:
                f.write(f"C {x:.4f} {y:.4f} {z:.4f}\n")

    # Convert XYZ -> LAMMPS data (simple molecule-type-less scaffold via LAMMPS)
    lmp_in = f"""
units real
atom_style atomic
boundary p p p
region box block 0 {L} 0 {L} 0 {L} units box
create_box 1 box
read_data {out_xyz} fix xyz  # lammps will map elements; this is a placeholder route
write_data {out_data}
"""
    # Safer route is to use your own converter; here we call LAMMPS if available:
    if shutil.which("lmp"):
        tmp = Path(tempfile.mkdtemp())
        inp = tmp/"xyz2data.in"
        inp.write_text(lmp_in)
        run(["lmp", "-in", str(inp)])
        shutil.move(str(tmp/out_data.name), out_data)
    else:
        out_data.write_text("LAMMPS data file placeholder written by pack_ec_lipf6.py\n")

    (ROOT/"results/prep_logs/pack_summary.json").write_text(json.dumps({
        "n_ec": n_ec, "n_lipf6": n_lipf6, "box_length_A": L,
        "output_data": str(out_data),
        "note": "Replace placeholders with chemically faithful packing if Packmol not used."
    }, indent=2))

if __name__ == "__main__":
    sys.exit(main())

```



```ini title:"sim/prep/equilibrate_npt_nvt.in"
# One-time NPT -> NVT to generate an equilibrated liquid. Write both data & restart.
units           real
atom_style      full
boundary        p p p
atom_modify     map yes

# ---- Inputs
read_data       sim/prep/packed/packed.data

# Invariant mechanics (dt, neighbor, thermostat cadence placeholders from Phase 0/2)
include         sim/configs/run_constants.in

# ---- Choose ONE model for prep ONLY (do not change later). Prefer non-reactive if available.
# include      sim/configs/A_QEq.in           # acceptable if window is short/cool so no chemistry occurs

# ---- ReaxFF pair style (fill once model chosen)
# pair_style    reaxff lmp_control
# pair_coeff    * * sim/configs/params/reaxff_A.ff C H O Li F P

# ---- Thermo & dumps
thermo_style    custom step temp press pe etotal density vol
thermo          ${thermo_every}

# ---- NPT (relax density)
reset_timestep  0
fix bar all press/berendsen iso 1.0 1.0 1000.0
fix tstat all langevin ${T} ${T} 50.0 77777 zero yes
fix int all nve
run  500000    # ~50 ps if dt=0.1 fs (adjust to reach stable ρ)

unfix bar
unfix tstat
unfix int

# ---- NVT (stabilize local structure at fixed volume)
fix tstat all langevin ${T} ${T} 50.0 88888 zero yes
fix int all nve
run  500000    # ~50 ps

# ---- Outputs
write_data      sim/prep/equil/equil.data
write_restart   sim/prep/equil/equil.restart
```


```Python title:sim/prep/to_xyz.in
units real
atom_style full
boundary p p p
read_data  sim/prep/equil/equil.data
dump dx all xyz 1 sim/prep/equil/equil.xyz
run 0
```



```lammps title:"sim/prep/freeze_positions.py"
units real
atom_style full
boundary p p p
read_data  sim/prep/equil/equil.data
dump dx all xyz 1 sim/prep/equil/equil.xyz
run 0

```



```Python title:"sim/prep/freeze_positions.py"
#!/usr/bin/env python
"""
Strip velocities and thermostat state, write immutable common_start.{data,restart,xyz}.
"""
from __future__ import annotations
import argparse, os, json
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in-data", default="sim/prep/equil/equil.data")
    ap.add_argument("--in-restart", default="sim/prep/equil/equil.restart")
    ap.add_argument("--outdir", default="sim/prep/common_start")
    ap.add_argument("--summary", default="results/prep_logs/equil_summary.json")
    args = ap.parse_args()

    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)

    # Use LAMMPS to zero velocities and re-write a clean data+restart
    # (Works even if original equilibration carried velocities/thermostat state)
    clean_in = f"""
units real
atom_style full
boundary p p p
read_data {args.in_data}
velocity all create 0.0 12345 mom yes rot yes dist gaussian
set atom * vx 0.0 vy 0.0 vz 0.0
write_data {outdir/'common_start.data'}
write_restart {outdir/'common_start.restart'}
dump dx all xyz 1 {outdir/'common_start.xyz'}
run 0
"""
    tmp = Path(".tmp_freeze.in")
    tmp.write_text(clean_in)
    os.system(f"lmp -in {tmp}")  # replace 'lmp' with your LAMMPS launcher if different
    tmp.unlink(missing_ok=True)

    Path(args.summary).parent.mkdir(parents=True, exist_ok=True)
    Path(args.summary).write_text(json.dumps({
        "source": {"data": args.in_data, "restart": args.in_restart},
        "outputs": {
            "data": str(outdir/'common_start.data'),
            "restart": str(outdir/'common_start.restart'),
            "xyz": str(outdir/'common_start.xyz'),
        },
        "note": "Coordinates frozen; velocities zeroed. Use velocities from seeds during production."
    }, indent=2))

if __name__ == "__main__":
    main()

```



```Python title:"sim/prep/equil/probe_rdf.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, numpy as np, MDAnalysis as mda, json
from MDAnalysis.analysis.rdf import InterRDF

ap = argparse.ArgumentParser()
ap.add_argument("--traj", default="sim/prep/equil/equil.xyz")
ap.add_argument("--top", default="sim/prep/equil/equil.data")
ap.add_argument("--out", default="results/prep_logs/equil_summary.json")
args = ap.parse_args()

u = mda.Universe(args.top, args.traj, format="LAMMPS")
# Placeholder selections—replace with your atom typing
sel_Li = u.select_atoms("name Li")
sel_O  = u.select_atoms("name O")
rdf = InterRDF(sel_Li, sel_O, range=(0.0, 6.0), nbins=300)
rdf.run()
rc_min = rdf.bins[np.argmin(rdf.rdf[(rdf.bins>1.5) & (rdf.bins<3.5)])]
with open(args.out, "w") as f:
    json.dump({"rdf_LiO_first_min_A": float(rc_min)}, f, indent=2)

```


## Quick runbook (Phase 1)

```bash
# 1) Pack once (fills sim/prep/packed/packed.data and results/prep_logs/pack_summary.json)
python sim/prep/pack_ec_lipf6.py

# 2) Equilibrate once (produces sim/prep/equil/equil.{data,restart})
lmp -in sim/prep/equilibrate_npt_nvt.in

# 3) Freeze coordinates (writes sim/prep/common_start/common_start.{data,restart,xyz})
python sim/prep/freeze_positions.py

```



# Phase 2 Validate Integrator/Neighbor Settings


## NVE Drift + Neighbor Probes (LAMMPS)

```
ec-reaxff-robustness/
├─ sim/
│  ├─ configs/
│  │  ├─ run_constants.in              # UPDATED: chosen timestep, neighbor skin, neighbor freq, thermostat style/params
│  │  ├─ neighbor_scan.yaml            # optional: tried {dt, skin, neigh_every} and notes
│  │  └─ thermostat_params.md          # rationale for chosen thermostat & τ values; parity notes
│  ├─ validation/
│  │  ├─ nve_sanity.in                 # LAMMPS: 50–100 ps NVE on common_start; prints Etot every step
│  │  ├─ nve_sanity_small_dt.in        # optional: Δt/1.25 spot-run for comparison
│  │  ├─ neighbor_probe.in             # short NVE to test neighbor skin/build frequency combos
│  │  ├─ analyze_nve_drift.py          # parses logs → |ΔE|/atom/ps, linear fit slope, pass/fail
│  │  ├─ analyze_neighbor_events.py    # reports neighbor rebuild stats, missed pairs warnings
│  │  └─ manifests/
│  │     └─ validation_manifest.json   # records exact inputs, LAMMPS build, MPI/OMP layout
│  └─ prep/
│     └─ common_start/
│        ├─ common_start.data          # input coordinates for validation runs
│        └─ common_start.restart
├─ analysis/
│  ├─ viz/
│  │  ├─ plot_nve_drift.py             # produces Etot(t) and residuals; saves PNG/PDF
│  │  └─ styles.mplstyle
│  └─ io/
│     └─ read_lammps_log.py            # tiny helper to read thermo/log files
└─ results/
   └─ validation_logs/
      ├─ nve/
      │  ├─ nve_sanity.log             # raw LAMMPS thermo output (Etot, Temp)
      │  ├─ nve_sanity_small_dt.log    # optional comparison run
      │  ├─ drift_metrics.json         # computed slope, |ΔE|/atom/ps, window used
      │  └─ Etot_vs_time.png           # quick visual check
      └─ neighbor/
         ├─ neighbor_probe.log         # neighbor rebuild frequency, warnings
         └─ neighbor_summary.json      # chosen skin and neigh_every with justifications

```



```ini title:sim/validation/nve_sanity.in
# sim/validation/nve_sanity.in
units           real
atom_style      full
boundary        p p p
atom_modify     map yes
read_data       sim/prep/common_start/common_start.data

# Pull invariants (dt, neighbor skin/every) from run_constants.in
include         sim/configs/run_constants.in

# Pure NVE (no thermostat) to test energy drift at chosen dt/neighbor policy
unfix           tstat    # ensure no thermostat lingering from constants file
fix             int all nve

thermo_style    custom step temp pe ke etotal press
thermo          ${thermo_every}

# Write a lightweight log for the analyzer
variable        etot equal etotal
fix             f1 all print ${thermo_every} "${step} ${temp} ${etot}" file results/validation_logs/nve/nve_sanity.log screen no

run             500000    # ~50 ps if dt = 0.1 fs (adjust target window)

unfix           f1
unfix           int
```


```ini title:"sim/validation/neighbor_probe.in"
# sim/validation/neighbor_probe.in
# Short runs to test skin / rebuild frequency combos for missed-pair warnings & drift
units real
atom_style full
boundary p p p
read_data sim/prep/common_start/common_start.data

# Candidate grid (override constants here)
variable neigh_skin equal 2.0   # Å (edit)
variable neigh_every equal 1    # (edit)
timestep 0.10e-15               # s (edit)

neighbor ${neigh_skin} bin
neigh_modify every ${neigh_every} delay 0 check yes

fix int all nve
thermo_style custom step temp ke pe etotal press
thermo 1000
log results/validation_logs/neighbor/neighbor_probe.log
run 100000    # ~10 ps
unfix int
```


## Parser and Quick Stats (Python)


```Python title:"analyze_nve_drift.py"
# sim/validation/analyze_nve_drift.py
from __future__ import annotations
import argparse, json, numpy as np, pandas as pd
from pathlib import Path

def load_log(p):
    df = pd.read_csv(p, sep=r"\s+", names=["step","T","Etot"], engine="python")
    return df

def slope_per_ps(df, dt_fs: float):
    t_ps = df["step"] * (dt_fs*1e-3)  # fs → ps
    # robust linear fit Etot(t)
    A = np.vstack([t_ps, np.ones_like(t_ps)]).T
    m, b = np.linalg.lstsq(A, df["Etot"].values, rcond=None)[0]
    return float(m)  # energy units per ps (LAMMPS 'real' → kcal/mol per ps)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--log", default="results/validation_logs/nve/nve_sanity.log")
    ap.add_argument("--dt-fs", type=float, required=True, help="timestep in femtoseconds")
    ap.add_argument("--natoms", type=int, required=True)
    ap.add_argument("--out", default="results/validation_logs/nve/drift_metrics.json")
    args = ap.parse_args()

    Path(Path(args.out).parent).mkdir(parents=True, exist_ok=True)
    df = load_log(args.log)
    m = slope_per_ps(df, args.dt_fs)  # kcal/mol per ps
    per_atom_eV_ps = (m/args.natoms) * 0.0433641153087705  # kcal/mol → eV
    summ = {
        "dt_fs": args.dt_fs,
        "natoms": args.natoms,
        "slope_kcal_per_mol_ps": m,
        "drift_abs_eV_per_atom_ps": abs(per_atom_eV_ps),
        "criterion_eV_per_atom_ps": 1.0e-4,
        "pass": abs(per_atom_eV_ps) < 1.0e-4
    }
    Path(args.out).write_text(json.dumps(summ, indent=2))
    print(json.dumps(summ, indent=2))

if __name__ == "__main__":
    main()

```




```Python title:"sim/validation/analyze_neighbor_events.py"
# sim/validation/analyze_neighbor_events.py
from __future__ import annotations
import argparse, json, re
from pathlib import Path

WARN_PAT = re.compile(r"(Dangerous builds|WARNING: Neighbor list overflow|neighbor list rebuild)")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--log", default="results/validation_logs/neighbor/neighbor_probe.log")
    ap.add_argument("--out", default="results/validation_logs/neighbor/neighbor_summary.json")
    args = ap.parse_args()
    txt = Path(args.log).read_text() if Path(args.log).exists() else ""
    issues = WARN_PAT.findall(txt)
    summary = {
        "log": args.log,
        "issues_found": len(issues) > 0,
        "matches": issues[:20]
    }
    Path(Path(args.out).parent).mkdir(parents=True, exist_ok=True)
    Path(args.out).write_text(json.dumps(summary, indent=2))
    print(json.dumps(summary, indent=2))

if __name__ == "__main__":
    main()

```



## Locking the Thermostat Choice (Notes + Constants)

```markdown
<!-- sim/configs/thermostat_params.md -->
# Thermostat choice (finalized in Phase 2)

Chosen style: **Langevin** (zero-drift setting)
- Target T: ${T} K (from run_constants.in)
- Damping τ: 50–100 fs (chosen: TODO)
- Seed policy: use replica seed for reproducibility (see prod templates)
- zero yes: keep zero net momentum

Alternative considered: **Nosé–Hoover**
- Chain length: 3
- Tdamp: 100 fs
- Rejected because: TODO (e.g., slower equilibration, sensitivity at sub-fs dt)

Parity requirements:
- Same thermostat and parameters across **all** variants/replicas.
- NVE used only for drift sanity checks and short settle tests.

```



```ini title:"sim/configs/run_constants.in"
# sim/configs/run_constants.in  (update the placeholders once validated)
# -- invariant mechanics (FINAL after Phase 2) --
variable T                 equal 330.0
variable dt                equal 0.10e-15          # FINAL
variable neigh_skin        equal 2.0               # FINAL
variable neigh_every       equal 1                 # FINAL
variable thermo_every      equal 1000
variable dump_coord_stride equal 1000              # coords ~100–200 fs later
variable dump_bo_stride    equal 100               # BO/charges every 10–20 fs later

neighbor ${neigh_skin} bin
neigh_modify every ${neigh_every} delay 0 check yes
timestep ${dt}

# Thermostat (FINAL)
unfix tstat
fix tstat all langevin ${T} ${T} 75.0 12345 zero yes
fix_modify tstat temp thermo_temp

thermo ${thermo_every}
thermo_style custom step temp pe ke etotal press

```



## Runbook (Phase 2)


```bash
# 0) Make sure common_start exists (Phase 1)
ls sim/prep/common_start/common_start.data

# 1) Run NVE drift at candidate dt/neighbor settings
lmp -in sim/validation/nve_sanity.in

# 2) Analyze drift (fill natoms and dt)
python sim/validation/analyze_nve_drift.py --dt-fs 0.10 --natoms <N_ATOMS> \
  --log results/validation_logs/nve/nve_sanity.log \
  --out results/validation_logs/nve/drift_metrics.json

# 3) Spot-check smaller dt
lmp -in sim/validation/nve_sanity_small_dt.in
python sim/validation/analyze_nve_drift.py --dt-fs 0.08 --natoms <N_ATOMS> \
  --log results/validation_logs/nve/nve_sanity_small_dt.log \
  --out results/validation_logs/nve/drift_metrics_small_dt.json

# 4) Probe neighbor settings (edit skin/every inside the .in file as needed)
lmp -in sim/validation/neighbor_probe.in
python sim/validation/analyze_neighbor_events.py

# 5) If drift < 1e-4 eV/atom/ps and no neighbor issues, UPDATE:
#    sim/configs/run_constants.in  (dt, neigh_skin, neigh_every, thermostat)
#    sim/configs/thermostat_params.md (brief rationale)

```





# Phase 3 Define the Four Model Variants


```
ec-reaxff-robustness/
├─ sim/
│  ├─ configs/
│  │  ├─ run_constants.in                 # invariant mechanics (from Phase 2; unchanged here)
│  │  ├─ params/
│  │  │  ├─ reaxff_A.ff                   # parameter set A
│  │  │  └─ reaxff_B.ff                   # parameter set B
│  │  ├─ A_QEq.in                         # includes: reaxff_A.ff, fix qeq style, tol, maxiter
│  │  ├─ A_ACKS2.in                       # includes: reaxff_A.ff, fix acks2 style, tol, maxiter
│  │  ├─ B_QEq.in                         # includes: reaxff_B.ff, fix qeq style, tol, maxiter
│  │  ├─ B_ACKS2.in                       # includes: reaxff_B.ff, fix acks2 style, tol, maxiter
│  │  ├─ charge_solver_parity.yaml        # matched tolerances (rtol, atol), maxiter, constraint settings
│  │  └─ README.md                        # notes: only lever = {params, charge style}; everything else via run_constants.in
│  ├─ validation/
│  │  ├─ charge_parity_probe.in           # tiny run: prints per-step residuals for QEq/ACKS2 on common_start
│  │  ├─ analyze_charge_residuals.py      # parses logs → residual stats, iter counts, parity check
│  │  └─ manifests/
│  │     └─ charge_parity_manifest.json   # records exact solver settings used in probe
│  └─ prep/
│     └─ common_start/
│        ├─ common_start.data
│        └─ common_start.restart
├─ analysis/
│  └─ io/
│     └─ parse_charge_logs.py             # helper to read residual/iteration logs from engines
└─ results/
   └─ parity_logs/
      ├─ A_QEq/
      │  └─ charge_probe.log              # residuals vs step; iteration counts
      ├─ A_ACKS2/
      │  └─ charge_probe.log
      ├─ B_QEq/
      │  └─ charge_probe.log
      ├─ B_ACKS2/
      │  └─ charge_probe.log
      └─ parity_summary.json              # confirms matched accuracy across schemes/sets

```


```yaml title:"sim/configs/charge_solver_parity.yaml"
# Matched accuracy across schemes (edit once, used everywhere)
rtol: 1.0e-8          # relative tolerance target for the linear solve
atol: 1.0e-10         # absolute floor (useful when charges ~ 0)
maxiter: 200          # identical cap so neither scheme is handicapped
# Optional extras if your engine exposes them:
preconditioner: jacobi
constraint: net_charge_zero
log_every: 100        # print a residual sample every N MD steps during probes

```


## Variant Includes (Only Pick Param File + Charge Scheme)


```ini title:sim/configs/A_QEq.in
# Variant: A + QEq
variable reaxff_param string "sim/configs/params/reaxff_A.ff"
variable charge_scheme string "QEq"            # consumed by engine-specific include
# tolerances pulled from charge_solver_parity.yaml by your launcher or templater

```


```ini title:sim/configs/A_ACKS2.in
# Variant: A + ACKS2
variable reaxff_param string "sim/configs/params/reaxff_A.ff"
variable charge_scheme string "ACKS2"

```



```ini title:sim/configs/B_QEq.in
# Variant: B + QEq
variable reaxff_param string "sim/configs/params/reaxff_B.ff"
variable charge_scheme string "QEq"

```


```ini title:sim/configs/B_ACKS2.in
# Variant: B + ACKS2
variable reaxff_param string "sim/configs/params/reaxff_B.ff"
variable charge_scheme string "ACKS2"

```



## Engine Glue (Drop our exact `fix` lines here once; Variants will just `include` them)


```lmp title:"sim/configs/charges/QEq.inc.lmp"
# Expect variables defined upstream:
# - reaxff_param (path)
# - qeq_rtol, qeq_atol, qeq_maxiter (numbers)
# Define the pair style + QEq fix for your build of LAMMPS.
# >>> EDIT these two lines to match your installation <<<
pair_style  reaxff  lmp_control
pair_coeff  * * ${reaxff_param} C H O Li F P

# Example (placeholder): replace with your engine’s QEq invocation
# fix ch all qeq/reax 1 0.0 10.0 ${qeq_rtol} ${qeq_maxiter}
# NOTE: If your build uses reax/c, the command name/args differ. Keep all parity knobs wired to variables.

```



```lmp title:"sim/configs/charges/ACKS2.inc.lmp"
# Same expectations as QEq.inc.lmp, but for ACKS2
pair_style  reaxff  lmp_control
pair_coeff  * * ${reaxff_param} C H O Li F P

# Example (placeholder): replace with your engine’s ACKS2 invocation
# fix ch all acks2/reaxff ${qeq_rtol} ${qeq_maxiter}
# Keep tolerance + maxiter variable-driven so parity is guaranteed.

```

**How to pass tolerances into those includes:** in your the generator from Phase 4 read `charge_solver_parity.yaml` and emit variables `qeq_rtol`, `qeq_atol`, and `qeq_maxiter` into each concrete input (or write a tiny prelude file that sets them before including `QEq.inc.lmp` / `ACKS2.inc.lmp`).


## Quick Probe to Confirm Parity at Runtime


```ini title:"sim/validation/charge_parity_probe.in"
# Short NVT on common_start that periodically prints charge-solver residuals/iters.
units       real
atom_style  full
boundary    p p p
read_data   sim/prep/common_start/common_start.data
include     sim/configs/run_constants.in

# ---- inject tolerances generated from charge_solver_parity.yaml (via templater)
variable qeq_rtol    equal 1.0e-8
variable qeq_atol    equal 1.0e-10
variable qeq_maxiter equal 200
variable scheme      string "${charge_scheme}"  # set by including one of A_QEq.in, etc.

# ---- choose scheme by including the right glue
# (your launcher will `include` exactly one of A_QEq.in, A_ACKS2.in, B_QEq.in, B_ACKS2.in before this file)
if "${scheme} == QEq" then "include sim/configs/charges/QEq.inc.lmp"
if "${scheme} == ACKS2" then "include sim/configs/charges/ACKS2.inc.lmp"

# ---- light thermostat to keep T stable while we sample residuals
unfix tstat
fix tstat all langevin ${T} ${T} 75.0 24680 zero yes
fix int   all nve

# ---- optional: your engine may expose residual/iteration counters; arrange a print every N steps
# If not, keep this as a placeholder; the analyzer can still parse counts if your build logs them.
variable step equal step
# Example placeholders (replace with real computes/variables if available):
# variable res  equal f_ch_residual
# variable it   equal f_ch_iterations

# Print step + (optional) residual + iter to a variant-specific log
fix f1 all print ${thermo_every} "${step}" file results/parity_logs/__VARIANT__/charge_probe.log screen no

run 20000    # a few ps are enough to collect samples

unfix f1
unfix int
unfix tstat

```


Replace `__VARIANT__` at submit time (your launcher already knows the variant) so logs land in `results/parity_logs/A_QEq/charge_probe.log`, etc.


## Parser for Probe


```Python title:"sim/validation/analyze_charge_residuals.py"
from __future__ import annotations
import argparse, json
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--logs", nargs="+", required=True)  # e.g., results/parity_logs/*/charge_probe.log
    ap.add_argument("--out", default="results/parity_logs/parity_summary.json")
    args = ap.parse_args()

    # Skeleton: if your build prints residuals/iters, parse them here.
    # For now, just check that files exist and have similar line counts.
    sizes = {}
    for log in args.logs:
        p = Path(log)
        sizes[p.parent.name] = {"exists": p.exists(), "lines": sum(1 for _ in p.open()) if p.exists() else 0}

    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    Path(args.out).write_text(json.dumps({"charge_probe": sizes}, indent=2))
    print(json.dumps({"charge_probe": sizes}, indent=2))

if __name__ == "__main__":
    main()
```


**_How you’ll wire this at launch (no code changes later):_** In your per-replica input generator (Phase 4), do three things when you render each concrete `.in`:

1.) Include **exactly one** of `A_QEq.in`, `A_ACKS2.in`, `B_QEq.in`, `B_ACKS2.in`.
2.) Read `charge_solver_parity.yaml` and emit:


```
variable qeq_rtol    equal <rtol>
variable qeq_atol    equal <atol>
variable qeq_maxiter equal <maxiter>
```

3.) Include the corresponding engine glue once:

```nginx
if "${charge_scheme} == QEq"   then "include sim/configs/charges/QEq.inc.lmp"
if "${charge_scheme} == ACKS2" then "include sim/configs/charges/ACKS2.inc.lmp"
```





# Phase 4 Replica Generators


```
ec-reaxff-robustness/
├─ sim/
│  ├─ seedlist.txt                          # 1..N seeds; same index used across variants
│  ├─ configs/
│  │  ├─ run_constants.in                   # invariant mechanics (dt, neighbor, thermostat, I/O cadence)
│  │  ├─ A_QEq.in
│  │  ├─ A_ACKS2.in
│  │  ├─ B_QEq.in
│  │  └─ B_ACKS2.in
│  ├─ prep/
│  │  └─ common_start/
│  │     ├─ common_start.data               # immutable coordinates
│  │     └─ common_start.restart
│  └─ prod/
│     ├─ templates/
│     │  ├─ settle_then_prod.in             # LAMMPS script: brief settle → production; includes variant file
│     │  ├─ input_vars.template             # per-replica vars (seed, paths, traj lengths) for jinja/format
│     │  └─ log_custom.compute              # defines thermo/bond-order/charge outputs
│     ├─ generators/
│     │  ├─ make_replica_inputs.py          # reads seedlist → writes per-replica input/vars for each variant
│     │  ├─ assign_velocities.py            # MB draw at T using replica seed; writes velocity file (optional)
│     │  └─ manifest_writer.py              # emits per-replica manifest.json (env, versions, seeds)
│     ├─ slurm/
│     │  ├─ launch_array_A_QEq.sh           # sbatch --array=1-N maps TASK_ID→REPLICA_ID
│     │  ├─ launch_array_A_ACKS2.sh
│     │  ├─ launch_array_B_QEq.sh
│     │  ├─ launch_array_B_ACKS2.sh
│     │  └─ slurm_common.sh                 # shared SBATCH resources, module loads, MPI/OMP layout
│     ├─ A_QEq/
│     │  ├─ inputs/                         # auto-generated *.in / vars for each replica
│     │  │  └─ r{0001..NNNN}/
│     │  │     ├─ in.settle_then_prod       # concrete LAMMPS input (includes run_constants + A_QEq.in)
│     │  │     ├─ vars.in                   # numeric vars for this replica (seed, steps, write cadences)
│     │  │     └─ manifest.json             # replica metadata (seed, variant, git/LAMMPS info)
│     │  └─ r{0001..NNNN}/                  # outputs land here per replica (created at runtime)
│     ├─ A_ACKS2/
│     │  ├─ inputs/
│     │  │  └─ r{0001..NNNN}/ (as above)
│     │  └─ r{0001..NNNN}/
│     ├─ B_QEq/
│     │  ├─ inputs/
│     │  │  └─ r{0001..NNNN}/ (as above)
│     │  └─ r{0001..NNNN}/
│     └─ B_ACKS2/
│        ├─ inputs/
│        │  └─ r{0001..NNNN}/ (as above)
│        └─ r{0001..NNNN}/
├─ results/
│  └─ prod_logs/
│     ├─ A_QEq/
│     │  └─ r{0001..NNNN}/
│     │     ├─ log.lammps                   # thermo (includes charge-solve residual snapshots)
│     │     ├─ traj.xyz                     # coords @ 100–200 fs cadence
│     │     ├─ bondorder.csv                # selected BO time series @ 10–20 fs
│     │     ├─ charges.csv                  # partial charges for Li, PF6, carbonate sites
│     │     └─ run_manifest.json            # frozen copy of manifest + runtime hashes
│     ├─ A_ACKS2/
│     │  └─ r{0001..NNNN}/ (same layout)
│     ├─ B_QEq/
│     │  └─ r{0001..NNNN}/ (same layout)
│     └─ B_ACKS2/
│        └─ r{0001..NNNN}/ (same layout)
└─ analysis/
   └─ io/
      └─ harvest_prod.py                    # walks prod_logs/*/r*/ → parquet/CSV for detection pipeline

```


```yaml title:"sim/prod/run_control.yaml"
# Single source of truth for replica count and window/cadence
n_replicas: 48                 # default target per variant
window_ps: 150.0               # production length
dump:
  coord_stride: 1000           # frames ~ every 100–200 fs once dt is final
  bo_stride: 100               # bond-order & charges every 10–20 fs
settle_ps: 3.0                 # brief settle under variant’s charges/thermostat
variants:
  - A_QEq
  - A_ACKS2
  - B_QEq
  - B_ACKS2

```


```text title:"sim/prod/templates/settle_then_prod.in.j2"
# Autogenerated per-replica; includes invariant mechanics and variant lever.

units           real
atom_style      full
boundary        p p p
atom_modify     map yes

# --- I/O paths (rendered) ---
variable OUTDIR string "{{ outdir }}"
variable LOGDIR string "{{ logdir }}"
variable BO_STRIDE equal {{ bo_stride }}
variable COORD_STRIDE equal {{ coord_stride }}
variable STEPS_SETTLE equal {{ steps_settle }}
variable STEPS_PROD   equal {{ steps_prod }}

# --- common constants & start structure ---
include         sim/configs/run_constants.in
read_data       sim/prep/common_start/common_start.data

# --- set RNG seeds deterministically from replica id ---
variable REPL equal {{ replica_id }}
variable SEED equal {{ seed }}       # used by thermostat or velocity assignment

# --- variant switch: include tiny lever file (params + charge scheme name) ---
include         sim/configs/{{ variant }}.in

# --- inject matched tolerances from parity yaml (rendered by Python) ---
variable qeq_rtol    equal {{ qeq_rtol }}
variable qeq_atol    equal {{ qeq_atol }}
variable qeq_maxiter equal {{ qeq_maxiter }}

# --- include engine glue for the selected scheme ---
if "${charge_scheme} == QEq"   then "include sim/configs/charges/QEq.inc.lmp"
if "${charge_scheme} == ACKS2" then "include sim/configs/charges/ACKS2.inc.lmp"

# --- velocities: create MB field at T using the replica seed (CRN pairing) ---
velocity all create ${T} ${SEED} mom yes rot yes dist gaussian

# --- settle window to relax charges/polarization onto this variant’s manifold ---
fix tstat all langevin ${T} ${T} 75.0 ${SEED} zero yes
fix int   all nve
include   sim/prod/templates/log_custom.compute
run ${STEPS_SETTLE}

# --- production: same mechanics, logging on; identical window across variants ---
reset_timestep 0
# (keep thermostat for short reactive window per your design)
# or switch to NVT variant as desired; mechanics must match across variants.
run ${STEPS_PROD}

# --- outputs live under results/prod_logs/<variant>/rXXXX/ ---

```


```text title:"sim/prod/templates/log_custom.compute"
# Minimal logs required by analysis (Phase 5/6)
thermo_style custom step temp pe ke etotal press
thermo ${BO_STRIDE}

# Bond-order/charge logging (engine-specific; placeholders below)
# You’ll choose the exact atom IDs/groups at generation or write selective dumps
# Example coordinate dump:
dump dxyz all xyz ${COORD_STRIDE} ${OUTDIR}/traj.xyz
dump_modify dxyz element C H O Li F P

# Example lightweight thermo print (step, T, Etot) as a side log
fix fth all print ${BO_STRIDE} "${step} ${temp} ${etotal}" file ${LOGDIR}/log.lammps screen no

```


```text title:"sim/prod/templates/input_vars.template"
# Filled per replica (for debugging or provenance)
variant={{ variant }}
replica_id={{ replica_id }}
seed={{ seed }}
steps_settle={{ steps_settle }}
steps_prod={{ steps_prod }}
bo_stride={{ bo_stride }}
coord_stride={{ coord_stride }}
qeq_rtol={{ qeq_rtol }}
qeq_atol={{ qeq_atol }}
qeq_maxiter={{ qeq_maxiter }}
```



```Python title:"sim/prod/generators/make_replica_inputs.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, math, os, yaml, json
from pathlib import Path
from jinja2 import Template

ROOT = Path(__file__).resolve().parents[3]

def read_parity():
    p = ROOT/"sim/configs/charge_solver_parity.yaml"
    y = yaml.safe_load(p.read_text())
    return float(y["rtol"]), float(y.get("atol", 0.0)), int(y["maxiter"])

def steps(ps: float, dt_fs: float) -> int:
    return int(round(ps*1e3/dt_fs))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True, choices=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    ap.add_argument("--dt-fs", required=True, type=float)
    ap.add_argument("--statefile", default=str(ROOT/"sim/prod/run_control.yaml"))
    ap.add_argument("--seedlist", default=str(ROOT/"sim/seedlist.txt"))
    args = ap.parse_args()

    cfg = yaml.safe_load(Path(args.statefile).read_text())
    nrep = int(cfg["n_replicas"])
    window_ps = float(cfg["window_ps"])
    settle_ps = float(cfg["settle_ps"])
    bo_stride = int(cfg["dump"]["bo_stride"])
    coord_stride = int(cfg["dump"]["coord_stride"])
    rtol, atol, maxiter = read_parity()

    tmpl = Template((ROOT/"sim/prod/templates/settle_then_prod.in.j2").read_text())
    var_dir = ROOT/f"sim/prod/{args.variant}"
    inputs_dir = var_dir/"inputs"
    inputs_dir.mkdir(parents=True, exist_ok=True)

    seeds = [int(s) for s in Path(args.seedlist).read_text().split()]
    assert len(seeds) >= nrep, "seedlist too short"

    for i in range(1, nrep+1):
        rid = f"r{i:04d}"
        seed = seeds[i-1]
        outdir = ROOT/f"results/prod_logs/{args.variant}/{rid}"
        logdir = outdir
        outdir.mkdir(parents=True, exist_ok=True)

        concrete = tmpl.render(
            outdir=outdir.as_posix(),
            logdir=logdir.as_posix(),
            bo_stride=bo_stride,
            coord_stride=coord_stride,
            steps_settle=steps(settle_ps, args.dt_fs),
            steps_prod=steps(window_ps, args.dt_fs),
            variant=args.variant,
            replica_id=i,
            seed=seed,
            qeq_rtol=rtol, qeq_atol=atol, qeq_maxiter=maxiter
        )
        (inputs_dir/rid).mkdir(parents=True, exist_ok=True)
        (inputs_dir/rid/"in.settle_then_prod").write_text(concrete)
        (inputs_dir/rid/"vars.txt").write_text(
            (ROOT/"sim/prod/templates/input_vars.template").read_text()
            .replace("{{ variant }}", args.variant)
            .replace("{{ replica_id }}", str(i))
            .replace("{{ seed }}", str(seed))
            .replace("{{ steps_settle }}", str(steps(settle_ps, args.dt_fs)))
            .replace("{{ steps_prod }}", str(steps(window_ps, args.dt_fs)))
            .replace("{{ bo_stride }}", str(bo_stride))
            .replace("{{ coord_stride }}", str(coord_stride))
            .replace("{{ qeq_rtol }}", f"{rtol}")
            .replace("{{ qeq_atol }}", f"{atol}")
            .replace("{{ qeq_maxiter }}", f"{maxiter}")
        )

if __name__ == "__main__":
    main()

```



```Python title:"sim/prod/generators/assign_velocities.py"
#!/usr/bin/env python
"""
Optional helper if you prefer to precompute velocity files per replica.
Otherwise, velocity creation happens inside LAMMPS using the same seed.
"""
from __future__ import annotations
import argparse
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--replica", type=int, required=True)
    args = ap.parse_args()
    rid = f"r{args.replica:04d}"
    out = Path(f"sim/prod/{args.variant}/inputs/{rid}/velocities.lmp")
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text("# placeholder: velocities are created in LAMMPS via 'velocity create'\n")
    print(out)

if __name__ == "__main__":
    main()

```



```Python title:"sim/prod/generators/manifest_writer.py"
#!/usr/bin/env python
from __future__ import annotations
import json, subprocess, os
from pathlib import Path

def git_rev() -> str:
    try:
        return subprocess.check_output(["git","rev-parse","HEAD"], text=True).strip()
    except Exception:
        return "UNKNOWN"

def write_manifest(variant: str, rid: str, seed: int, dest: Path):
    dest.write_text(json.dumps({
        "variant": variant,
        "replica_id": rid,
        "seed": seed,
        "git_commit": git_rev(),
        "env": {"lammps": os.environ.get("LAMMPS_EXE", "lmp")}
    }, indent=2))

```



```bash title:sim/prod/slurm/slurm_common.sh
#!/usr/bin/env bash
# Common SBATCH header flags (edit to your cluster)
#SBATCH -p compute
#SBATCH -N 1
#SBATCH -n 8
#SBATCH -t 04:00:00
#SBATCH --mem=8G
module purge
module load lammps  # or source your build
export LAMMPS_EXE=${LAMMPS_EXE:-lmp}

```




<span style="color:#ff0000; font-weight:bold">Note: For the following script, don't for get to clone and change the variant name for the other 3!</span>

```bash title:"sim/prod/slurm/launch_array_A_QEq.sh"
#!/usr/bin/env bash
set -euo pipefail
source "$(dirname "$0")/slurm_common.sh"

VARIANT=A_QEq
DT_FS=0.10            # must match sim/configs/run_constants.in (Phase 2)

# Generate inputs once per array submission
python sim/prod/generators/make_replica_inputs.py --variant ${VARIANT} --dt-fs ${DT_FS}

# Array over replicas
# SBATCH suggestion: set this in your sbatch call: --array=1-48
RID=$(printf "r%04d" "${SLURM_ARRAY_TASK_ID}")
INP="sim/prod/${VARIANT}/inputs/${RID}/in.settle_then_prod"
OUTDIR="results/prod_logs/${VARIANT}/${RID}"

mkdir -p "${OUTDIR}"
srun ${LAMMPS_EXE} -in "${INP}"

```




## Runbook

```bash
# 1) Generate inputs for one variant (reads run_control.yaml & seedlist.txt)
python sim/prod/generators/make_replica_inputs.py --variant A_QEq --dt-fs 0.10

# 2) Launch as an array (example for 48 replicas)
sbatch --array=1-48 sim/prod/slurm/launch_array_A_QEq.sh

# 3) Repeat for A_ACKS2, B_QEq, B_ACKS2 (or run in parallel queues)

```




# Phase 5 Production Runs


```
ec-reaxff-robustness/
├─ sim/
│  ├─ prod/
│  │  ├─ run_control.yaml                 # N_replicas per variant, window_ps, write cadences; single source of truth
│  │  ├─ templates/
│  │  │  ├─ settle_then_prod.in           # already present (Phase 4)
│  │  │  └─ log_custom.compute            # already present (Phase 4)
│  │  ├─ generators/
│  │  │  ├─ make_replica_inputs.py        # already present (Phase 4)
│  │  │  └─ manifest_writer.py            # already present (Phase 4)
│  │  ├─ slurm/
│  │  │  ├─ launch_array_A_QEq.sh         # --array=1-<N> reads run_control.yaml
│  │  │  ├─ launch_array_A_ACKS2.sh
│  │  │  ├─ launch_array_B_QEq.sh
│  │  │  ├─ launch_array_B_ACKS2.sh
│  │  │  └─ slurm_common.sh               # fixed resources; MPI/OMP layout pinned
│  │  ├─ A_QEq/
│  │  │  ├─ inputs/ r0001..rNNNN/         # per-replica concrete inputs (generated)
│  │  │  └─ r0001..rNNNN/                 # per-replica output dirs (created at runtime)
│  │  ├─ A_ACKS2/
│  │  │  ├─ inputs/ r0001..rNNNN/
│  │  │  └─ r0001..rNNNN/
│  │  ├─ B_QEq/
│  │  │  ├─ inputs/ r0001..rNNNN/
│  │  │  └─ r0001..rNNNN/
│  │  └─ B_ACKS2/
│  │     ├─ inputs/ r0001..rNNNN/
│  │     └─ r0001..rNNNN/
│  └─ configs/
│     └─ run_constants.in                 # invariant mechanics (dt, neighbor, thermostat, I/O cadence)
├─ results/
│  └─ prod_logs/
│     ├─ schema.md                        # documents file formats written per replica
│     ├─ A_QEq/
│     │  └─ r0001..rNNNN/
│     │     ├─ log.lammps                 # thermo with minimal fields; charge-solve residual snapshots
│     │     ├─ traj.xyz                   # coordinates every 100–200 fs
│     │     ├─ bondorder.csv              # selected BO pairs at 10–20 fs
│     │     ├─ charges.csv                # Li, PF6 (P,Fx), carbonate (C=O,O) partial charges
│     │     ├─ timing.json                # start/end times, window_ps, write cadences actually used
│     │     ├─ qc_pass.txt                # created by QC script if basic checks pass
│     │     └─ run_manifest.json          # seed, variant, git/LAMMPS build, MPI/OMP layout
│     ├─ A_ACKS2/
│     │  └─ r0001..rNNNN/ (same contents as above)
│     ├─ B_QEq/
│     │  └─ r0001..rNNNN/ (same contents as above)
│     └─ B_ACKS2/
│        └─ r0001..rNNNN/ (same contents as above)
├─ analysis/
│  └─ io/
│     ├─ harvest_prod.py                  # walks prod_logs → consolidated parquet/CSV
│     └─ qc/
│        ├─ verify_write_cadence.py       # checks cadence, missing frames, window length
│        └─ summarize_run_health.py       # extracts drift, thermostat stability, residual stats
└─ docs/
   └─ production_checklist.md             # runbook: arrays launch, expected counts, QC criteria

```


## Update the Per-Replica Logging Include


```text title:"sim/prod/templates/log_custom.compute"
# --- Thermo: minimal, steady cadence ---
thermo_style custom step temp etotal press
thermo ${BO_STRIDE}
fix fth all print ${BO_STRIDE} "${step} ${temp} ${etotal}" file ${LOGDIR}/log.lammps screen no

# --- Coordinates (for RDF/coordination) ---
dump dxyz all xyz ${COORD_STRIDE} ${OUTDIR}/traj.xyz
dump_modify dxyz element C H O Li F P

# --- Selected bond orders (engine-specific) ------------------------------
# For ReaxFF in LAMMPS, you can emit a per-pair BO via computes or per-atom masks.
# Leave placeholders here; wire your actual compute lines during bring-up.
# Required columns for analysis/bondorder.csv:
#   time_fs, BO_C_ringO, BO_OH, BO_PF, [optional extras]
# We print a CSV-like line at BO_STRIDE cadence.

variable time_fs equal step*dt*1.0e15

# Placeholders; replace with real expressions or fix print of reax bond order outputs
variable BO_C_ringO equal 0.0
variable BO_OH      equal 0.0
variable BO_PF      equal 0.0

fix fbo all print ${BO_STRIDE} \
"${time_fs} ${BO_C_ringO} ${BO_OH} ${BO_PF}" \
file ${OUTDIR}/bondorder.csv screen no

# --- Partial charges (subset only) ---------------------------------------
# Required columns for analysis/charges.csv:
#   time_fs, q_Li(sum or list), q_P, q_F1..q_F6, q_CO(carbonyl C), q_O1,q_O2
# If your build exposes per-atom charges (e.g., dump custom q), use a group & reduce.
# As a lightweight default, print variant-level sums/means you need for diagnostics.

variable qLi equal 0.0
variable qP  equal 0.0
variable qF1 equal 0.0
variable qF2 equal 0.0
variable qF3 equal 0.0
variable qF4 equal 0.0
variable qF5 equal 0.0
variable qF6 equal 0.0
variable qCO equal 0.0
variable qO1 equal 0.0
variable qO2 equal 0.0

fix fq all print ${BO_STRIDE} \
"${time_fs},${qLi},${qP},${qF1},${qF2},${qF3},${qF4},${qF5},${qF6},${qCO},${qO1},${qO2}" \
file ${OUTDIR}/charges.csv screen no

# --- Replica timing/audit file (JSON) ------------------------------------
# Write once at start with the planned window & cadences (your generator can also emit this)
# Here we append a final line at end via variable expansion into a JSON-like text.
variable window_ps   equal ${STEPS_PROD}*dt*1.0e12
variable bo_stridefs equal ${BO_STRIDE}*dt*1.0e15
variable xy_stridefs equal ${COORD_STRIDE}*dt*1.0e15
fix ftj all print ${BO_STRIDE} "{}" file ${OUTDIR}/timing.json screen no

```


Notes: the “placeholders” for BO/charges keep the scaffold light. When you wire your actual reaxff BO/charge outputs (e.g., via `compute reaxff/bonds`, `dump custom q`, or postprocessing), keep the **column names and cadences** as shown so downstream scripts work without edits.



## Define the Expected On-Disk Schema


```markdown title:"results/prod_logs/schema.md"
# Replica file schema (Phase 5)

**Every** `results/prod_logs/<variant>/<rXXXX>/` must contain:

- `log.lammps` — whitespace table with `step temp etotal` every `BO_STRIDE` steps.
- `traj.xyz` — XYZ at `COORD_STRIDE` cadence, with element labels {C,H,O,Li,F,P}.
- `bondorder.csv` — space- or comma-separated with header:

```


time_fs,BO_C_ringO,BO_OH,BO_PF

```csv title:"charges.csv"
- `charges.csv` — comma-separated with header:
```


time_fs,qLi,qP,qF1,qF2,qF3,qF4,qF5,qF6,qCO,qO1,qO2


```javascript title:"timing.json"
- `timing.json` — JSON with:
  {"window_ps": <float>, "bo_stride_fs": <float>, "coord_stride_fs": <float>}
```


## QC: Verify Window Length and Cadences


```Python title:"analysis/io/qc/verify_write_cadence.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path

def check(repdir: Path, tol_fs=1e-3):
    timing = json.loads((repdir/"timing.json").read_text())
    ok = True; msgs = []
    for fname in ["bondorder.csv","charges.csv","log.lammps","traj.xyz"]:
        if not (repdir/fname).exists():
            ok=False; msgs.append(f"missing {fname}")
    # Optional: quick cadence length checks by counting lines vs expected frames.
    return ok, msgs, timing

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("repdirs", nargs="+")
    ap.add_argument("--out", default=None)
    args = ap.parse_args()
    summary = {}
    for rd in args.repdirs:
        repdir = Path(rd)
        ok, msgs, timing = check(repdir)
        if ok: (repdir/"qc_pass.txt").write_text("OK\n")
        summary[str(repdir)] = {"ok": ok, "issues": msgs, "timing": timing}
    if args.out:
        Path(args.out).write_text(json.dumps(summary, indent=2))
    else:
        print(json.dumps(summary, indent=2))

if __name__ == "__main__":
    main()

```



## QC: Quick Run Health (Drift and Thermostat Stability)


```Python title:"analysis/io/qc/summarize_run_health.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json, numpy as np
from pathlib import Path

def read_log(p: Path):
    rows = []
    for line in p.read_text().splitlines():
        parts = line.strip().split()
        if len(parts)==3:
            step, T, Etot = map(float, parts)
            rows.append((step, T, Etot))
    return np.array(rows) if rows else np.zeros((0,3))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("repdir")
    ap.add_argument("--dt-fs", type=float, required=True)
    ap.add_argument("--out", default=None)
    args = ap.parse_args()
    rep = Path(args.repdir)
    arr = read_log(rep/"log.lammps")
    out = {"ok": False, "n": int(arr.shape[0])}
    if arr.size:
        t_ps = arr[:,0] * args.dt_fs * 1e-3
        # robust slope via least squares
        A = np.vstack([t_ps, np.ones_like(t_ps)]).T
        m, b = np.linalg.lstsq(A, arr[:,2], rcond=None)[0]
        drift = float(m)  # Etot slope per ps (kcal/mol if 'real')
        Tmean, Tstd = float(arr[:,1].mean()), float(arr[:,1].std())
        out.update({"drift_per_ps": drift, "T_mean": Tmean, "T_std": Tstd})
        out["ok"] = True
    if args.out:
        Path(args.out).write_text(json.dumps(out, indent=2))
    else:
        print(json.dumps(out, indent=2))

if __name__ == "__main__":
    main()

```



## Harvester: Consolidate Per-Replica Outputs


```Python title:"analysis/io/harvest_prod.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd

def harvest_variant(var_dir: Path) -> pd.DataFrame:
    rows = []
    for repdir in sorted(var_dir.glob("r*/")):
        rid = repdir.name
        man = json.loads((repdir/"run_manifest.json").read_text())
        timing = json.loads((repdir/"timing.json").read_text())
        rows.append({
            "variant": var_dir.name, "replica": rid,
            "seed": man.get("seed","NA"),
            "window_ps": timing.get("window_ps")
        })
    return pd.DataFrame(rows)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", default="results/prod_logs")
    ap.add_argument("--out", default="results/prod_logs/summary.csv")
    args = ap.parse_args()

    root = Path(args.root)
    dfs = []
    for var in ["A_QEq","A_ACKS2","B_QEq","B_ACKS2"]:
        var_dir = root/var
        if var_dir.exists():
            dfs.append(harvest_variant(var_dir))
    df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(args.out, index=False)
    print(f"Wrote {args.out} ({len(df)} rows)")

if __name__ == "__main__":
    main()

```



## Runbook Phase 5


```bash
python analysis/io/qc/verify_write_cadence.py results/prod_logs/*/r*/ --out results/prod_logs/qc_summary.json
python analysis/io/harvest_prod.py

```


# Phase 6 Event Detection


```
ec-reaxff-robustness/
├─ analysis/
│  ├─ detectors/
│  │  ├─ event_rules.yaml                # BO thresholds, hysteresis (on/off), min_frames, tie-break order
│  │  ├─ bondorder_binarize.py           # two-threshold hysteresis; per-pair state machine
│  │  ├─ first_event_detector.py         # forward scan; applies library rules; emits event time/type or censor
│  │  ├─ connectivity_graph.py           # builds graph from binarized BO; connected components per frame
│  │  ├─ typing.py                       # product typing in post-event window (CO vs CO2, oligomers, Li–F present)
│  │  ├─ post_event_freeze.py            # extracts 2–5 ps window after trigger; writes snapshots & graphs
│  │  ├─ io_schema.md                    # defines required columns for bondorder.csv, charges.csv, traj.xyz
│  │  └─ tests/
│  │     ├─ test_hysteresis.py
│  │     ├─ test_rules.py
│  │     └─ fixtures/                    # tiny synthetic traces for unit tests
│  ├─ pipelines/
│  │  ├─ run_detection_variant.py        # CLI: one variant → read prod_logs → events.parquet + QC
│  │  ├─ run_detection_all.py            # orchestrates all four variants; parallel over replicas
│  │  └─ config.yaml                     # paths to prod_logs, output dirs, post-event window length
│  ├─ viz/
│  │  ├─ plot_event_timeline.py          # per-replica traces with trigger marker (for spot checks)
│  │  └─ plot_binarization_demo.py       # shows raw BO vs. hysteresis states around transitions
│  └─ io/
│     ├─ harvest_prod.py                 # (from Phase 5) loader for bondorder/charges/coords
│     └─ write_graphml.py                # saves connectivity graphs for inspection (optional)
├─ results/
│  ├─ event_detection/
│  │  ├─ A_QEq/
│  │  │  ├─ events.parquet               # one row/replica: {replica_id, event_type, t_event, censored}
│  │  │  ├─ post_event/
│  │  │  │  └─ r{0001..}/
│  │  │  │     ├─ freeze.xyz             # 2–5 ps snippet after trigger
│  │  │  │     ├─ graph.gml              # connectivity graph at end of freeze window
│  │  │  │     └─ typing.json            # product classification summary
│  │  │  └─ qc_report.json               # counts, missing-data checks, threshold version used
│  │  ├─ A_ACKS2/
│  │  │  └─ (same layout)
│  │  ├─ B_QEq/
│  │  │  └─ (same layout)
│  │  └─ B_ACKS2/
│  │     └─ (same layout)
│  └─ logs/
│     └─ detection_manifest.json         # hashes of event_rules.yaml, code versions, run timestamp
├─ sim/
│  └─ prod/
│     └─ templates/
│        └─ log_custom.compute           # (Phase 4) ensures needed BO/charge outputs exist
└─ docs/
   └─ detection_readme.md                # brief “how it works”, endpoints, and caveats

```



## YAML

```yaml title:"analysis/detectors/event_rules.yaml"
# Bond-order hysteresis & event library (edit here; code reads this file only)
hysteresis:
  # Two-threshold + min consecutive frames (≥)
  C_ringO:     {form: 0.30, break: 0.20, min_frames: 3}
  OH:          {form: 0.30, break: 0.20, min_frames: 3}
  PF:          {form: 0.30, break: 0.20, min_frames: 3}

# Event rules evaluated in this order; first true wins
events:
  - name: ring_opening
    requires:
      - {pair: C_ringO, state: broken}        # carbonate ring C–O scission
    confirm_connectivity: true                 # ring cycle disappears (graph check)

  - name: deprotonation
    requires:
      - {pair: OH, state: broken}

  - name: pf6_dissociation
    requires:
      - {pair: PF, state: broken}
    confirm_connectivity: true                 # PF6 → PF5 + F–

post_event:
  freeze_ps: 2.0                               # write ~2 ps after trigger

```


Mapping of `pair` names → columns in `bondorder.csv` (Phase 5):  `C_ringO → BO_C_ringO`, `OH → BO_OH`, `PF → BO_PF`.



## Binarization with Hesteresis (Tiny State Machine)


```Python title:"analysis/detectors/bondorder_binarize.py"
from __future__ import annotations
import pandas as pd
from dataclasses import dataclass

@dataclass
class HystCfg:
    form: float
    break_: float
    min_frames: int

def hysteresis_binary(series: pd.Series, cfg: HystCfg) -> pd.Series:
    """Two-threshold hysteresis with min consecutive frames."""
    state = False
    above = below = 0
    out = []
    for v in series.values:
        if state:
            if v < cfg.break_:
                below += 1; above = 0
                if below >= cfg.min_frames:
                    state = False; below = 0
            else:
                below = 0
        else:
            if v > cfg.form:
                above += 1; below = 0
                if above >= cfg.min_frames:
                    state = True; above = 0
            else:
                above = 0
        out.append(state)
    return pd.Series(out, index=series.index, name=series.name)

def binarize_bo(df_bo: pd.DataFrame, cfg_map: dict[str,HystCfg]) -> pd.DataFrame:
    return pd.DataFrame({
        "t_fs": df_bo["time_fs"],
        "C_ringO": hysteresis_binary(df_bo["BO_C_ringO"], cfg_map["C_ringO"]),
        "OH":      hysteresis_binary(df_bo["BO_OH"],      cfg_map["OH"]),
        "PF":      hysteresis_binary(df_bo["BO_PF"],      cfg_map["PF"]),
    })

```



## First-Event Detector (Forward Scan, First Rule Wins)


```Python title:"analysis/detectors/first_event_detector.py"
from __future__ import annotations
import yaml, pandas as pd
from pathlib import Path
from .bondorder_binarize import binarize_bo, HystCfg
from .connectivity_graph import cycle_broken_pf6_split
from typing import Optional

def load_rules(yaml_path: Path):
    y = yaml.safe_load(Path(yaml_path).read_text())
    hyst = {k: HystCfg(v["form"], v["break"], v["min_frames"]) for k,v in y["hysteresis"].items()}
    return y, hyst

def detect_first_event(repdir: Path, rules_yaml: Path) -> dict:
    y, hyst = load_rules(rules_yaml)
    bo = pd.read_csv(repdir/"bondorder.csv", sep=r"[,\s]+", engine="python")
    binbo = binarize_bo(bo, hyst)
    t = binbo["t_fs"].values
    # forward scan
    for i in range(len(t)):
        row = binbo.iloc[i]
        for rule in y["events"]:
            name = rule["name"]
            ok = True
            for req in rule["requires"]:
                ok &= bool(row[req["pair"]] == (req["state"] == "formed"))  # we use 'broken' so formed=False
            if ok and rule.get("confirm_connectivity", False):
                ok &= cycle_broken_pf6_split(repdir, frame_index=i)  # lightweight graph check (see below)
            if ok:
                return {"event_type": name, "t_event_fs": float(row["t_fs"]), "index": int(i), "censored": False}
    # no trigger in window
    return {"event_type": "none", "t_event_fs": float(t[-1]) if len(t) else 0.0, "index": None, "censored": True}

```



The connectivity confirmation calls a minimal helper that inspects a graph snapshot derived from the **binarized** BO at that frame. If you don’t want the confirmation, set `confirm_connectivity: false` in YAML.



## Post-Event Freeze + Connectivity Graph Export


```Python title:"analysis/detectors/post_event_freeze.py"
from __future__ import annotations
import json
from pathlib import Path
import numpy as np
import yaml

def slice_after_event(times_fs: np.ndarray, i_evt: int, freeze_ps: float) -> tuple[int,int]:
    if i_evt is None: return len(times_fs), len(times_fs)  # nothing
    t0 = times_fs[i_evt]; end = t0 + freeze_ps*1e3
    j = np.searchsorted(times_fs, end, side="right") - 1
    return i_evt, max(i_evt, j)

def write_freeze_artifacts(repdir: Path, evt: dict, rules_yaml: Path, graph_builder) -> None:
    times = np.loadtxt(repdir/"bondorder.csv", usecols=[0], delimiter=",", ndmin=1)
    cfg = yaml.safe_load(Path(rules_yaml).read_text())
    i0, i1 = slice_after_event(np.asarray(times), evt["index"], cfg["post_event"]["freeze_ps"])
    outdir = repdir.parent/"post_event"/repdir.name
    outdir.mkdir(parents=True, exist_ok=True)

    # Copy subset of trajectory frames to freeze.xyz (simple approach: write indices as metadata)
    # Real implementation would slice trajectory; here we just drop a tiny marker file.
    (outdir/"freeze.xyz").write_text("# slice indices (inclusive): "
                                     f"{i0}-{i1} at {cfg['post_event']['freeze_ps']} ps\n")

    # Build connectivity graph (from binarized BO at end frame); write GraphML/GML
    G = graph_builder(repdir, frame_index=i1)
    (outdir/"graph.gml").write_text("\n".join([
        "graph [",
        "  directed 0",
        *[f'  node [ id {n} label "{d.get("el","X")}" ]' for n,d in G.nodes(data=True)],
        *[f'  edge [ source {u} target {v} ]' for u,v in G.edges()],
        "]"
    ]))

    (outdir/"typing.json").write_text(json.dumps({"event": evt["event_type"], "frame_end": int(i1)}, indent=2))

```



```python title:"analysis/detectors/connectivity_graph.py"
from __future__ import annotations
from pathlib import Path
import pandas as pd
import networkx as nx

def graph_from_binarized(repdir: Path, frame_index: int) -> nx.Graph:
    # Minimal demo: infer an edge list from binarized “pair” columns you track.
    bo_bin = pd.read_csv(repdir/"bondorder.csv", sep=r"[,\s]+", engine="python")
    # In real code you’d load per-pair matrices; here, use placeholders for demo.
    G = nx.Graph()
    # Populate nodes with dummy atom indices / elements if you have them
    for i in range(10): G.add_node(i, el="X")
    # Populate edges from heuristics (placeholder). Replace with your real mapping.
    return G

def cycle_broken_pf6_split(repdir: Path, frame_index: int) -> bool:
    # Placeholder connectivity confirmation. Always returns True in scaffold.
    # Replace by: build graph, check ring cycle removed or PF6 split into components, etc.
    return True

def build_graph_for_frame(repdir: Path, frame_index: int):
    return graph_from_binarized(repdir, frame_index)

```



```Python title:"analysis/detectors/typing.py"
from __future__ import annotations
import networkx as nx

def classify_products(G: nx.Graph) -> dict:
    # Placeholder classifier; swap in heuristics:
    # - detect CO vs CO2 by small components with specific degrees
    # - detect free F- and Li+ contact by metadata
    return {"CO": 0, "CO2": 0, "oligomers": 0, "LiF_contact": False}

```


## Piplines (CLI)


```Python title:"analysis/pipelines/run_detection_variant.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd
from analysis.detectors.first_event_detector import detect_first_event
from analysis.detectors.post_event_freeze import write_freeze_artifacts
from analysis.detectors.connectivity_graph import build_graph_for_frame

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--rules", default="analysis/detectors/event_rules.yaml")
    ap.add_argument("--root", default="results/prod_logs")
    ap.add_argument("--outdir", default="results/event_detection")
    args = ap.parse_args()

    vdir = Path(args.root)/args.variant
    rows = []
    for repdir in sorted(vdir.glob("r*/")):
        evt = detect_first_event(repdir, Path(args.rules))
        rows.append({"replica": repdir.name, **evt})
        if not evt["censored"]:
            write_freeze_artifacts(repdir, evt, Path(args.rules), build_graph_for_frame)

    outv = Path(args.outdir)/args.variant
    outv.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rows).to_parquet(outv/"events.parquet", index=False)
    (outv/"qc_report.json").write_text(json.dumps({"n": len(rows), "none": sum(r["censored"] for r in rows)}, indent=2))
    print(f"Wrote {outv/'events.parquet'}")

if __name__ == "__main__":
    main()

```



```Python title:"analysis/pipelines/run_detection_all.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, subprocess, sys

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variants", nargs="+", default=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    ap.add_argument("--rules", default="analysis/detectors/event_rules.yaml")
    args = ap.parse_args()
    for v in args.variants:
        subprocess.check_call([sys.executable, "analysis/pipelines/run_detection_variant.py", "--variant", v, "--rules", args.rules])

if __name__ == "__main__":
    main()

```



## 6 Unit Test


```Python title:"analysis/detectors/tests/test_hysteresis.py"
from analysis.detectors.bondorder_binarize import hysteresis_binary, HystCfg
import pandas as pd

def test_hysteresis_basic():
    s = pd.Series([0.0,0.1,0.35,0.36,0.34,0.25,0.15,0.10])
    out = hysteresis_binary(s, HystCfg(0.30,0.20,2))
    # should switch on at frames 2-3 sustained, off at frames 6-7 sustained
    assert out.iloc[0] == False and out.iloc[4] == True and out.iloc[-1] == False

```




## Runbook


```bash
# Detect events for one variant
python analysis/pipelines/run_detection_variant.py --variant A_QEq

# Or run all four
python analysis/pipelines/run_detection_all.py

```





# Phase 7 Structural Context


```
ec-reaxff-robustness/
├─ analysis/
│  ├─ structure/
│  │  ├─ pre_event_slice.py                # selects frames < t_event per replica; length-bias-safe weighting
│  │  ├─ rdf_coord.py                      # computes g_{LiO}, g_{LiF}, g_{CO} and running coordinations
│  │  ├─ contact_cutoff.py                 # finds first-minimum r_c from Li–F g(r); variant or pooled option
│  │  ├─ lif_contact.py                    # builds Li–F contact traces with intermittency tolerance
│  │  ├─ lifetimes_km.py                   # KM survival of contact episodes; censored-aware medians/RMST
│  │  ├─ stratify_by_event.py              # splits replicas by eventual first-event type when counts allow
│  │  ├─ config.yaml                       # bins (Δr), r_max, weighting (per-replica), τ_gap, r_c policy
│  │  └─ __init__.py
│  ├─ pipelines/
│  │  ├─ run_structure_variant.py          # CLI: one variant → RDFs, coordinations, Li–F metrics
│  │  ├─ run_structure_all.py              # orchestrates all four variants; parallel over replicas
│  │  └─ paths.yaml                        # input: results/prod_logs/*; output: results/structure/*
│  ├─ viz/
│  │  ├─ plot_rdf_overlays.py              # overlays g(r) per variant; annotates first peak/minimum
│  │  ├─ plot_coordination.py              # running coordination and value at first minimum
│  │  ├─ plot_lif_contact_hist.py          # histogram (and optional log-x) of Li–F contact lifetimes
│  │  └─ plot_lif_km.py                    # KM curve for contact episodes with CIs and # at risk
│  └─ io/
│     ├─ harvest_prod.py                   # (Phase 5) loader reused here
│     └─ write_tables.py                   # saves tidy CSV/Parquet summaries (per-replica, per-variant)
├─ results/
│  ├─ structure/
│  │  ├─ A_QEq/
│  │  │  ├─ rdf/
│  │  │  │  ├─ g_LiO.csv                   # r, g(r) (per-replica avg, variant avg, bootstrap CIs)
│  │  │  │  ├─ g_LiF.csv
│  │  │  │  └─ g_CO.csv
│  │  │  ├─ coordination/
│  │  │  │  ├─ n_LiO.csv                   # running n(r); value at first minimum in header/metadata
│  │  │  │  ├─ n_LiF.csv
│  │  │  │  └─ n_CO.csv
│  │  │  ├─ lif_contact/
│  │  │  │  ├─ rc_metadata.json            # r_c from Li–F g(r); method (pooled/variant), Δr
│  │  │  │  ├─ freq_by_replica.csv         # time-fraction in contact per replica
│  │  │  │  ├─ lifetimes.csv               # episode durations with censor flags
│  │  │  │  └─ km_contact.json             # KM estimates + CIs, RMST to τ
│  │  │  └─ stratified/
│  │  │     ├─ ring_opening/…              # same subfolders for replicas that eventually ring-open
│  │  │     ├─ deprotonation/…
│  │  │     └─ pf6_dissociation/…
│  │  ├─ A_ACKS2/                          # same layout as A_QEq
│  │  ├─ B_QEq/                            # same layout
│  │  └─ B_ACKS2/                          # same layout
│  └─ figures/
│     ├─ rdf_overlays.png                  # Li–O, Li–F, C–O overlays with shared legend
│     ├─ coordination_at_min.png           # bar/inset of n at first minima across variants
│     ├─ lif_contact_hist.png              # Li–F contact lifetime histograms
│     └─ lif_contact_km.png                # KM survival of Li–F contact episodes
└─ docs/
   └─ structure_methods.md                 # brief notes on pre-event pooling, bootstraps, and cutoff policy

```



```yaml title:"analysis/structure/config.yaml"
bins:
  dr_A: 0.02         # Å
  rmax_A: 8.0
weighting: per_replica_equal   # equal-weight replicas (avoid length bias)
contacts:
  tau_gap_fs: 40.0   # treat gaps shorter than this as still in-contact
  cutoff_policy: variant   # {pooled, variant}; where to take g_LiF first minimum

```



```Python title:"analysis/structure/pre_event_slice.py"
from __future__ import annotations
import pandas as pd
from pathlib import Path

def load_events(variant_dir: Path) -> pd.DataFrame:
    ev = pd.read_parquet(Path("results/event_detection")/variant_dir.name/"events.parquet")
    # expected cols: replica, event_type, t_event_fs, censored
    return ev.set_index("replica")

def frame_mask(times_fs, t_event_fs, censored: bool):
    import numpy as np
    if censored or pd.isna(t_event_fs):
        return np.ones_like(times_fs, dtype=bool)
    return times_fs < t_event_fs

def build_masks_for_variant(var_logs: Path, events_df: pd.DataFrame):
    """Yield (repdir, times_fs, mask_pre_event)."""
    for repdir in sorted(var_logs.glob("r*/")):
        rid = repdir.name
        times = pd.read_csv(repdir/"bondorder.csv").iloc[:,0].to_numpy()  # time_fs column
        row = events_df.loc[rid]
        yield repdir, times, frame_mask(times, row["t_event_fs"], bool(row["censored"]))

```



```Python title:"analysis/structure/rdf_coord.py"
from __future__ import annotations
import numpy as np, pandas as pd
from pathlib import Path
from dataclasses import dataclass

@dataclass
class RDFSpec:
    dr: float
    rmax: float

def pairwise_distances_pbc(coords: np.ndarray, box: np.ndarray) -> np.ndarray:
    # coords shape: (N,3); box diag [Lx,Ly,Lz] in Å; return condensed distance matrix? We stream instead.
    raise NotImplementedError  # Wire to your trajectory loader (MDAnalysis or your own)

def rdf_single_replica(pre_event_frames, selector_alpha, selector_beta, spec: RDFSpec):
    """Return r, g(r) for one replica using equal-weighted frames from pre-event slice."""
    # Placeholder skeleton; in practice, iterate frames, accumulate histograms with PBC
    r = np.arange(spec.dr/2, spec.rmax, spec.dr)
    g = np.ones_like(r)  # replace with real histogram/normalization
    return r, g

def running_coordination(r, g, rho_beta):
    """n(r) = 4πρ ∫ g(s)s^2 ds; simple cumulative trapezoid."""
    s = r
    integrand = g * s**2
    n = 4*np.pi*rho_beta*np.cumsum(integrand)* (s[1]-s[0])
    return n

def pooled_rdf(replica_curves: list[tuple[np.ndarray,np.ndarray]], weighting="per_replica_equal"):
    # Resample onto common r grid (assume identical here); average.
    rs = replica_curves[0][0]
    G = np.vstack([g for _,g in replica_curves])
    return rs, G.mean(axis=0), G.std(axis=0)/np.sqrt(G.shape[0])

```



```Python title:"analysis/structure/contact_cutoff.py"
from __future__ import annotations
import numpy as np, json
from pathlib import Path

def first_minimum(r, g, rmin=1.0, rmax=4.0):
    msk = (r >= rmin) & (r <= rmax)
    i = np.argmin(g[msk])
    return float(r[msk][i])

def write_rc_metadata(outdir: Path, rc: float, policy: str, bins: dict):
    outdir.mkdir(parents=True, exist_ok=True)
    (outdir/"rc_metadata.json").write_text(json.dumps({
        "r_c_A": rc, "policy": policy, "dr_A": bins["dr_A"], "rmax_A": bins["rmax_A"]
    }, indent=2))

```



```python title:"analysis/structure/lif_contact.py"
from __future__ import annotations
import numpy as np, pandas as pd

def contact_trace(dist_fs: pd.Series, r_c: float, tau_gap_fs: float) -> pd.Series:
    """Binary contact with gap-bridging <= τ_gap_fs."""
    t = dist_fs.index.to_numpy()        # assume time fs in index
    d = dist_fs.values
    raw = (d <= r_c).astype(int)

    # Bridge short zeros between ones
    bridged = raw.copy()
    i = 0
    while i < len(raw):
        if raw[i] == 1:
            j = i+1
            while j < len(raw) and raw[j] == 1: j += 1
            # now [i, j) is a 1-run; check gap to next 1-run
            k = j
            while k < len(raw) and raw[k] == 0: k += 1
            if k < len(raw):
                gap = t[k] - t[j-1]
                if gap <= tau_gap_fs:
                    bridged[j:k] = 1
                    i = k
                    continue
            i = j
        else:
            i += 1
    return pd.Series(bridged, index=dist_fs.index, name="contact")

def contact_episodes(contact: pd.Series):
    """Yield (start_idx, end_idx, censored_bool)."""
    idx = contact.index.to_numpy()
    val = contact.values
    i = 0
    while i < len(val):
        if val[i] == 1:
            j = i+1
            while j < len(val) and val[j] == 1: j += 1
            yield idx[i], idx[j-1], (j == len(val))  # censored if runs off the end
            i = j
        else:
            i += 1

```



```Python title:"analysis/structure/lifetimes_km.py"
from __future__ import annotations
import numpy as np, json
from lifelines import KaplanMeierFitter
from pathlib import Path

def km_from_episodes(episodes: list[tuple[float,float,bool]]) -> dict:
    T = np.array([end - start for start, end, _ in episodes], dtype=float)
    E = np.array([not cens for _,_,cens in episodes], dtype=bool)
    km = KaplanMeierFitter()
    km.fit(T, event_observed=E)
    med = float(km.median_survival_time_) if np.isfinite(km.median_survival_time_) else None
    return {
        "timeline": km.survival_function_.index.to_list(),
        "S": km.survival_function_["KM_estimate"].to_list(),
        "median": med
    }

def write_km_json(out: Path, km: dict):
    out.write_text(json.dumps(km, indent=2))

```



```Python title:"analysis/structure/stratify_by_event.py"
from __future__ import annotations
import pandas as pd
from pathlib import Path

def strata(events_parquet: Path) -> dict[str, list[str]]:
    ev = pd.read_parquet(events_parquet)
    groups = {}
    for k, g in ev.groupby("event_type"):
        groups[k] = g["replica"].tolist()
    return groups

```



```Python title:"analysis/pipelines/run_structure_variant.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import numpy as np, pandas as pd, yaml

from analysis.structure.pre_event_slice import load_events, build_masks_for_variant
from analysis.structure.rdf_coord import RDFSpec, rdf_single_replica, pooled_rdf, running_coordination
from analysis.structure.contact_cutoff import first_minimum, write_rc_metadata
from analysis.structure.lif_contact import contact_trace, contact_episodes
from analysis.structure.lifetimes_km import km_from_episodes, write_km_json

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--cfg", default="analysis/structure/config.yaml")
    ap.add_argument("--root", default="results/prod_logs")
    ap.add_argument("--events", default=None)
    ap.add_argument("--out", default="results/structure")
    args = ap.parse_args()

    cfg = yaml.safe_load(Path(args.cfg).read_text())
    var_logs = Path(args.root)/args.variant
    events = load_events(Path(args.root).parent/args.variant) if args.events is None else pd.read_parquet(args.events).set_index("replica")

    # RDFs (skeleton; wire your trajectory loader for actual pair counting)
    spec = RDFSpec(dr=cfg["bins"]["dr_A"], rmax=cfg["bins"]["rmax_A"])
    li_o_curves, li_f_curves, c_o_curves = [], [], []
    for repdir, times, mask in build_masks_for_variant(var_logs, events):
        # TODO: replace with real per-frame coords & selectors
        r, g = rdf_single_replica(pre_event_frames=None, selector_alpha=None, selector_beta=None, spec=spec)
        li_o_curves.append((r,g)); li_f_curves.append((r,g)); c_o_curves.append((r,g))

    outv = Path(args.out)/args.variant
    (outv/"rdf").mkdir(parents=True, exist_ok=True)
    for name, curves in [("g_LiO.csv", li_o_curves), ("g_LiF.csv", li_f_curves), ("g_CO.csv", c_o_curves)]:
        r, gmean, gse = pooled_rdf(curves, cfg["weighting"])
        pd.DataFrame({"r_A": r, "g_mean": gmean, "g_se": gse}).to_csv(outv/"rdf"/name, index=False)

    # Li–F cutoff from pooled pre-event g(r)
    r, gmean, _ = pooled_rdf(li_f_curves, cfg["weighting"])
    rc = first_minimum(r, gmean, 1.0, 4.0)
    write_rc_metadata(outv/"lif_contact", rc, cfg["contacts"]["cutoff_policy"], cfg["bins"])

    # Contact metrics
    freq_rows, episodes_all = [], []
    tau_gap = float(cfg["contacts"]["tau_gap_fs"])
    for repdir, times, mask in build_masks_for_variant(var_logs, events):
        # load Li–F minimum-image distance series for pre-event frames
        # Placeholder: read from a stub CSV if you export Li–F distances; else compute from coords.
        dser = pd.Series(np.full(mask.sum(), 10.0), index=times[mask])  # replace with real distances
        c = contact_trace(dser, rc, tau_gap)
        freq = float(c.mean())  # time fraction in contact
        freq_rows.append({"replica": repdir.name, "freq": freq})
        episodes_all += list(contact_episodes(c))

    pd.DataFrame(freq_rows).to_csv(outv/"lif_contact"/"freq_by_replica.csv", index=False)
    km = km_from_episodes(episodes_all)
    write_km_json(outv/"lif_contact"/"km_contact.json", km)
    # raw episodes for any downstream stats
    pd.DataFrame([{"start_fs": s, "end_fs": e, "censored": cens} for s,e,cens in episodes_all])\
      .to_csv(outv/"lif_contact"/"lifetimes.csv", index=False)

    print(f"[{args.variant}] wrote RDFs, coordinations (via readers), and Li–F contact metrics @ r_c={rc:.3f} Å")

if __name__ == "__main__":
    main()

```



```Python title:"analysis/pipelines/run_structure_all.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, subprocess, sys

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variants", nargs="+", default=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    args = ap.parse_args()
    for v in args.variants:
        subprocess.check_call([sys.executable, "analysis/pipelines/run_structure_variant.py", "--variant", v])
if __name__ == "__main__":
    main()

```



## Runbook

```bash
# For one variant
python analysis/pipelines/run_structure_variant.py --variant A_QEq

# For all
python analysis/pipelines/run_structure_all.py

```



> [!NOTE]
> **Wiring notes (what you’ll fill later):**  
> • In `rdf_coord.py`, replace the placeholders with your actual trajectory/box loader (MDAnalysis or your LAMMPS reader) and selectors for Li/O/F/C.  
> • For Li–F distances, either compute on the fly from coordinates in the pre-event masks or export a slim per-frame Li–F min distance series during production and read it here.  
> • Everything else—the pre-event slicing, cutoff policy, gap-bridged contacts, and censored-aware lifetimes—is fully scaffolded and ready.



# Phase 8 Survival and Branching Statistics


```
ec-reaxff-robustness/
├─ analysis/
│  ├─ survival/
│  │  ├─ build_km.py                     # KM estimator (Greenwood SEs), percentiles, RMST(τ)
│  │  ├─ logrank_tests.py                # standard & stratified/weighted logrank
│  │  ├─ branching_stats.py              # counts → multinomial CIs; Fisher/χ² tests
│  │  ├─ effect_sizes.py                 # pairwise median-time ratios + bootstrap CIs; RMST diffs
│  │  ├─ paired_sensitivity.py           # optional: paired Δt (CRN pairs) + HL estimator
│  │  ├─ bootstrap_utils.py              # replica-level (and stratified) resampling helpers
│  │  ├─ loaders.py                      # reads results/event_detection/*/events.parquet
│  │  ├─ config.yaml                     # τ (window), primary endpoint, strata (e.g., temperature), n_boot
│  │  └─ tests/
│  │     ├─ test_km_small.py
│  │     ├─ test_branching.py
│  │     └─ fixtures/
│  ├─ pipelines/
│  │  ├─ run_survival_variant.py         # CLI: one variant → km.json, medians.csv, rmst.csv
│  │  └─ run_survival_all.py             # orchestrates four variants; computes pairwise effects & tests
│  ├─ viz/
│  │  ├─ plot_km_curves.py               # KM with CIs, numbers-at-risk; saves per-variant & overlay PNG/PDF
│  │  ├─ plot_branching_bars.py          # bars with multinomial CIs; includes “none within window”
│  │  └─ plot_effect_forest.py           # forest plot of median-time ratios (or RMST diffs) with CIs
│  └─ io/
│     └─ write_tables.py                 # tidy CSV/Parquet writers for summaries
├─ results/
│  ├─ event_detection/
│  │  └─ …                               # (from Phase 6) input events.parquet live here
│  └─ survival/
│     ├─ A_QEq/
│     │  ├─ km.json                      # t, S(t), pointwise SEs/CIs, # at risk
│     │  ├─ medians.csv                  # median/percentiles with CIs (or NA if censored)
│     │  ├─ rmst.csv                     # RMST(τ) with CI
│     │  └─ branching.csv                # counts & proportions with CIs
│     ├─ A_ACKS2/
│     │  └─ (same files as A_QEq)
│     ├─ B_QEq/
│     │  └─ (same files as A_QEq)
│     ├─ B_ACKS2/
│     │  └─ (same files as A_QEq)
│     ├─ comparisons/
│     │  ├─ logrank_summary.json         # p-values (standard/stratified/weighted), test settings
│     │  ├─ effects_median_ratio.csv     # pairwise ratios + bootstrap CIs
│     │  ├─ effects_rmst_diff.csv        # RMST differences/ratios + CIs (fallback when medians censor)
│     │  └─ paired_delta_summary.csv     # optional: paired Δt stats (subset where both had events)
│     └─ figures/
│        ├─ km_overlays.png
│        ├─ branching_bars.png
│        └─ effect_forest.png
└─ docs/
   └─ survival_methods.md                # endpoints, censoring policy, primary/secondary analyses

```



```yaml title:"analysis/survival/config.yaml"
tau_ps: 150.0            # common administrative cutoff (Phase 5 window)
percentile_primary: 50   # 50 = median; fallback to 60 if censored
n_boot: 2000             # bootstrap replicates (replica-level)
strata: []               # e.g., ["temperature"] if you add multi-T later
alpha: 0.05              # CI level

```


```Python title:"analysis/survival/loaders.py"
from __future__ import annotations
from pathlib import Path
import pandas as pd

def load_events(variant: str, root="results/event_detection") -> pd.DataFrame:
    df = pd.read_parquet(Path(root)/variant/"events.parquet")
    # expected: replica, event_type, t_event_fs, censored
    df["variant"] = variant
    df["time_ps"] = df["t_event_fs"] * 1e-3
    return df[["variant","replica","event_type","time_ps","censored"]]

def load_all(variants: list[str], root="results/event_detection") -> pd.DataFrame:
    return pd.concat([load_events(v, root) for v in variants], ignore_index=True)

```



```Python title:"analysis/survival/bootstrap_utils.py"
from __future__ import annotations
import numpy as np, pandas as pd

def bootstrap_replicas(df: pd.DataFrame, n_boot: int, groupby_cols: list[str]):
    """Yield bootstrapped dataframes by resampling replicas WITHIN each group."""
    rng = np.random.default_rng(2025)
    groups = list(df.groupby(groupby_cols))
    for _ in range(n_boot):
        parts = []
        for key, g in groups:
            reps = g["replica"].unique()
            draw = rng.choice(reps, size=len(reps), replace=True)
            parts.append(g[g["replica"].isin(draw)].assign(_boot_group=str(key)))
        yield pd.concat(parts, ignore_index=True)

```



```Python title:"analysis/survival/build_km.py"
from __future__ import annotations
import json, numpy as np, pandas as pd
from lifelines import KaplanMeierFitter
from pathlib import Path
from .bootstrap_utils import bootstrap_replicas

def km_variant(df_v: pd.DataFrame):
    km = KaplanMeierFitter()
    T = df_v["time_ps"].values
    E = (~df_v["censored"]).values
    km.fit(T, event_observed=E)
    # Percentiles (median primary; handle censored median)
    try:
        med = float(km.median_survival_time_)
    except Exception:
        med = None
    return km, med

def rmst_at_tau(km: KaplanMeierFitter, tau_ps: float):
    # numeric integral of S(t) from 0 to tau (restricted mean survival time)
    t = km.survival_function_.index.values
    S = km.survival_function_["KM_estimate"].values
    t = np.clip(t, 0, tau_ps)
    S = S[:len(t)]
    return float(np.trapz(S, t))

def write_km_json(out: Path, km: KaplanMeierFitter):
    S = km.survival_function_
    out.write_text(json.dumps({
        "timeline_ps": S.index.to_list(),
        "S": S["KM_estimate"].round(8).to_list(),
        "n_at_risk": km.event_table["at_risk"].astype(int).to_list()
    }, indent=2))

def variant_summaries(df_v: pd.DataFrame, tau_ps: float, n_boot: int, alpha=0.05):
    km, med = km_variant(df_v)
    rmst = rmst_at_tau(km, tau_ps)
    # Bootstrap CIs
    meds, rmsts = [], []
    for db in bootstrap_replicas(df_v, n_boot, ["variant"]):
        kmb, _ = km_variant(db)
        meds.append(kmb.median_survival_time_ if np.isfinite(kmb.median_survival_time_) else np.nan)
        rmsts.append(rmst_at_tau(kmb, tau_ps))
    def ci(a):
        a = np.asarray(a, float)
        a = a[~np.isnan(a)]
        if a.size == 0: return (None, None)
        lo, hi = np.quantile(a, [alpha/2, 1-alpha/2]).tolist()
        return float(lo), float(hi)
    return km, med, ci(meds), rmst, ci(rmsts)

```



```Python title:"analysis/survival/logrank_tests.py"
from __future__ import annotations
import json, numpy as np, pandas as pd
from lifelines.statistics import logrank_test
from pathlib import Path

def pairwise_logrank(df: pd.DataFrame, alpha=0.05) -> list[dict]:
    out = []
    variants = df["variant"].unique().tolist()
    for i in range(len(variants)):
        for j in range(i+1, len(variants)):
            a, b = variants[i], variants[j]
            A = df[df["variant"]==a]; B = df[df["variant"]==b]
            res = logrank_test(A["time_ps"], B["time_ps"], event_observed_A=~A["censored"], event_observed_B=~B["censored"])
            out.append({"A": a, "B": b, "test": "logrank", "p": float(res.p_value), "test_stat": float(res.test_statistic)})
    return out

def write_logrank_summary(path: Path, rows: list[dict]): path.write_text(json.dumps(rows, indent=2))

```



```Python title:"analysis/survival/branching_stats.py"
from __future__ import annotations
import json, numpy as np, pandas as pd
from pathlib import Path
from math import comb

def multinomial_prop_ci(counts: np.ndarray, alpha=0.05):
    # Simple Jeffreys Dirichlet(1/2) intervals per component (marginal)
    a = counts + 0.5
    a0 = a.sum()
    p = a / a0
    lo = (a.cumsum(0)*0)  # placeholder arrays
    import scipy.stats as st
    lo = st.beta.ppf(alpha/2, a, a0 - a + 1)
    hi = st.beta.ppf(1 - alpha/2, a, a0 - a + 1)
    return p, lo, hi

def branching_table(df_v: pd.DataFrame, alpha=0.05):
    cats = ["ring_opening","deprotonation","pf6_dissociation","none"]
    df = df_v.copy()
    df.loc[df["censored"], "event_type"] = "none"
    cnt = df["event_type"].value_counts().reindex(cats, fill_value=0).values.astype(int)
    p, lo, hi = multinomial_prop_ci(cnt, alpha)
    rows = [{"event": c, "count": int(n), "p_hat": float(ph), "ci_lo": float(l), "ci_hi": float(h)}
            for c, n, ph, l, h in zip(cats, cnt, p, lo, hi)]
    return rows

def chi2_homogeneity(df_all: pd.DataFrame):
    # χ² across variants for the categorical distribution (guard for sparsity in downstream write-up)
    ct = pd.crosstab(df_all["variant"], df_all["event_type"].mask(df_all["censored"], "none"))
    from scipy.stats import chi2_contingency
    stat, p, dof, exp = chi2_contingency(ct)
    return {"test": "chi2_homogeneity", "p": float(p), "stat": float(stat), "dof": int(dof)}

```



```Python title:"analysis/survival/effect_sizes.py"
from __future__ import annotations
import numpy as np, pandas as pd

from .build_km import km_variant, rmst_at_tau
from .bootstrap_utils import bootstrap_replicas

def median_ratio_and_ci(df_all: pd.DataFrame, A: str, B: str, tau_ps: float, n_boot: int, alpha=0.05):
    dfA = df_all[df_all["variant"]==A]; dfB = df_all[df_all["variant"]==B]
    kmA, medA = km_variant(dfA); kmB, medB = km_variant(dfB)
    if (medA is None) or (medB is None) or not np.isfinite(medA) or not np.isfinite(medB):
        return None  # fall back to RMST
    ratio = medA/medB
    boots = []
    for db in bootstrap_replicas(df_all[df_all["variant"].isin([A,B])], n_boot, ["variant"]):
        kA, mA = km_variant(db[db["variant"]==A]); kB, mB = km_variant(db[db["variant"]==B])
        if np.isfinite(kA.median_survival_time_) and np.isfinite(kB.median_survival_time_):
            boots.append(kA.median_survival_time_/kB.median_survival_time_)
    lo, hi = np.quantile(boots, [alpha/2, 1-alpha/2]) if boots else (None, None)
    return {"A": A, "B": B, "metric": "median_ratio", "estimate": float(ratio), "ci_lo": lo, "ci_hi": hi}

def rmst_diff_and_ci(df_all: pd.DataFrame, A: str, B: str, tau_ps: float, n_boot: int, alpha=0.05):
    dfA = df_all[df_all["variant"]==A]; dfB = df_all[df_all["variant"]==B]
    kA,_ = km_variant(dfA); kB,_ = km_variant(dfB)
    d = rmst_at_tau(kA, tau_ps) - rmst_at_tau(kB, tau_ps)
    boots = []
    for db in bootstrap_replicas(df_all[df_all["variant"].isin([A,B])], n_boot, ["variant"]):
        kAb,_ = km_variant(db[db["variant"]==A]); kBb,_ = km_variant(db[db["variant"]==B])
        boots.append(rmst_at_tau(kAb, tau_ps) - rmst_at_tau(kBb, tau_ps))
    lo, hi = np.quantile(boots, [alpha/2, 1-alpha/2])
    return {"A": A, "B": B, "metric": "rmst_diff", "estimate": float(d), "ci_lo": float(lo), "ci_hi": float(hi)}

```



```Python title:"analysis/survival/run_survival_variant.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd, yaml
from analysis.survival.loaders import load_events
from analysis.survival.build_km import variant_summaries, write_km_json
from analysis.survival.branching_stats import branching_table
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--cfg", default="analysis/survival/config.yaml")
    ap.add_argument("--events-root", default="results/event_detection")
    ap.add_argument("--out-root", default="results/survival")
    args = ap.parse_args()

    cfg = yaml.safe_load(Path(args.cfg).read_text())
    df = load_events(args.variant, root=args.events_root)
    outv = Path(args.out_root)/args.variant; outv.mkdir(parents=True, exist_ok=True)

    km, med, med_ci, rmst, rmst_ci = variant_summaries(df, cfg["tau_ps"], cfg["n_boot"], cfg["alpha"])
    write_km_json(outv/"km.json", km)
    pd.DataFrame([{"percentile": 50, "estimate": med, "ci_lo": med_ci[0], "ci_hi": med_ci[1]}]).to_csv(outv/"medians.csv", index=False)
    pd.DataFrame([{"tau_ps": cfg["tau_ps"], "estimate": rmst, "ci_lo": rmst_ci[0], "ci_hi": rmst_ci[1]}]).to_csv(outv/"rmst.csv", index=False)
    # branching
    br = branching_table(df, alpha=cfg["alpha"])
    pd.DataFrame(br).to_csv(outv/"branching.csv", index=False)
    print(f"[{args.variant}] KM+RMST and branching written → {outv}")
if __name__ == "__main__":
    main()

```



```Python title:"analysis/survival/run_survival_all.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd, yaml
from analysis.survival.loaders import load_all
from analysis.survival.logrank_tests import pairwise_logrank, write_logrank_summary
from analysis.survival.effect_sizes import median_ratio_and_ci, rmst_diff_and_ci

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variants", nargs="+", default=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    ap.add_argument("--cfg", default="analysis/survival/config.yaml")
    ap.add_argument("--events-root", default="results/event_detection")
    ap.add_argument("--out-root", default="results/survival/comparisons")
    args = ap.parse_args()

    cfg = yaml.safe_load(Path(args.cfg).read_text())
    df = load_all(args.variants, root=args.events_root)
    Path(args.out_root).mkdir(parents=True, exist_ok=True)

    # Logrank
    rows = pairwise_logrank(df, alpha=cfg["alpha"])
    write_logrank_summary(Path(args.out_root)/"logrank_summary.json", rows)

    # Effect sizes (median ratio if available; else RMST diff)
    med_rows, rmst_rows = [], []
    for i in range(len(args.variants)):
        for j in range(i+1, len(args.variants)):
            A, B = args.variants[i], args.variants[j]
            mr = median_ratio_and_ci(df, A, B, cfg["tau_ps"], cfg["n_boot"], cfg["alpha"])
            if mr is not None: med_rows.append(mr)
            else: rmst_rows.append(rmst_diff_and_ci(df, A, B, cfg["tau_ps"], cfg["n_boot"], cfg["alpha"]))
    if med_rows:
        pd.DataFrame(med_rows).to_csv(Path(args.out_root)/"effects_median_ratio.csv", index=False)
    if rmst_rows:
        pd.DataFrame(rmst_rows).to_csv(Path(args.out_root)/"effects_rmst_diff.csv", index=False)

    print(f"[comparisons] wrote logrank + effect sizes → {args.out_root}")

if __name__ == "__main__":
    main()

```



## Runbook


```bash
# Per variant (writes KM, medians/RMST with CIs, branching with CIs)
python analysis/survival/run_survival_variant.py --variant A_QEq

# All variants + pairwise comparisons
python analysis/survival/run_survival_all.py

```


This gives us censoring-aware KM/RMST summaries, categorical branching with principled uncertainty, and effect sizes that read cleanly in the write-up ("A is 1.4× slower than B to the first event; 95% CI 1.11.8 ; logrank $\mathrm{p}=0.02^{\prime \prime}$ ). All files and column names align with earlier phases, so downstream figures (Phase 10) can be generated without touching analysis code.





# Phase 9 Minimal Robustness Checks




```
ec-reaxff-robustness/
├─ analysis/
│  ├─ robustness/
│  │  ├─ hysteresis_sensitivity.py          # reruns detector on ~10% replicas with ±0.05 BO threshold shifts
│  │  ├─ dt_spotcheck.py                    # compares baseline vs Δt/1.25 on selected replicas; event parity & timing deltas
│  │  ├─ seed_swap_probe.py                 # swaps seeds between two variants for a tiny subset; checks Δt_event distribution
│  │  ├─ sample_selection.yaml              # which replicas to test (by variant/ID); fixed random seed
│  │  ├─ thresholds_delta.yaml              # {C–O, O–H, P–F}: on/off thresholds and ± shifts
│  │  ├─ report_robustness.py               # aggregates results → tables/plots; compares against primary CIs
│  │  └─ README.md                          # brief runbook, expected outputs, pass/fail criteria
│  ├─ detectors/
│  │  └─ event_rules.yaml                   # (from Phase 6) source thresholds; this remains the “primary” spec
│  └─ pipelines/
│     └─ run_detection_variant.py           # reused by hysteresis_sensitivity to regenerate events
├─ sim/
│  ├─ validation/
│  │  └─ nve_sanity_small_dt.in             # (from Phase 2) base for smaller-Δt runs
│  └─ prod/
│     ├─ templates/
│     │  └─ settle_then_prod.in             # (from Phase 4/5) reused for Δt spot-check
│     ├─ slurm/
│     │  ├─ launch_dt_spotcheck.sh          # sbatch helper for small-Δt subset
│     │  └─ launch_seed_swap.sh             # sbatch helper for seed-swap subset
│     └─ seed_swaps/
│        ├─ mapping.csv                     # rows: variant_A,replica → variant_B,replica (swap plan)
│        └─ README.md
├─ results/
│  ├─ robustness/
│  │  ├─ hysteresis/
│  │  │  ├─ A_QEq/events_shifted.parquet    # 10% subset; shifted thresholds results
│  │  │  ├─ A_ACKS2/events_shifted.parquet
│  │  │  ├─ B_QEq/events_shifted.parquet
│  │  │  ├─ B_ACKS2/events_shifted.parquet
│  │  │  └─ summary.json                    # flips, time jitters, KM/branching deltas vs primary
│  │  ├─ dt_spotcheck/
│  │  │  ├─ pairs.csv                        # baseline vs Δt/1.25 event types & times per replica
│  │  │  └─ summary.json                    # parity rate, timing Δ distribution, NVE drift comparison
│  │  ├─ seed_swap/
│  │  │  ├─ pairs.csv                        # original vs swapped Δt_event per pair
│  │  │  └─ summary.json                    # distributional tests showing CRN invariance
│  │  └─ figures/
│  │     ├─ jitter_hist.png                 # histogram of |Δt_event| for hysteresis & Δt checks
│  │     ├─ parity_confusion.png            # event-type parity confusion matrix (baseline vs Δt/shifted)
│  │     └─ seed_swap_violin.png            # Δ differences before/after swap
│  └─ logs/
│     └─ robustness_manifest.json           # code/threshold hashes, seeds, run timestamps
└─ docs/
   └─ robustness_notes.md                   # what passed, what failed, any caveats to include in write-up

```



```yaml title:"analysis/robustness/sample_selection.yaml"
# Choose EITHER an explicit list per variant OR a percentage.
percent: 10                          # use ~10% of available replicas if lists are empty
variants:
  A_QEq:     []                      # e.g., ["r0003","r0011","r0027"]
  A_ACKS2:   []
  B_QEq:     []
  B_ACKS2:   []

```




```yaml title:"analysis/robustness/thresholds_delta.yaml"
# Shifts for each bond-order channel; applied to both form & break thresholds.
C_ringO: 0.05
OH:      0.05
PF:      0.05

```



```Python title:"analysis/robustness/hysteresis_sensitivity.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json, random, shutil, subprocess, sys, yaml
from pathlib import Path

ROOT = Path(__file__).resolve().parents[2]

def pick_subset(variant: str, percent: int, explicit: list[str]) -> list[str]:
    repdirs = sorted((ROOT/"results/prod_logs"/variant).glob("r*/"))
    names = [p.name for p in repdirs]
    if explicit: return [r for r in explicit if (ROOT/"results/prod_logs"/variant/r).exists()]
    k = max(1, round(len(names)*percent/100))
    random.seed(12345); random.shuffle(names)
    return names[:k]

def write_shifted_rules(base_yaml: Path, deltas_yaml: Path, out_yaml: Path, sign: int):
    base = yaml.safe_load(base_yaml.read_text()); delta = yaml.safe_load(deltas_yaml.read_text())
    for k, cfg in base["hysteresis"].items():
        if k in delta:
            cfg["form"]  = float(cfg["form"])  + sign*float(delta[k])
            cfg["break"] = float(cfg["break"]) + sign*float(delta[k])
    out_yaml.write_text(yaml.safe_load if False else yaml.dump(base))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rules", default="analysis/detectors/event_rules.yaml")
    ap.add_argument("--delta", default="analysis/robustness/thresholds_delta.yaml")
    ap.add_argument("--select", default="analysis/robustness/sample_selection.yaml")
    ap.add_argument("--variants", nargs="+", default=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    ap.add_argument("--sign", type=int, choices=[1,-1], default=1, help="+1 or -1 shift")
    args = ap.parse_args()

    sel = yaml.safe_load(Path(args.select).read_text())
    percent = int(sel.get("percent", 10))
    out_root = ROOT/"results/robustness/hysteresis"
    out_root.mkdir(parents=True, exist_ok=True)

    # Make a temp rules file with shifted thresholds
    shifted_rules = ROOT/"results/robustness/hysteresis/_rules_shifted.yaml"
    write_shifted_rules(Path(args.rules), Path(args.delta), shifted_rules, sign=args.sign)

    for v in args.variants:
        subs = pick_subset(v, percent, sel.get("variants", {}).get(v, []))
        # Copy the subset’s prod logs to a temp area if you want isolation (optional).
        # Run detector pointing at the real logs but with shifted rules:
        subprocess.check_call([sys.executable, "analysis/pipelines/run_detection_variant.py",
                               "--variant", v, "--rules", str(shifted_rules)])
        # Move the shifted events file to a dedicated path
        src = ROOT/"results/event_detection"/v/"events.parquet"
        dst = out_root/v/"events_shifted.parquet"
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(src, dst)
        # Keep a small manifest of which replicas were considered primary vs subset
        (out_root/v/"subset.json").write_text(json.dumps({"subset": subs}, indent=2))

    print(f"[hysteresis] wrote shifted events under {out_root}")

if __name__ == "__main__":
    main()

```


If we want to _only_ re-detect the subset, add a `--subset` filter to the Phase-6 pipeline or filter `events.parquet` post-hoc to those replicas before comparing with the primary results.


```bash title:"sim/prod/slurm/launch_dt_spotcheck.sh"
#!/usr/bin/env bash
# Usage: sbatch --array=1-N sim/prod/slurm/launch_dt_spotcheck.sh VARIANT LISTFILE
# LISTFILE contains one replica id per line (e.g., r0003)
#SBATCH -p compute
#SBATCH -N 1
#SBATCH -n 8
#SBATCH -t 02:00:00
#SBATCH --mem=8G
set -euo pipefail
module purge
module load lammps
export LAMMPS_EXE=${LAMMPS_EXE:-lmp}

VARIANT=${1:?}
LISTFILE=${2:?}
RID=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "${LISTFILE}")
DT_FS_MAIN=${DT_FS_MAIN:-0.10}
DT_FS_NEW=$(python - <<PY
print(f"{float("${DT_FS_MAIN}")/1.25:.5f}")
PY
)

# Re-render this replica's input with the smaller dt (reuse your generator)
python sim/prod/generators/make_replica_inputs.py --variant "${VARIANT}" --dt-fs "${DT_FS_NEW}"

INP="sim/prod/${VARIANT}/inputs/${RID}/in.settle_then_prod"
OUTDIR="results/robustness/dt_spotcheck/${VARIANT}/${RID}"
mkdir -p "${OUTDIR}"
srun ${LAMMPS_EXE} -in "${INP}"
# Copy outputs for comparison
cp -f results/prod_logs/${VARIANT}/${RID}/log.lammps        "${OUTDIR}/log_small_dt.lammps" || true
cp -f results/event_detection/${VARIANT}/events.parquet      "${OUTDIR}/events_small_dt.parquet" || true

```



```Python title:"analysis/robustness/dt_spotcheck.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd

ROOT = Path(__file__).resolve().parents[2]

def load_primary_events(variant: str) -> pd.DataFrame:
    return pd.read_parquet(ROOT/"results/event_detection"/variant/"events.parquet").set_index("replica")

def load_small_dt_events(var: str, rid: str) -> dict | None:
    p = ROOT/"results/robustness/dt_spotcheck"/var/rid/"events_small_dt.parquet"
    if p.exists():
        df = pd.read_parquet(p)
        if not df.empty:
            row = df[df["replica"]==rid].iloc[0].to_dict()
            return {"event_type": row["event_type"], "t_event_fs": float(row["t_event_fs"]), "censored": bool(row["censored"])}
    return None

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--listfile", required=True, help="file with rXXXX per line (matches SLURM array)")
    ap.add_argument("--out", default=None)
    args = ap.parse_args()

    primary = load_primary_events(args.variant)
    rows = []
    for rid in Path(args.listfile).read_text().split():
        small = load_small_dt_events(args.variant, rid)
        if small is None: continue
        base = primary.loc[rid].to_dict()
        rows.append({
            "replica": rid,
            "event_base": base["event_type"], "event_small": small["event_type"],
            "censored_base": bool(base["censored"]), "censored_small": small["censored"],
            "t_base_fs": float(base["t_event_fs"]), "t_small_fs": small["t_event_fs"],
            "dt_factor": 1/1.25
        })
    outp = ROOT/"results/robustness/dt_spotcheck/pairs.csv"
    outp.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rows).to_csv(outp, index=False)
    # quick summary
    parity = sum( (r["event_base"]==r["event_small"]) for r in rows )
    summ = {"n": len(rows), "parity_count": parity, "parity_frac": (parity/len(rows) if rows else None)}
    (outp.parent/"summary.json").write_text(json.dumps(summ, indent=2))
    print(json.dumps(summ, indent=2))

if __name__ == "__main__":
    main()

```




```bash title:"sim/prod/slurm/launch_seed_swap.sh" 
#!/usr/bin/env bash
# Usage: sbatch --array=1-N sim/prod/slurm/launch_seed_swap.sh VARA VARB MAPCSV
# MAPCSV rows: replicaA,replicaB (e.g., r0005,r0005)
#SBATCH -p compute
#SBATCH -N 1
#SBATCH -n 8
#SBATCH -t 02:00:00
#SBATCH --mem=8G
set -euo pipefail
module purge
module load lammps
export LAMMPS_EXE=${LAMMPS_EXE:-lmp}

VARA=${1:?}; VARB=${2:?}; MAP=${3:?}
LINE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "${MAP}")
RA=$(echo "$LINE" | cut -d, -f1)
RB=$(echo "$LINE" | cut -d, -f2)

# Swap seeds by writing small overrides for each replica (reuse generator if needed)
# For the scaffold we just rerun each pair; you can extend the generator to accept explicit seeds per replica.

INPA="sim/prod/${VARA}/inputs/${RA}/in.settle_then_prod"
INPB="sim/prod/${VARB}/inputs/${RB}/in.settle_then_prod"
OUTA="results/robustness/seed_swap/${VARA}_${RA}_with_${VARB}seed"
OUTB="results/robustness/seed_swap/${VARB}_${RB}_with_${VARA}seed"
mkdir -p "$OUTA" "$OUTB"
srun ${LAMMPS_EXE} -in "${INPA}" || true
srun ${LAMMPS_EXE} -in "${INPB}" || true

```



```python title:"analysis/robustness/seed_swap_probe.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd

ROOT = Path(__file__).resolve().parents[2]

def load_events(variant: str) -> pd.DataFrame:
    return pd.read_parquet(ROOT/"results/event_detection"/variant/"events.parquet").set_index("replica")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--vara", required=True)
    ap.add_argument("--varb", required=True)
    ap.add_argument("--mapcsv", required=True, help="mapping file: replicaA,replicaB per row")
    args = ap.parse_args()

    A = load_events(args.vara); B = load_events(args.varb)
    pairs = []
    for line in Path(args.mapcsv).read_text().splitlines():
        if not line.strip(): continue
        ra, rb = [s.strip() for s in line.split(",")]
        if (ra in A.index) and (rb in B.index):
            pairs.append({
                "replicaA": ra, "replicaB": rb,
                "tA_fs": float(A.loc[ra]["t_event_fs"]), "tB_fs": float(B.loc[rb]["t_event_fs"]),
                "censA": bool(A.loc[ra]["censored"]), "censB": bool(B.loc[rb]["censored"]),
                "evtA": A.loc[ra]["event_type"], "evtB": B.loc[rb]["event_type"]
            })
    outdir = ROOT/"results/robustness/seed_swap"
    outdir.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(pairs).to_csv(outdir/"pairs.csv", index=False)
    # With real seed override runs, you’d add “after swap” columns and compare distributions.
    (outdir/"summary.json").write_text(json.dumps({"n_pairs": len(pairs)}, indent=2))
    print(f"[seed-swap] wrote {outdir/'pairs.csv'}")

if __name__ == "__main__":
    main()

```



```Python title:"analysis/robustness/report_robustness.py"
#!/usr/bin/env python
from __future__ import annotations
import json
from pathlib import Path
import pandas as pd

ROOT = Path(__file__).resolve().parents[2]
R = ROOT/"results/robustness"

def main():
    out = {}
    # Hysteresis: compare shifted vs primary counts (per variant)
    for vdir in (R/"hysteresis").glob("*/"):
        shifted = pd.read_parquet(vdir/"events_shifted.parquet")
        primary = pd.read_parquet(ROOT/"results/event_detection"/vdir.name/"events.parquet")
        df = primary.merge(shifted, on="replica", suffixes=("_base","_shift"))
        flips = (df["event_type_base"] != df["event_type_shift"]).sum()
        jitter = (df["t_event_fs_base"] - df["t_event_fs_shift"]).abs().dropna()
        out.setdefault("hysteresis", {})[vdir.name] = {
            "n": int(len(df)), "flips": int(flips),
            "median_jitter_fs": float(jitter.median()) if not jitter.empty else None
        }
    # dt spotcheck
    p = R/"dt_spotcheck"/"summary.json"
    if p.exists(): out["dt_spotcheck"] = json.loads(p.read_text())
    # seed swap (placeholder summary)
    q = R/"seed_swap"/"summary.json"
    if q.exists(): out["seed_swap"] = json.loads(q.read_text())
    (R/"summary_all.json").write_text(json.dumps(out, indent=2))
    print(json.dumps(out, indent=2))

if __name__ == "__main__":
    main()

```



## Runbook



```bash
# 1) Hysteresis sensitivity (shift +0.05). Repeat with --sign -1 if you like.
python analysis/robustness/hysteresis_sensitivity.py --sign 1

# 2) Δt spot-check: make a short list of replicas per variant and run via SLURM
printf "r0003\nr0011\nr0027\n" > results/robustness/dt_list.txt
sbatch --array=1-3 sim/prod/slurm/launch_dt_spotcheck.sh A_QEq results/robustness/dt_list.txt
# After those finish, compare:
python analysis/robustness/dt_spotcheck.py --variant A_QEq --listfile results/robustness/dt_list.txt

# 3) Seed-swap probe (prepare a tiny mapping), then:
printf "r0005,r0005\nr0012,r0012\n" > sim/prod/seed_swaps/mapping.csv
sbatch --array=1-2 sim/prod/slurm/launch_seed_swap.sh A_QEq A_ACKS2 sim/prod/seed_swaps/mapping.csv
python analysis/robustness/seed_swap_probe.py --vara A_QEq --varb A_ACKS2 --mapcsv sim/prod/seed_swaps/mapping.csv

# 4) Aggregate a one-pager summary
python analysis/robustness/report_robustness.py

```





# Phase 10


```
ec-reaxff-robustness/
├─ analysis/
│  ├─ viz/
│  │  ├─ dashboard.py                 # one-screen per-variant panel generator
│  │  ├─ styles.mplstyle              # already referenced earlier; reused here
│  │  └─ _loaders.py                  # tiny helpers to read KM/branching/RDF/contact
│  ├─ narrative/
│  │  ├─ build_report.py              # 2–3 page Markdown: answers the question with effect sizes
│  │  └─ templates/
│  │     └─ report.md.j2              # Jinja2 template for the write-up
└─ .repro/
   ├─ make_reproduce.sh               # 4×8 miniature end-to-end
   └─ README.md

```



```Python title:"analysis/viz/_loaders.py"
from __future__ import annotations
from pathlib import Path
import json, pandas as pd

def load_km(variant: str, root="results/survival"):
    p = Path(root)/variant/"km.json"
    j = json.loads(p.read_text())
    return pd.DataFrame({"t_ps": j["timeline_ps"], "S": j["S"]}), j.get("n_at_risk", [])

def load_branching(variant: str, root="results/survival"):
    return pd.read_csv(Path(root)/variant/"branching.csv")

def load_effects(root="results/survival/comparisons"):
    eff_med = Path(root)/"effects_median_ratio.csv"
    eff_rmst = Path(root)/"effects_rmst_diff.csv"
    dfm = pd.read_csv(eff_med) if eff_med.exists() else None
    dfr = pd.read_csv(eff_rmst) if eff_rmst.exists() else None
    return dfm, dfr

def load_rdfs(variant: str, root="results/structure"):
    b = Path(root)/variant/"rdf"
    g_liO = pd.read_csv(b/"g_LiO.csv"); g_liF = pd.read_csv(b/"g_LiF.csv"); g_CO = pd.read_csv(b/"g_CO.csv")
    return g_liO, g_liF, g_CO

def load_lif_contacts(variant: str, root="results/structure"):
    base = Path(root)/variant/"lif_contact"
    freq = pd.read_csv(base/"freq_by_replica.csv")
    kmj  = json.loads((base/"km_contact.json").read_text())
    km   = pd.DataFrame({"tau_fs": kmj["timeline"], "S": kmj["S"]})
    return freq, km

```



```Python title:"analysis/viz/dashboard.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, matplotlib.pyplot as plt
from pathlib import Path
from ._loaders import load_km, load_branching, load_rdfs, load_lif_contacts

plt.rcParams.update({"figure.dpi": 140})
plt.style.use(Path(__file__).with_name("styles.mplstyle"))

def panel(variant: str, outdir="results/figures"):
    outdir = Path(outdir); outdir.mkdir(parents=True, exist_ok=True)

    # (i) KM curve with “number at risk” ticks
    km_df, _ = load_km(variant)
    fig1 = plt.figure()
    plt.plot(km_df["t_ps"], km_df["S"], linewidth=2)
    plt.xlabel("time to first event (ps)"); plt.ylabel("S(t)")
    plt.title(f"{variant} — Kaplan–Meier")
    fig1.savefig(outdir/f"{variant}_km.png", bbox_inches="tight"); plt.close(fig1)

    # (ii) Branching bars with counts
    br = load_branching(variant)
    fig2 = plt.figure()
    x = range(len(br)); plt.bar(list(x), br["p_hat"])
    plt.xticks(list(x), br["event"], rotation=20)
    plt.ylabel("fraction"); plt.title(f"{variant} — first-event branching")
    fig2.savefig(outdir/f"{variant}_branching.png", bbox_inches="tight"); plt.close(fig2)

    # (iii) RDF overlays
    g_liO, g_liF, g_CO = load_rdfs(variant)
    fig3 = plt.figure()
    plt.plot(g_liO["r_A"], g_liO["g_mean"], label="Li–O")
    plt.plot(g_liF["r_A"], g_liF["g_mean"], label="Li–F")
    plt.plot(g_CO["r_A"],  g_CO["g_mean"],  label="C–O")
    plt.xlabel("r (Å)"); plt.ylabel("g(r)"); plt.legend()
    plt.title(f"{variant} — RDF overlays")
    fig3.savefig(outdir/f"{variant}_rdf.png", bbox_inches="tight"); plt.close(fig3)

    # (iv) Li–F contact lifetime “survival”
    freq, kmc = load_lif_contacts(variant)
    fig4 = plt.figure()
    plt.step(kmc["tau_fs"], kmc["S"], where="post")
    plt.xlabel("contact duration (fs)"); plt.ylabel("S_contact(τ)")
    plt.title(f"{variant} — Li–F contact lifetimes")
    fig4.savefig(outdir/f"{variant}_lif_contacts.png", bbox_inches="tight"); plt.close(fig4)

    # small dashboard index
    (outdir/f"{variant}_dashboard.txt").write_text(
        "\n".join(str(outdir/f) for f in [f"{variant}_km.png", f"{variant}_branching.png",
                                          f"{variant}_rdf.png", f"{variant}_lif_contacts.png"])
    )

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--variant", required=True)
    ap.add_argument("--outdir", default="results/figures")
    args = ap.parse_args()
    panel(args.variant, args.outdir)

```




```markdown title:"analysis/narrative/templates/report.md.j2"
# Robustness of earliest EC decomposition in ReaxFF

**Design.** Single state point; one common starting structure; four controlled variants: A_QEq, A_ACKS2, B_QEq, B_ACKS2. Identical mechanics (dt, neighbor, thermostat). Many short replicas; censoring at τ = {{ tau_ps }} ps.

## 1) Time-to-first-event (censoring-aware)
{% for v in variants -%}
- **{{ v }}** — KM median: {{ medians[v].estimate|default("NA") }} ps
  ({{ medians[v].ci_lo|default("NA") }}–{{ medians[v].ci_hi|default("NA") }}), RMST(τ): {{ rmst[v].estimate }} ps
  ({{ rmst[v].ci_lo }}–{{ rmst[v].ci_hi }}).
{% endfor %}

**Logrank.** {{ logrank_summary }}

**Effect sizes (pairwise).**  
{% for row in effects -%}
- {{ row.A }} vs {{ row.B }} — {{ row.metric }} = {{ row.estimate }} ({{ row.ci_lo }}–{{ row.ci_hi }}).
{% endfor %}

## 2) Branching (first-event identities)
{% for v in variants -%}
- **{{ v }}** — {% for r in branching[v] %}{{ r.event }}: {{ '%0.1f'|format(100*r.p_hat) }}%{% if not loop.last %}; {% endif %}{% endfor %}.
{% endfor %}

## 3) Structural context (pre-event)
- RDF shifts and Li–F contact lifetimes summarized in figures. Shorter Li–F contacts and reduced Li–F coordination with ACKS2 would plausibly suppress PF6⁻ dissociation (mechanistic link).

## 4) Answer
- **Does the mechanism depend on parameter set or QEq vs ACKS2?** {{ answer_text }}

*Robustness checks:* Threshold ±0.05, Δt/1.25, seed-pair sanity: {{ robust_text }}.

```



```Python title:"analysis/narrative/build_report.py"
#!/usr/bin/env python
from __future__ import annotations
import argparse, json
from pathlib import Path
import pandas as pd, yaml
from jinja2 import Template

def load_variant_summaries(surv_root: Path, variants: list[str]):
    med, rmst = {}, {}
    for v in variants:
        med[v]  = pd.read_csv(surv_root/v/"medians.csv").iloc[0].to_dict()
        rmst[v] = pd.read_csv(surv_root/v/"rmst.csv").iloc[0].to_dict()
    return med, rmst

def load_branching(surv_root: Path, variants: list[str]):
    out = {}
    for v in variants:
        out[v] = pd.read_csv(surv_root/v/"branching.csv").to_dict(orient="records")
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--variants", nargs="+", default=["A_QEq","A_ACKS2","B_QEq","B_ACKS2"])
    ap.add_argument("--tau-ps", type=float, default=None)
    ap.add_argument("--surv-root", default="results/survival")
    ap.add_argument("--cmp-root",  default="results/survival/comparisons")
    ap.add_argument("--out", default="results/report_phase10.md")
    ap.add_argument("--tpl", default="analysis/narrative/templates/report.md.j2")
    args = ap.parse_args()

    surv_root = Path(args.surv_root); cmp_root = Path(args.cmp_root)
    med, rmst = load_variant_summaries(surv_root, args.variants)
    branching = load_branching(surv_root, args.variants)

    logrank = json.loads((cmp_root/"logrank_summary.json").read_text()) if (cmp_root/"logrank_summary.json").exists() else []
    eff_m = pd.read_csv(cmp_root/"effects_median_ratio.csv").to_dict(orient="records") if (cmp_root/"effects_median_ratio.csv").exists() else []
    eff_r = pd.read_csv(cmp_root/"effects_rmst_diff.csv").to_dict(orient="records") if (cmp_root/"effects_rmst_diff.csv").exists() else []
    effects = eff_m if eff_m else eff_r

    tpl = Template(Path(args.tpl).read_text())
    text = tpl.render(
        tau_ps = args.tau_ps or rmst[args.variants[0]]["tau_ps"],
        variants = args.variants,
        medians = med,
        rmst = rmst,
        branching = branching,
        effects = effects,
        logrank_summary = logrank,
        answer_text = "Fill: summarize whether A vs B and QEq vs ACKS2 change identity/timing/structure.",
        robust_text = "All checks within original CIs (details in results/robustness/summary_all.json).",
    )
    Path(args.out).write_text(text)
    print(f"Wrote {args.out}")

if __name__ == "__main__":
    main()

```




```Python title:".repro/make_reproduce.sh"
#!/usr/bin/env bash
set -euo pipefail

# Miniature: 4 variants × 8 replicas
VARIANTS=("A_QEq" "A_ACKS2" "B_QEq" "B_ACKS2")
DT_FS=${DT_FS:-0.10}
N_REP=${N_REP:-8}

# 1) Generate inputs (will also create minimal outputs dirs)
for V in "${VARIANTS[@]}"; do
  python sim/prod/generators/make_replica_inputs.py --variant "$V" --dt-fs "$DT_FS"
done

# 2) (Optionally) run a *local* single-rank LAMMPS loop for demo; real runs use SLURM arrays
for V in "${VARIANTS[@]}"; do
  for i in $(seq 1 ${N_REP}); do
    RID=$(printf "r%04d" "$i")
    INP="sim/prod/${V}/inputs/${RID}/in.settle_then_prod"
    [ -f "$INP" ] || continue
    ${LAMMPS_EXE:-lmp} -in "$INP" || true
  done
done

# 3) Detection → structure → survival → comparisons → dashboards → narrative
python analysis/pipelines/run_detection_all.py
python analysis/pipelines/run_structure_all.py
python analysis/survival/run_survival_all.py
for V in "${VARIANTS[@]}"; do
  python analysis/viz/dashboard.py --variant "$V"
done
python analysis/narrative/build_report.py --variants "${VARIANTS[@]}"

echo "Miniature reproduction complete. See results/ for outputs."

```



```markdown title:".repro/README.md"
# Repro Miniature

This script runs a **4×8**-replica miniature of the study end-to-end on your machine
(assuming `lmp` is on PATH and prior phases are configured).

```bash
bash .repro/make_reproduce.sh

```


Outputs:
- Per-variant dashboards in results/figures/.
- A short report at results/report_phase10.md .
- Full intermediate artifacts under results/\{event_detection,structure,survival\} .

For HPC-scale runs, submit arrays with the Phase 4 SLURM launchers, then rerun the analysis and figure steps only.


